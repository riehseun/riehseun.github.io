{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supervised Leanring : we are given a data set, already know what our correct outputs are. Ex. Regression and Classification<br/>\n",
    "Regression : continuous output. Map input to continuous function<br/>\n",
    "Classification : discrete output. Map input to discrete categories<br/><br/>\n",
    "Unsupervised learning : we have little or no idea what our correct outputs are. We derive structure from data<br/><br/>\n",
    "Reinforcement learning : TBD<br/><br/>\n",
    "Recommender system : TBD<br/><br/>\n",
    "Regression<br/>\n",
    "$m$ = number of training examples<br/>\n",
    "$x$ = inputs<br/>\n",
    "$y$ = outputs<br/>\n",
    "$h$ = hypothesis<br/>\n",
    "$\\theta$ = parameter<br/>\n",
    "Hypothesis : $h_\\theta(x) = \\theta_0 + \\theta_1 x$<br/><br/>\n",
    "Paramaters : $\\theta_0, \\theta_1$<br/><br/>\n",
    "Cost : $J(\\theta_0, \\theta_1) =  \\dfrac{1}{2m}\\sum_{i=1}^{m} (h_\\theta(x^{(i)})-y^{(i)})^2$<br/><br/>\n",
    "Objective : Min $J(\\theta_0, \\theta_1)$<br/><br/> \n",
    "Gradient descent<br/>\n",
    "Start with some $\\theta_0, \\theta_1$<br/>\n",
    "Keep changing $\\theta_0, \\theta_1$ to reduce $J(\\theta_0, \\theta_1)$ until reaching minimum<br/>\n",
    "Repeat until convergence (Simultaneous update all j)<br/><br/>\n",
    "$\\theta_j := \\theta_j - \\alpha\\frac{\\partial}{\\partial \\theta_j}J(\\theta_0, \\theta_1)$<br/>\n",
    "Small $\\alpha$ ? slow<br/>\n",
    "Large $\\alpha$ ? may diverge<br/>\n",
    "No need to decrease $\\alpha$ since gradient descent will automatically take smaller steps as it reaches towards optimum<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple variable<br/><br/>\n",
    "Hypothesis : $h_\\theta(x) = \\theta^{T}x$<br/><br/>\n",
    "Paramaters : $\\theta$<br/><br/>\n",
    "Cost : $J(\\theta) =  \\dfrac{1}{2m}\\sum_{i=1}^{m} (h_\\theta(x^{(i)})-y^{(i)})^2$<br/><br/>\n",
    "Objective : Min $J(\\theta)$<br/><br/> \n",
    "Repeat until convergence (Simultaneous update all j)<br/><br/>\n",
    "$\\theta_j := \\theta_j - \\alpha\\frac{\\partial}{\\partial \\theta_j}J(\\theta)$<br/><br/>\n",
    "Feature Scaling : Make sure feature scales are between -1 and +1<br/>\n",
    "Mean normalization : Shift $x_i$ to $x_i - \\mu_i$ so that means are zero<br/><br/>\n",
    "Normal Equation : method to solve $\\theta$ analytically<br/><br/>\n",
    "Solve for $\\theta_0, \\theta_1, ... , \\theta_m$<br/><br/>\n",
    "where $J(\\theta_0, \\theta_1, ... , \\theta_m) =  \\dfrac{1}{2m}\\sum_{i=1}^{m} (h_\\theta(x^{(i)})-y^{(i)})^2$ and $\\frac{\\partial}{\\partial \\theta_j}J(\\theta)$ = 0 for all j<br/><br/>\n",
    "$\\theta = (X^{T}X)^{-1}X^{T}y$<br/><br/>\n",
    "If $X^{T}X$ is non-invertible?<br/>\n",
    "Redundant features (linearly dependent) or too many features (n > m)<br/><br/>\n",
    "Gradient descent - need to choose $\\alpha$, need to iterate, works when n is large<br/>\n",
    "Normal equation - no need to choose $\\alpha$, no need to iterate, slow when n is large<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression : classification<br/>\n",
    "threshold classifier output<br/>\n",
    "If $h_\\theta(x)$ $\\ge$ 0.5, then y = 1<br/>\n",
    "If $h_\\theta(x)$ < 0.5, then y = 0<br/>\n",
    "We want to model $h_\\theta(x)$ as probability such that 0 $\\le$ $h_\\theta(x)$ $\\le$ 1<br/>\n",
    "Let $h_\\theta(x)$ = $g(\\theta^{T}X)$ where $g(z) = \\frac{1}{1 + e^{-z}}$, then $h_\\theta(x) = \\frac{1}{1 + e^{-\\theta^{T}X}}$ (sigmoid function)<br/>  \n",
    "Logistic regression cost function<br/>\n",
    "$J(\\theta) = -\\frac{1}{m}[\\sum_{i=1}^{m} y^{(i)}logh_\\theta(x^{(i)}) + (1-y^{(i)})log(1-h_\\theta(x^{(i)}))]$<br/>\n",
    "Advanced Gradient descent : conjugate gradient / BFGS / L-BFGS<br/>\n",
    "One vs All<br/>\n",
    "Train a logistic regression classifier $h_\\theta^{(i)}(x)$ for each class i to predict probability that $y = i$<br/>\n",
    "Pick the class that maximizes $h_\\theta^{(i)}(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem of overfitting - too many features fitting training examples very well but fail to generalize for new examples<br/>\n",
    "\n",
    "Regularization - reduce magnitude of parameters<br/>\n",
    "\n",
    "$J(\\theta) =  \\dfrac{1}{2m}[\\sum_{i=1}^{m} (h_\\theta(x^{(i)})-y^{(i)})^2 + \\lambda\\sum_{j=1}^{n}\\theta_j^2]$<br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cost function<br/>\n",
    "Logistic regression: $J(\\theta) = -\\frac{1}{m}[\\sum_{i=1}^{m} y^{(i)}logh_\\theta(x^{(i)}) + (1-y^{(i)})log(1-h_\\theta(x^{(i)}))] + \\frac{\\lambda}{2m}\\sum_{j=1}^{n}\\theta_j^2$<br/><br/>\n",
    "Neural network: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
