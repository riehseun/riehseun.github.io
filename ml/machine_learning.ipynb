{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Introduction</h1>\n",
    "<br/>\n",
    "Supervised Leanring : we are given a data set, already know what our correct outputs are. Ex. Regression and Classification\n",
    "<br/>\n",
    "Regression : continuous output. Map input to continuous function\n",
    "<br/>\n",
    "Classification : discrete output. Map input to discrete categories\n",
    "<br/><br/>\n",
    "Unsupervised learning : we have little or no idea what our correct outputs are. We derive structure from data\n",
    "<br/><br/>\n",
    "Reinforcement learning : TBD\n",
    "<br/><br/>\n",
    "Recommender system : TBD\n",
    "<br/>\n",
    "<h1>Regression with two variables</h1>\n",
    "<br/>\n",
    "$m$ = number of training examples\n",
    "<br/>\n",
    "$x$ = inputs\n",
    "<br/>\n",
    "$y$ = outputs\n",
    "<br/>\n",
    "$h$ = hypothesis\n",
    "<br/>\n",
    "$\\theta$ = parameter\n",
    "<br/><br/>\n",
    "Hypothesis : $h_\\theta(x) = \\theta_0 + \\theta_1 x$\n",
    "<br/><br/>\n",
    "Paramaters : $\\theta_0, \\theta_1$\n",
    "<br/><br/>\n",
    "Cost : $J(\\theta_0, \\theta_1) =  \\dfrac{1}{2m}\\sum_{i=1}^{m} (h_\\theta(x^{(i)})-y^{(i)})^2$\n",
    "<br/><br/>\n",
    "Objective : Min $J(\\theta_0, \\theta_1)$\n",
    "<br/><br/> \n",
    "<h2>Gradient descent</h2>\n",
    "<br/>\n",
    "Start with some $\\theta_0, \\theta_1$\n",
    "<br/>\n",
    "Keep changing $\\theta_0, \\theta_1$ to reduce $J(\\theta_0, \\theta_1)$ until reaching minimum\n",
    "<br/>\n",
    "Repeat until convergence (Simultaneous update all j)\n",
    "<br/><br/>\n",
    "$\\theta_j := \\theta_j - \\alpha\\frac{\\partial}{\\partial \\theta_j}J(\\theta_0, \\theta_1)$\n",
    "<br/>\n",
    "Small $\\alpha$ ? slow\n",
    "<br/>\n",
    "Large $\\alpha$ ? may diverge\n",
    "<br/>\n",
    "No need to decrease $\\alpha$ since gradient descent will automatically take smaller steps as it reaches towards optimum\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Regression with Multiple variable</h1>\n",
    "<br/>\n",
    "Hypothesis : $h_\\theta(x) = \\theta^{T}x$\n",
    "<br/><br/>\n",
    "Paramaters : $\\theta$\n",
    "<br/><br/>\n",
    "Cost : $J(\\theta) =  \\dfrac{1}{2m}\\sum_{i=1}^{m} (h_\\theta(x^{(i)})-y^{(i)})^2$\n",
    "<br/><br/>\n",
    "Objective : Min $J(\\theta)$\n",
    "<br/><br/> \n",
    "<h2>Gradient descent</h2>\n",
    "<br/>\n",
    "Repeat until convergence (Simultaneous update all j)\n",
    "<br/><br/>\n",
    "$\\theta_j := \\theta_j - \\alpha\\frac{\\partial}{\\partial \\theta_j}J(\\theta)$\n",
    "<br/><br/>\n",
    "Feature Scaling : Make sure feature scales are between -1 and +1\n",
    "<br/>\n",
    "Mean normalization : Shift $x_i$ to $x_i - \\mu_i$ so that means are zero\n",
    "<br/><br/>\n",
    "<h2>Normal Equation</h2>\n",
    "<br/>\n",
    "Method to solve $\\theta$ analytically\n",
    "<br/><br/>\n",
    "Solve for $\\theta_0, \\theta_1, ... , \\theta_m$\n",
    "<br/>\n",
    "where $J(\\theta_0, \\theta_1, ... , \\theta_m) =  \\dfrac{1}{2m}\\sum_{i=1}^{m} (h_\\theta(x^{(i)})-y^{(i)})^2$ and $\\frac{\\partial}{\\partial \\theta_j}J(\\theta)$ = 0 for all j\n",
    "<br/><br/>\n",
    "$\\theta = (X^{T}X)^{-1}X^{T}y$\n",
    "<br/><br/>\n",
    "If $X^{T}X$ is non-invertible?\n",
    "<br/>\n",
    "Redundant features (linearly dependent) or too many features (n > m)\n",
    "<br/><br/>\n",
    "Gradient descent - need to choose $\\alpha$, need to iterate, works when n is large\n",
    "<br/>\n",
    "Normal equation - no need to choose $\\alpha$, no need to iterate, slow when n is large\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Logistic regression : classification</h1>\n",
    "<br/>\n",
    "Threshold classifier output\n",
    "<br/>\n",
    "If $h_\\theta(x)$ $\\ge$ 0.5, then y = 1\n",
    "<br/>\n",
    "If $h_\\theta(x)$ < 0.5, then y = 0\n",
    "<br/>\n",
    "We want to model $h_\\theta(x)$ as probability such that 0 $\\le$ $h_\\theta(x)$ $\\le$ 1\n",
    "<br/><br/>\n",
    "Let $h_\\theta(x)$ = $g(\\theta^{T}X)$ where $g(z) = \\frac{1}{1 + e^{-z}}$, then $h_\\theta(x) = \\frac{1}{1 + e^{-\\theta^{T}X}}$ (sigmoid function)\n",
    "<br/><br/>  \n",
    "Logistic regression cost function\n",
    "<br/><br/>\n",
    "$J(\\theta) = -\\frac{1}{m}[\\sum_{i=1}^{m} y^{(i)}logh_\\theta(x^{(i)}) + (1-y^{(i)})log(1-h_\\theta(x^{(i)}))]$\n",
    "<br/><br/>\n",
    "<h2>Advanced Gradient descent</h2>\n",
    "<br/>\n",
    "conjugate gradient / BFGS / L-BFGS\n",
    "<br/>\n",
    "<h2>One vs All</h2>\n",
    "<br/>\n",
    "Train a logistic regression classifier $h_\\theta^{(i)}(x)$ for each class i to predict probability that $y = i$\n",
    "<br/>\n",
    "Pick the class that maximizes $h_\\theta^{(i)}(x)$\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Problem of overfitting</h2>\n",
    "<br/>\n",
    "Too many features fitting training examples very well but fail to generalize for new examples\n",
    "<br/>\n",
    "Regularization - reduce magnitude of parameters\n",
    "<br/><br/>\n",
    "$J(\\theta) =  \\dfrac{1}{2m}[\\sum_{i=1}^{m} (h_\\theta(x^{(i)})-y^{(i)})^2 + \\lambda\\sum_{j=1}^{n}\\theta_j^2]$\n",
    "<br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Neural Network</h1>\n",
    "<img src=\"img/1.png\" style=\"width: 500px;\"/>\n",
    "$a_i^{(j)}$ = activation of unit i in layer j\n",
    "<br/><br/>\n",
    "$\\Theta^{(j)}$ = weight matrix controlling mapping from layer $j$ to $j+1$\n",
    "<br/><br/>\n",
    "$a_1^{(2)} = g(\\Theta_{10}^{(1)}x_0 + \\Theta_{11}^{(1)}x_1 + \\Theta_{12}^{(1)}x_2 + \\Theta_{13}^{(1)}x_3)$\n",
    "<br/><br/>\n",
    "$a_2^{(2)} = g(\\Theta_{20}^{(1)}x_0 + \\Theta_{21}^{(1)}x_1 + \\Theta_{22}^{(1)}x_2 + \\Theta_{23}^{(1)}x_3)$\n",
    "<br/><br/>\n",
    "$a_3^{(2)} = g(\\Theta_{30}^{(1)}x_0 + \\Theta_{31}^{(1)}x_1 + \\Theta_{32}^{(1)}x_2 + \\Theta_{33}^{(1)}x_3)$\n",
    "<br/><br/>\n",
    "$h_{\\Theta}(x) = a_1^{(3)} = g(\\Theta_{10}^{(2)}x_0 + \\Theta_{11}^{(2)}x_1 + \\Theta_{12}^{(2)}x_2 + \\Theta_{13}^{(2)}x_3)$\n",
    "<br/><br/>\n",
    "$z^{(2)} = \\Theta^{(1)}a^{(1)}$\n",
    "<br/><br/>\n",
    "$a^{(2)} = g(z^{(2)})$\n",
    "<br/><br/>\n",
    "$z^{(3)} = \\Theta^{(2)}a^{(2)}$\n",
    "<br/><br/>\n",
    "$a^{(3)} = g(z^{(3)}) = h_{\\Theta}(x)$\n",
    "<br/>\n",
    "<h1>Cost function</h1>\n",
    "<br/>\n",
    "Logistic regression: $J(\\theta) = -\\frac{1}{m}[\\sum_{i=1}^{m} y^{(i)}logh_\\theta(x^{(i)}) + (1-y^{(i)})log(1-h_\\theta(x^{(i)}))] + \\frac{\\lambda}{2m}\\sum_{j=1}^{n}\\theta_j^2$\n",
    "<br/><br/>\n",
    "Neural network: $J(\\theta) = -\\frac{1}{m}[\\sum_{i=1}^{m}\\sum_{k=1}^{K} y_{k}^{(i)}logh_\\theta(x^{(i)})_{k} + (1-y_{k}^{(i)})log(1-h_\\theta(x^{(i)})_{k})] + \\frac{\\lambda}{2m}\\sum_{l=1}^{L-1}\\sum_{i=1}^{s_{l}}\\sum_{j=1}^{s_{l+1}}(\\theta_{j}^{(l)})^2$\n",
    "<br/><br/>\n",
    "Need to compute\n",
    "<br/><br/>\n",
    "$-J(\\Theta)$\n",
    "<br/><br/>\n",
    "$-\\frac{\\partial}{\\partial\\Theta_{ij}^{(l)}}J(\\Theta)$\n",
    "<img src=\"img/2.png\" style=\"width: 300px;\"/>\n",
    "$a^{(1)} = x$\n",
    "<br/><br/>\n",
    "$z^{(2)} = \\Theta^{(1)}a^{(1)}$\n",
    "<br/><br/>\n",
    "$a^{(2)} = g(z^{(2)})$ (add $a_{0}^{(2)}$)\n",
    "<br/><br/> \n",
    "$z^{(3)} = \\Theta^{(2)}a^{(2)}$\n",
    "<br/><br/>\n",
    "$a^{(3)} = g(z^{(3)})$ (add $a_{0}^{(3)}$)\n",
    "<br/><br/> \n",
    "$z^{(4)} = \\Theta^{(3)}a^{(3)}$\n",
    "<br/><br/>\n",
    "$a^{(4)} = g(z^{(3)}) = h_{\\Theta}(x)$\n",
    "<br/>\n",
    "<h2>Backpropagation</h2>\n",
    "<br/>\n",
    "$\\delta_{j}^{(l)}$ = error of node $j$ in layer $l$\n",
    "<br/><br/>\n",
    "$\\delta_{j}^{(4)} = a_{j}^{(4)} - y_{j}$\n",
    "<br/><br/>\n",
    "$\\delta_{j}^{(3)} = (\\Theta^{(3)})^{T}\\delta^{(4)}.*g^{\\prime}(z^{(3)})$\n",
    "<br/><br/>\n",
    "$\\delta_{j}^{(2)} = (\\Theta^{(2)})^{T}\\delta^{(3)}.*g^{\\prime}(z^{(2)})$\n",
    "<br/><br/>\n",
    "<h2>Gradient checking</h2>\n",
    "<br/>\n",
    "Compare $\\frac{\\partial}{\\partial\\Theta_{jk}^{(l)}}J(\\Theta)$ vs numerical estiamte of $J(\\Theta)$\n",
    "<br/><br/>\n",
    "Turn off during training, otherwise code runs very slow\n",
    "<br/><br/>\n",
    "Random initialization\n",
    "<br/>\n",
    "Symmetry breaking\n",
    "<br/>\n",
    "<h2>Putting all together</h2>\n",
    "<br/>\n",
    "1. Randomly initialize weights\n",
    "<br/>\n",
    "2. Compute forward prop to get $h_{\\Theta}(x^{(i)})$ for any $x^{(i)}$\n",
    "<br/>\n",
    "3. Compute cost $J(\\Theta)$\n",
    "<br/>\n",
    "4. Compute backward prop to get $\\frac{\\partial}{\\partial\\Theta_{jk}^{(l)}}J(\\Theta)$\n",
    "<br/>\n",
    "5. Iterate through $(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), ... , (x^{(m)}, y^{(m)})$ and do forward and backward prop for $(x^{(i)}, y^{(i)})$. Get $a^{(l)}$ and $\\delta^{(l)}$ for $l = 2, ... , L$\n",
    "<br/>\n",
    "6. Gradient checking\n",
    "<br/>\n",
    "7. Use gradient descent to minimize $J(\\Theta)$\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Deciding what to do</h1>\n",
    "<br/>\n",
    "Bias(underfit) : both $J_{train}(\\theta)$ and $J_{cv}(\\theta)$ are high\n",
    "<br/>\n",
    "Variance(overfit) : $J_{train}(\\theta)$ is low but $J_{cv}(\\theta)$ is high\n",
    "<br/>\n",
    "High Variance? Get more data, Try smaller set of features, increase learning rate $\\lambda$\n",
    "<br/>\n",
    "High Bias? Try adding polynomial features, try additional set of features, decrease learning rate $\\lambda$\n",
    "<br/><br/>\n",
    "In neural network, use regularization $\\lambda$ to overcome overfitting\n",
    "<br/><br/>\n",
    "Precision = true positive / (true postivie + false positive)\n",
    "<br/>\n",
    "Recall = true positive / (true postivie + false negative)\n",
    "<br/>\n",
    "We want to predict 1 when $h_{\\theta}(X) \\ge$ threshold\n",
    "<br/>\n",
    "F1 score = $2\\frac{PR}{P+R}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Support Vector Machine</h1>\n",
    "<br/>\n",
    "Alternative view of logistic regression\n",
    "<br/>\n",
    "$h_{\\theta}(x) = \\frac{1}{1+exp(-\\theta^{T}x)}$\n",
    "<br/>\n",
    "if $y = 1$, we want $h_{\\theta}(x) = 1, \\theta^{T}x >> 0$ or $ \\theta^{T}x \\ge 1$\n",
    "<br/>\n",
    "if $y = 0$, we want $h_{\\theta}(x) = 0, \\theta^{T}x << 0$ or $ \\theta^{T}x \\le -1$\n",
    "<br/>\n",
    "<h2>Cost function</h2>\n",
    "<br/>\n",
    "Logistic regression: $J(\\theta) = -\\frac{1}{m}[\\sum_{i=1}^{m} y^{(i)}logh_\\theta(x^{(i)}) + (1-y^{(i)})log(1-h_\\theta(x^{(i)}))] + \\frac{\\lambda}{2m}\\sum_{j=1}^{n}\\theta_j^2$\n",
    "<br/><br/>\n",
    "Support vector machine: $C\\frac{1}{m}[\\sum_{i=1}^{m} y^{(i)}cost_{1}(\\theta^{T}x^{(i)}) + (1-y^{(i)})cost_{0}(\\theta^{T}x^{(i)})] + \\frac{1}{2}\\sum_{j=1}^{n}\\theta_j^2$\n",
    "<br/><br/>\n",
    "Kernel : idea is to compute features based on proximity to landmarks $l^{(1)}, l^{(2)}, l^{(3)}, ...$\n",
    "<br/>\n",
    "$f_{1}$ = Similarity$(x, l^{(1)}) = exp(-\\frac{\\left\\|x-l^{(1)}\\right\\|^2}{2\\sigma^2})$\n",
    "<br/>\n",
    "If $x = l^(1) : f_{1} = 1$\n",
    "<br/>\n",
    "if $x$ is far from $l^(1) : f_{1} = 0$\n",
    "<br/><br/>\n",
    "Hypothesis : Given $x$, compute features $f$. Predict $y = 1$ if $\\theta^{T}x \\ge 0$\n",
    "<br/>\n",
    "Training : $C\\frac{1}{m}[\\sum_{i=1}^{m} y^{(i)}cost_{1}(\\theta^{T}f^{(i)}) + (1-y^{(i)})cost_{0}(\\theta^{T}f^{(i)})] + \\frac{1}{2}\\sum_{j=1}^{n}\\theta_j^2$\n",
    "<br/><br/>\n",
    "Parameters : Large $C$ or small $\\sigma^{2}$, low bias and high variance. Small $C$ or large $\\sigma^{2}$, high bias and low variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Clustering</h1>\n",
    "<br/>\n",
    "In supervised learning, training set : ${(x^{(1)}, y^{(1)}), ... , (x^{(m)}, y^{(m)})}$\n",
    "<br/>\n",
    "In unsupervised learning, training set : ${(x^{(1)}), ... , (x^{(m)})}$\n",
    "<br/>\n",
    "<h2>K-means algorithm</h2>\n",
    "<br/>\n",
    "Input : K(number of clusters) and training set ${(x^{(1)}), ... , (x^{(m)})}$\n",
    "<br/>\n",
    "Randomly initialize centroids : $\\mu_{1}, ... , \\mu_{K}$\n",
    "<br/>\n",
    "Repeat\n",
    "<br/>\n",
    "for 1 to m\n",
    "<br/>\n",
    "$c^{(i)}$ = index (from 1 to K) of cluster centroid closest to $x^{(i)}$\n",
    "<br/>\n",
    "for 1 to K\n",
    "<br/>\n",
    "$\\mu_{k}$ = average of points assigned to cluster $k$\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Principle Component Analysis</h1>\n",
    "<br/>\n",
    "Demension reduction technique\n",
    "<br/>\n",
    "Reduce from $n$ dimension to $k$ dimension : Find $k$ vectors $u^{(1)}, u^{(2)}, ... , u^{(k)}$ onto which to project the data, in order to minimize the error\n",
    "<br/>\n",
    "<br/>\n",
    "Training Set : $x^{(1)}, x^{(2)}, ... , x^{(m)}$\n",
    "<br/>\n",
    "Preprocessing - feature scaling and mean normalization\n",
    "<br/>\n",
    "Compute \"covariance matrix\"\n",
    "<br/>\n",
    "$\\Sigma = \\frac{1}{m}\\sum_{i=1}^{n}(x^{(i)})(x^{(i)})^{T}$\n",
    "<br/>\n",
    "Compute \"eigenvectors of matrix $\\Sigma$\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
