{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Introduction</h1>\n",
    "Supervised Leanring : we are given a data set, already know what our correct outputs are. Ex. Regression and Classification<br/>\n",
    "Regression : continuous output. Map input to continuous function<br/>\n",
    "Classification : discrete output. Map input to discrete categories<br/><br/>\n",
    "Unsupervised learning : we have little or no idea what our correct outputs are. We derive structure from data<br/><br/>\n",
    "Reinforcement learning : TBD<br/><br/>\n",
    "Recommender system : TBD<br/><br/>\n",
    "<h1>Regression with two variables</h1>\n",
    "$m$ = number of training examples<br/>\n",
    "$x$ = inputs<br/>\n",
    "$y$ = outputs<br/>\n",
    "$h$ = hypothesis<br/>\n",
    "$\\theta$ = parameter<br/>\n",
    "Hypothesis : $h_\\theta(x) = \\theta_0 + \\theta_1 x$<br/><br/>\n",
    "Paramaters : $\\theta_0, \\theta_1$<br/><br/>\n",
    "Cost : $J(\\theta_0, \\theta_1) =  \\dfrac{1}{2m}\\sum_{i=1}^{m} (h_\\theta(x^{(i)})-y^{(i)})^2$<br/><br/>\n",
    "Objective : Min $J(\\theta_0, \\theta_1)$<br/><br/> \n",
    "Gradient descent<br/>\n",
    "Start with some $\\theta_0, \\theta_1$<br/>\n",
    "Keep changing $\\theta_0, \\theta_1$ to reduce $J(\\theta_0, \\theta_1)$ until reaching minimum<br/>\n",
    "Repeat until convergence (Simultaneous update all j)<br/><br/>\n",
    "$\\theta_j := \\theta_j - \\alpha\\frac{\\partial}{\\partial \\theta_j}J(\\theta_0, \\theta_1)$<br/>\n",
    "Small $\\alpha$ ? slow<br/>\n",
    "Large $\\alpha$ ? may diverge<br/>\n",
    "No need to decrease $\\alpha$ since gradient descent will automatically take smaller steps as it reaches towards optimum<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Regression with Multiple variable</h1>\n",
    "Hypothesis : $h_\\theta(x) = \\theta^{T}x$<br/><br/>\n",
    "Paramaters : $\\theta$<br/><br/>\n",
    "Cost : $J(\\theta) =  \\dfrac{1}{2m}\\sum_{i=1}^{m} (h_\\theta(x^{(i)})-y^{(i)})^2$<br/><br/>\n",
    "Objective : Min $J(\\theta)$<br/><br/> \n",
    "Repeat until convergence (Simultaneous update all j)<br/><br/>\n",
    "$\\theta_j := \\theta_j - \\alpha\\frac{\\partial}{\\partial \\theta_j}J(\\theta)$<br/><br/>\n",
    "Feature Scaling : Make sure feature scales are between -1 and +1<br/>\n",
    "Mean normalization : Shift $x_i$ to $x_i - \\mu_i$ so that means are zero<br/><br/>\n",
    "Normal Equation : method to solve $\\theta$ analytically<br/><br/>\n",
    "Solve for $\\theta_0, \\theta_1, ... , \\theta_m$<br/><br/>\n",
    "where $J(\\theta_0, \\theta_1, ... , \\theta_m) =  \\dfrac{1}{2m}\\sum_{i=1}^{m} (h_\\theta(x^{(i)})-y^{(i)})^2$ and $\\frac{\\partial}{\\partial \\theta_j}J(\\theta)$ = 0 for all j<br/><br/>\n",
    "$\\theta = (X^{T}X)^{-1}X^{T}y$<br/><br/>\n",
    "If $X^{T}X$ is non-invertible?<br/>\n",
    "Redundant features (linearly dependent) or too many features (n > m)<br/><br/>\n",
    "Gradient descent - need to choose $\\alpha$, need to iterate, works when n is large<br/>\n",
    "Normal equation - no need to choose $\\alpha$, no need to iterate, slow when n is large<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Logistic regression : classification</h1>\n",
    "threshold classifier output<br/>\n",
    "If $h_\\theta(x)$ $\\ge$ 0.5, then y = 1<br/>\n",
    "If $h_\\theta(x)$ < 0.5, then y = 0<br/>\n",
    "We want to model $h_\\theta(x)$ as probability such that 0 $\\le$ $h_\\theta(x)$ $\\le$ 1<br/>\n",
    "Let $h_\\theta(x)$ = $g(\\theta^{T}X)$ where $g(z) = \\frac{1}{1 + e^{-z}}$, then $h_\\theta(x) = \\frac{1}{1 + e^{-\\theta^{T}X}}$ (sigmoid function)<br/>  \n",
    "Logistic regression cost function<br/>\n",
    "$J(\\theta) = -\\frac{1}{m}[\\sum_{i=1}^{m} y^{(i)}logh_\\theta(x^{(i)}) + (1-y^{(i)})log(1-h_\\theta(x^{(i)}))]$<br/>\n",
    "Advanced Gradient descent : conjugate gradient / BFGS / L-BFGS<br/>\n",
    "One vs All<br/>\n",
    "Train a logistic regression classifier $h_\\theta^{(i)}(x)$ for each class i to predict probability that $y = i$<br/>\n",
    "Pick the class that maximizes $h_\\theta^{(i)}(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem of overfitting - too many features fitting training examples very well but fail to generalize for new examples<br/>\n",
    "Regularization - reduce magnitude of parameters<br/>\n",
    "$J(\\theta) =  \\dfrac{1}{2m}[\\sum_{i=1}^{m} (h_\\theta(x^{(i)})-y^{(i)})^2 + \\lambda\\sum_{j=1}^{n}\\theta_j^2]$<br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Neural Network</h1>\n",
    "<img src=\"img/1.png\" style=\"width: 500px;\"/>\n",
    "$a_i^{(j)}$ = activation of unit i in layer j<br/>\n",
    "$\\Theta^{(j)}$ = weight matrix controlling mapping from layer $j$ to $j+1$<br/><br/>\n",
    "$a_1^{(2)} = g(\\Theta_{10}^{(1)}x_0 + \\Theta_{11}^{(1)}x_1 + \\Theta_{12}^{(1)}x_2 + \\Theta_{13}^{(1)}x_3)$<br/>\n",
    "$a_2^{(2)} = g(\\Theta_{20}^{(1)}x_0 + \\Theta_{21}^{(1)}x_1 + \\Theta_{22}^{(1)}x_2 + \\Theta_{23}^{(1)}x_3)$<br/>\n",
    "$a_3^{(2)} = g(\\Theta_{30}^{(1)}x_0 + \\Theta_{31}^{(1)}x_1 + \\Theta_{32}^{(1)}x_2 + \\Theta_{33}^{(1)}x_3)$<br/>\n",
    "$h_{\\Theta}(x) = a_1^{(3)} = g(\\Theta_{10}^{(2)}x_0 + \\Theta_{11}^{(2)}x_1 + \\Theta_{12}^{(2)}x_2 + \\Theta_{13}^{(2)}x_3)$<br/><br/>\n",
    "$z^{(2)} = \\Theta^{(1)}a^{(1)}$<br/>\n",
    "$a^{(2)} = g(z^{(2)})$<br/>\n",
    "$z^{(3)} = \\Theta^{(2)}a^{(2)}$<br/>\n",
    "$a^{(3)} = g(z^{(3)}) = h_{\\Theta}(x)$<br/>\n",
    "Cost function<br/>\n",
    "Logistic regression: $J(\\theta) = -\\frac{1}{m}[\\sum_{i=1}^{m} y^{(i)}logh_\\theta(x^{(i)}) + (1-y^{(i)})log(1-h_\\theta(x^{(i)}))] + \\frac{\\lambda}{2m}\\sum_{j=1}^{n}\\theta_j^2$<br/><br/>\n",
    "Neural network: $J(\\theta) = -\\frac{1}{m}[\\sum_{i=1}^{m}\\sum_{k=1}^{K} y_{k}^{(i)}logh_\\theta(x^{(i)})_{k} + (1-y_{k}^{(i)})log(1-h_\\theta(x^{(i)})_{k})] + \\frac{\\lambda}{2m}\\sum_{l=1}^{L-1}\\sum_{i=1}^{s_{l}}\\sum_{j=1}^{s_{l+1}}(\\theta_{j}^{(l)})^2$<br/><br/>\n",
    "Need to compute<br/>\n",
    "$-J(\\Theta)$<br/>\n",
    "$-\\frac{\\partial}{\\partial\\Theta_{ij}^{(l)}}J(\\Theta)$<br/><br/>\n",
    "<img src=\"img/2.png\" style=\"width: 300px;\"/>\n",
    "$a^{(1)} = x$<br/>\n",
    "$z^{(2)} = \\Theta^{(1)}a^{(1)}$<br/>\n",
    "$a^{(2)} = g(z^{(2)})$ (add $a_{0}^{(2)}$)<br/> \n",
    "$z^{(3)} = \\Theta^{(2)}a^{(2)}$<br/>\n",
    "$a^{(3)} = g(z^{(3)})$ (add $a_{0}^{(3)}$)<br/> \n",
    "$z^{(4)} = \\Theta^{(3)}a^{(3)}$<br/>\n",
    "$a^{(4)} = g(z^{(3)}) = h_{\\Theta}(x)$<br/><br/>\n",
    "Backpropagation<br/>\n",
    "$\\delta_{j}^{(l)}$ = error of node $j$ in layer $l$<br/>\n",
    "$\\delta_{j}^{(4)} = a_{j}^{(4)} - y_{j}$<br/>\n",
    "$\\delta_{j}^{(3)} = (\\Theta^{(3)})^{T}\\delta^{(4)}.*g^{\\prime}(z^{(3)})$<br/>\n",
    "$\\delta_{j}^{(2)} = (\\Theta^{(2)})^{T}\\delta^{(3)}.*g^{\\prime}(z^{(2)})$<br/><br/>\n",
    "Gradient checking<br/>\n",
    "Compare $\\frac{\\partial}{\\partial\\Theta_{jk}^{(l)}}J(\\Theta)$ vs numerical estiamte of $J(\\Theta)$<br/>\n",
    "Turn off during training, otherwise code runs very slow<br/><br/>\n",
    "Random initialization<br/>\n",
    "Symmetry breaking<br/><br/>\n",
    "Putting all together<br/>\n",
    "1. Randomly initialize weights<br/>\n",
    "2. Compute forward prop to get $h_{\\Theta}(x^{(i)})$ for any $x^{(i)}$<br/>\n",
    "3. Compute cost $J(\\Theta)$<br/>\n",
    "4. Compute backward prop to get $\\frac{\\partial}{\\partial\\Theta_{jk}^{(l)}}J(\\Theta)$<br/>\n",
    "5. Iterate through $(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), ... , (x^{(m)}, y^{(m)})$<br/>\n",
    "do forward and backward prop for $(x^{(i)}, y^{(i)})$. Get $a^{(l)}$ and $\\delta^{(l)}$ for $l = 2, ... , L$<br/>\n",
    "6. Gradient checking<br/>\n",
    "7. Use gradient descent to minimize $J(\\Theta)$<br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Deciding what to do</h1>\n",
    "Bias(underfit) : both $J_{train}(\\theta)$ and $J_{cv}(\\theta)$ are high<br/>\n",
    "Variance(overfit) : $J_{train}(\\theta)$ is low but $J_{cv}(\\theta)$ is high<br/>\n",
    "High Variance? Get more data, Try smaller set of features, increase learning rate $\\lambda$<br/>\n",
    "High Bias? Try adding polynomial features, try additional set of features, decrease learning rate $\\lambda$<br/>\n",
    "In neural network, use regularization $\\lambda$ to overcome overfitting<br/>\n",
    "Precision = true positive / (true postivie + false positive)<br/>\n",
    "Recall = true positive / (true postivie + false negative)<br/>\n",
    "We want to predict 1 when $h_{\\theta}(X) \\ge$ threshold<br/>\n",
    "F1 score = $2\\frac{PR}{P+R}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Support Vector Machine</h1>\n",
    "Alternative view of logistic regression<br/>\n",
    "$h_{\\theta}(x) = \\frac{1}{1+exp(-\\theta^{T}x)}$<br/>\n",
    "if $y = 1$, we want $h_{\\theta}(x) = 1, \\theta^{T}x >> 0$ or $ \\theta^{T}x \\ge 1$<br/>\n",
    "if $y = 0$, we want $h_{\\theta}(x) = 0, \\theta^{T}x << 0$ or $ \\theta^{T}x \\le -1$<br/>\n",
    "Cost function<br/>\n",
    "Logistic regression: $J(\\theta) = -\\frac{1}{m}[\\sum_{i=1}^{m} y^{(i)}logh_\\theta(x^{(i)}) + (1-y^{(i)})log(1-h_\\theta(x^{(i)}))] + \\frac{\\lambda}{2m}\\sum_{j=1}^{n}\\theta_j^2$<br/><br/>\n",
    "Support vector machine: $C\\frac{1}{m}[\\sum_{i=1}^{m} y^{(i)}cost_{1}(\\theta^{T}x^{(i)}) + (1-y^{(i)})cost_{0}(\\theta^{T}x^{(i)})] + \\frac{1}{2}\\sum_{j=1}^{n}\\theta_j^2$<br/><br/>\n",
    "Kernel : idea is to compute features based on proximity to landmarks $l^{(1)}, l^{(2)}, l^{(3)}, ...$<br/>\n",
    "$f_{1}$ = Similarity$(x, l^{(1)}) = exp(-\\frac{\\left\\|x-l^{(1)}\\right\\|^2}{2\\sigma^2})$<br/>\n",
    "If $x = l^(1) : f_{1} = 1$<br/>\n",
    "if $x$ is far from $l^(1) : f_{1} = 0$<br/><br/>\n",
    "Hypothesis : Given $x$, compute features $f$. Predict $y = 1$ if $\\theta^{T}x \\ge 0$<br/>\n",
    "Training : $C\\frac{1}{m}[\\sum_{i=1}^{m} y^{(i)}cost_{1}(\\theta^{T}f^{(i)}) + (1-y^{(i)})cost_{0}(\\theta^{T}f^{(i)})] + \\frac{1}{2}\\sum_{j=1}^{n}\\theta_j^2$<br/><br/>\n",
    "Parameters : Large $C$ or small $\\sigma^{2}$, low bias and high variance. Small $C$ or large $\\sigma^{2}$, high bias and low variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Clustering</h1>\n",
    "In supervised learning, training set : ${(x^{(1)}, y^{(1)}), ... , (x^{(m)}, y^{(m)})}$<br/>\n",
    "In unsupervised learning, training set : ${(x^{(1)}), ... , (x^{(m)})}$<br/>\n",
    "K-means algorithm<br/>\n",
    "Input : K(number of clusters) and training set ${(x^{(1)}), ... , (x^{(m)})}$<br/>\n",
    "Randomly initialize centroids : $\\mu_{1}, ... , \\mu_{K}$<br/>\n",
    "Repeat<br/>\n",
    "for 1 to m<br/>\n",
    "$c^{(i)}$ = index (from 1 to K) of cluster centroid closest to $x^{(i)}$<br/>\n",
    "for 1 to K<br/>\n",
    "$\\mu_{k}$ = average of points assigned to cluster $k$<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Principle Component Analysis</h1><br/>\n",
    "Demension reduction technique<br/>\n",
    "Reduce from $n$ dimension to $k$ dimension : Find $k$ vectors $u^{(1)}, u^{(2)}, ... , u^{(k)}$ onto which to project the data, in order to minimize  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
