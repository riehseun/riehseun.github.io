{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Introduction</h1>\n",
    "<br/>\n",
    "Supervised Leanring : we are given a data set, already know what our correct outputs are. Ex. Regression and Classification\n",
    "<br/>\n",
    "- Regression : continuous output. Map input to continuous function\n",
    "<br/>\n",
    "- Classification : discrete output. Map input to discrete categories\n",
    "<br/><br/>\n",
    "Unsupervised learning : we have little or no idea what our correct outputs are. We derive structure from data\n",
    "<br/><br/>\n",
    "Reinforcement learning : TBD\n",
    "<br/><br/>\n",
    "Recommender system : TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Regression with two variables</h1>\n",
    "<br/>\n",
    "$m$ = number of training examples\n",
    "<br/>\n",
    "$x$ = inputs\n",
    "<br/>\n",
    "$y$ = outputs\n",
    "<br/>\n",
    "$h$ = hypothesis\n",
    "<br/>\n",
    "$\\theta$ = parameter\n",
    "<br/><br/>\n",
    "Hypothesis : $h_\\theta(x) = \\theta_{0} + \\theta_{1}x$\n",
    "<br/><br/>\n",
    "Paramaters : $\\theta_{0}, \\theta_{1}$\n",
    "<br/><br/>\n",
    "Cost : $J(\\theta_{0}, \\theta_{1}) = \\dfrac{1}{2m}\\displaystyle\\sum_{i=1}^{m}\\left(h_\\theta(x^{(i)})-y^{(i)}\\right)^2$\n",
    "<br/><br/>\n",
    "Objective : $\\displaystyle\\min J(\\theta_{0}, \\theta_{1})$\n",
    "<br/><br/> \n",
    "<h2>Gradient descent</h2>\n",
    "<br/>\n",
    "Start with some $\\theta_{0}, \\theta_{1}$\n",
    "<br/><br/>\n",
    "Keep changing $\\theta_{0}, \\theta_{1}$ to reduce $J(\\theta_{0}, \\theta_{1})$ until reaching minimum\n",
    "<br/><br/>\n",
    "Repeat until convergence (Simultaneous update all j)\n",
    "<br/><br/>\n",
    "$\\theta_{j} := \\theta_{j} - \\alpha\\dfrac{\\partial}{\\partial \\theta_{j}}J(\\theta_{0}, \\theta_{1})$\n",
    "<br/><br/>\n",
    "Small $\\alpha$ ? slow\n",
    "<br/><br/>\n",
    "Large $\\alpha$ ? may diverge\n",
    "<br/><br/>\n",
    "No need to decrease $\\alpha$ since gradient descent will automatically take smaller steps as it reaches towards optimum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Regression with Multiple variable</h1>\n",
    "<br/>\n",
    "Hypothesis : $h_\\theta(x) = \\theta^{T}x$\n",
    "<br/><br/>\n",
    "Paramaters : $\\theta$\n",
    "<br/><br/>\n",
    "Cost : $J(\\theta) =  \\dfrac{1}{2m}\\displaystyle\\sum_{i=1}^{m}\\left(h_\\theta(x^{(i)})-y^{(i)}\\right)^2$\n",
    "<br/><br/>\n",
    "Objective : Min $J(\\theta)$\n",
    "<br/><br/> \n",
    "<h2>Gradient descent</h2>\n",
    "<br/>\n",
    "Repeat until convergence (Simultaneous update all j)\n",
    "<br/><br/>\n",
    "$\\theta_{j} := \\theta_{j} - \\alpha\\dfrac{\\partial}{\\partial \\theta_{j}}J(\\theta)$\n",
    "<br/><br/>\n",
    "Feature Scaling : Make sure feature scales are between -1 and +1\n",
    "<br/><br/>\n",
    "Mean normalization : Shift $x_{i}$ to $x_{i} - \\mu_{i}$ so that means are zero\n",
    "<br/><br/>\n",
    "<h2>Normal Equation</h2>\n",
    "<br/>\n",
    "Method to solve $\\theta$ analytically\n",
    "<br/><br/>\n",
    "Solve for $\\theta_{0}, \\theta_{1} \\dots \\theta_{m}$\n",
    "<br/><br/>\n",
    "where $J(\\theta_{0}, \\theta_{1} \\dots \\theta_{m}) =  \\dfrac{1}{2m}\\displaystyle\\sum_{i=1}^{m} (h_\\theta(x^{(i)})-y^{(i)})^2$ and $\\dfrac{\\partial}{\\partial \\theta_{j}}J(\\theta)$ = 0 for all j\n",
    "<br/><br/>\n",
    "Solution is $\\theta = (X^{T}X)^{-1}X^{T}y$\n",
    "<br/><br/>\n",
    "If $X^{T}X$ is non-invertible? Redundant features (linearly dependent) or too many features (n > m)\n",
    "<br/><br/>\n",
    "Gradient descent - need to choose $\\alpha$, need to iterate, works when n is large\n",
    "<br/>\n",
    "Normal equation - no need to choose $\\alpha$, no need to iterate, slow when n is large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Logistic regression : classification</h1>\n",
    "<br/>\n",
    "Threshold classifier output\n",
    "<br/>\n",
    "If $h_\\theta(x)$ $\\ge$ 0.5, then y = 1\n",
    "<br/>\n",
    "If $h_\\theta(x)$ < 0.5, then y = 0\n",
    "<br/>\n",
    "We want to model $h_\\theta(x)$ as probability such that 0 $\\le$ $h_\\theta(x)$ $\\le$ 1\n",
    "<br/><br/>\n",
    "Let $h_\\theta(x)$ = $g(\\theta^{T}X)$ where $g(z) = \\dfrac{1}{1 + e^{-z}}$, then $h_\\theta(x) = \\dfrac{1}{1 + e^{-\\theta^{T}X}}$ (sigmoid function)\n",
    "<br/><br/>  \n",
    "Logistic regression cost function\n",
    "<br/><br/>\n",
    "$J(\\theta) = -\\dfrac{1}{m}\\left[\\displaystyle\\sum_{i=1}^{m} y^{(i)}logh_\\theta(x^{(i)}) + (1-y^{(i)})log(1-h_\\theta(x^{(i)}))\\right]$\n",
    "<br/><br/>\n",
    "<h2>Advanced Gradient descent</h2>\n",
    "<br/>\n",
    "conjugate gradient / BFGS / L-BFGS\n",
    "<br/>\n",
    "<h2>One vs All</h2>\n",
    "<br/>\n",
    "Train a logistic regression classifier $h_\\theta^{(i)}(x)$ for each class i to predict probability that $y = i$\n",
    "<br/>\n",
    "Pick the class that maximizes $h_\\theta^{(i)}(x)$\n",
    "<br/>\n",
    "\n",
    "<h2>Problem of overfitting</h2>\n",
    "<br/>\n",
    "Too many features fitting training examples very well but fail to generalize for new examples\n",
    "<br/>\n",
    "Regularization - reduce magnitude of parameters\n",
    "<br/><br/>\n",
    "$J(\\theta) =  \\dfrac{1}{2m}\\left[\\displaystyle\\sum_{i=1}^{m} (h_\\theta(x^{(i)})-y^{(i)})^2 + \\lambda\\displaystyle\\sum_{j=1}^{n}\\theta_j^2\\right]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Neural Network</h1>\n",
    "<img src=\"img/1.png\" style=\"width: 500px;\"/>\n",
    "$a_i^{(j)}$ = activation of unit i in layer j\n",
    "<br/><br/>\n",
    "$\\Theta^{(j)}$ = weight matrix controlling mapping from layer $j$ to $j+1$\n",
    "<br/><br/>\n",
    "$a_1^{(2)} = g\\left(\\Theta_{10}^{(1)}x_0 + \\Theta_{11}^{(1)}x_1 + \\Theta_{12}^{(1)}x_2 + \\Theta_{13}^{(1)}x_3\\right)$\n",
    "<br/><br/>\n",
    "$a_2^{(2)} = g\\left(\\Theta_{20}^{(1)}x_0 + \\Theta_{21}^{(1)}x_1 + \\Theta_{22}^{(1)}x_2 + \\Theta_{23}^{(1)}x_3\\right)$\n",
    "<br/><br/>\n",
    "$a_3^{(2)} = g\\left(\\Theta_{30}^{(1)}x_0 + \\Theta_{31}^{(1)}x_1 + \\Theta_{32}^{(1)}x_2 + \\Theta_{33}^{(1)}x_3\\right)$\n",
    "<br/><br/>\n",
    "$h_{\\Theta}(x) = a_1^{(3)} = g\\left(\\Theta_{10}^{(2)}x_0 + \\Theta_{11}^{(2)}x_1 + \\Theta_{12}^{(2)}x_2 + \\Theta_{13}^{(2)}x_3\\right)$\n",
    "<br/><br/>\n",
    "$z^{(2)} = \\Theta^{(1)}a^{(1)}$\n",
    "<br/><br/>\n",
    "$a^{(2)} = g(z^{(2)})$\n",
    "<br/><br/>\n",
    "$z^{(3)} = \\Theta^{(2)}a^{(2)}$\n",
    "<br/><br/>\n",
    "$a^{(3)} = g(z^{(3)}) = h_{\\Theta}(x)$\n",
    "<br/>\n",
    "\n",
    "<h1>Cost function</h1>\n",
    "<br/>\n",
    "Logistic regression: $J(\\theta) = -\\frac{1}{m}\\left[\\displaystyle\\sum_{i=1}^{m} y^{(i)}logh_\\theta(x^{(i)}) + (1-y^{(i)})log(1-h_\\theta(x^{(i)}))\\right] + \\dfrac{\\lambda}{2m}\\displaystyle\\sum_{j=1}^{n}\\theta_j^2$\n",
    "<br/><br/>\n",
    "Neural network: $J(\\theta) = -\\frac{1}{m}\\left[\\displaystyle\\sum_{i=1}^{m}\\displaystyle\\sum_{k=1}^{K} y_{k}^{(i)}logh_\\theta(x^{(i)})_{k} + (1-y_{k}^{(i)})log(1-h_\\theta(x^{(i)})_{k})\\right] + \\dfrac{\\lambda}{2m}\\displaystyle\\sum_{l=1}^{L-1}\\displaystyle\\sum_{i=1}^{s_{l}}\\displaystyle\\sum_{j=1}^{s_{l+1}}(\\theta_{j}^{(l)})^2$\n",
    "<br/><br/>\n",
    "Need to compute\n",
    "<br/><br/>\n",
    "$-J(\\Theta)$\n",
    "<br/><br/>\n",
    "$-\\dfrac{\\partial}{\\partial\\Theta_{ij}^{(l)}}J(\\Theta)$\n",
    "<img src=\"img/2.png\" style=\"width: 300px;\"/>\n",
    "$a^{(1)} = x$\n",
    "<br/><br/>\n",
    "$z^{(2)} = \\Theta^{(1)}a^{(1)}$\n",
    "<br/><br/>\n",
    "$a^{(2)} = g(z^{(2)})$ (add $a_{0}^{(2)}$)\n",
    "<br/><br/> \n",
    "$z^{(3)} = \\Theta^{(2)}a^{(2)}$\n",
    "<br/><br/>\n",
    "$a^{(3)} = g(z^{(3)})$ (add $a_{0}^{(3)}$)\n",
    "<br/><br/> \n",
    "$z^{(4)} = \\Theta^{(3)}a^{(3)}$\n",
    "<br/><br/>\n",
    "$a^{(4)} = g(z^{(3)}) = h_{\\Theta}(x)$\n",
    "<br/>\n",
    "<h2>Backpropagation</h2>\n",
    "<br/>\n",
    "$\\delta_{j}^{(l)}$ = error of node $j$ in layer $l$\n",
    "<br/><br/>\n",
    "$\\delta_{j}^{(4)} = a_{j}^{(4)} - y_{j}$\n",
    "<br/><br/>\n",
    "$\\delta_{j}^{(3)} = (\\Theta^{(3)})^{T}\\delta^{(4)}.*g^{\\prime}(z^{(3)})$\n",
    "<br/><br/>\n",
    "$\\delta_{j}^{(2)} = (\\Theta^{(2)})^{T}\\delta^{(3)}.*g^{\\prime}(z^{(2)})$\n",
    "<br/><br/>\n",
    "<h2>Gradient checking</h2>\n",
    "<br/>\n",
    "Compare $\\dfrac{\\partial}{\\partial\\Theta_{jk}^{(l)}}J(\\Theta)$ vs numerical estiamte of $J(\\Theta)$\n",
    "<br/><br/>\n",
    "Turn off during training, otherwise code runs very slow\n",
    "<br/><br/>\n",
    "Random initialization\n",
    "<br/>\n",
    "Symmetry breaking\n",
    "<br/>\n",
    "<h2>Putting all together</h2>\n",
    "<br/>\n",
    "1. Randomly initialize weights\n",
    "<br/>\n",
    "2. Compute forward prop to get $h_{\\Theta}(x^{(i)})$ for any $x^{(i)}$\n",
    "<br/>\n",
    "3. Compute cost $J(\\Theta)$\n",
    "<br/>\n",
    "4. Compute backward prop to get $\\dfrac{\\partial}{\\partial\\Theta_{jk}^{(l)}}J(\\Theta)$\n",
    "<br/>\n",
    "5. Iterate through $(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}) \\dots (x^{(m)}, y^{(m)})$ and do forward and backward prop for $(x^{(i)}, y^{(i)})$. Get $a^{(l)}$ and $\\delta^{(l)}$ for $l = 2 \\dots L$\n",
    "<br/>\n",
    "6. Gradient checking\n",
    "<br/>\n",
    "7. Use gradient descent to minimize $J(\\Theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Deciding what to do</h1>\n",
    "<br/>\n",
    "Bias(underfit) : both $J_{train}(\\theta)$ and $J_{cv}(\\theta)$ are high\n",
    "<br/>\n",
    "Variance(overfit) : $J_{train}(\\theta)$ is low but $J_{cv}(\\theta)$ is high\n",
    "<br/>\n",
    "High Variance? Get more data, Try smaller set of features, increase learning rate $\\lambda$\n",
    "<br/>\n",
    "High Bias? Try adding polynomial features, try additional set of features, decrease learning rate $\\lambda$\n",
    "<br/><br/>\n",
    "In neural network, use regularization $\\lambda$ to overcome overfitting\n",
    "<br/><br/>\n",
    "Precision = true positive / (true postivie + false positive)\n",
    "<br/>\n",
    "Recall = true positive / (true postivie + false negative)\n",
    "<br/>\n",
    "We want to predict 1 when $h_{\\theta}(X) \\ge$ threshold\n",
    "<br/>\n",
    "F1 score = $2\\dfrac{PR}{P+R}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Support Vector Machine</h1>\n",
    "<br/>\n",
    "Alternative view of logistic regression\n",
    "<br/>\n",
    "$h_{\\theta}(x) = \\dfrac{1}{1+exp(-\\theta^{T}x)}$\n",
    "<br/><br/>\n",
    "if $y = 1$, we want $h_{\\theta}(x) = 1, \\theta^{T}x >> 0$ or $ \\theta^{T}x \\ge 1$\n",
    "<br/>\n",
    "if $y = 0$, we want $h_{\\theta}(x) = 0, \\theta^{T}x << 0$ or $ \\theta^{T}x \\le -1$\n",
    "<br/>\n",
    "<h2>Cost function</h2>\n",
    "<br/>\n",
    "Logistic regression: $J(\\theta) = -\\frac{1}{m}\\left[\\displaystyle\\sum_{i=1}^{m} y^{(i)}logh_\\theta(x^{(i)}) + (1-y^{(i)})log(1-h_\\theta(x^{(i)}))\\right] + \\frac{\\lambda}{2m}\\displaystyle\\sum_{j=1}^{n}\\theta_j^2$\n",
    "<br/><br/>\n",
    "Support vector machine: $C\\frac{1}{m}\\left[\\displaystyle\\sum_{i=1}^{m} y^{(i)}cost_{1}(\\theta^{T}x^{(i)}) + (1-y^{(i)})cost_{0}(\\theta^{T}x^{(i)})\\right] + \\frac{1}{2}\\displaystyle\\sum_{j=1}^{n}\\theta_j^2$\n",
    "<br/><br/>\n",
    "Kernel : idea is to compute features based on proximity to landmarks $l^{(1)}, l^{(2)}, l^{(3)} \\dots$\n",
    "<br/>\n",
    "$f_{1}$ = Similarity$(x, l^{(1)}) = exp\\left(-\\dfrac{\\left\\|x-l^{(1)}\\right\\|^2}{2\\sigma^2}\\right)$\n",
    "<br/>\n",
    "If $x$ is close to $l^{(1)} : f_{1} = 1$\n",
    "<br/>\n",
    "if $x$ is far from $l^{(1)} : f_{1} = 0$\n",
    "<br/><br/>\n",
    "Hypothesis : Given $x$, compute features $f$. Predict $y = 1$ if $\\theta^{T}x \\ge 0$\n",
    "<br/>\n",
    "Training : $C\\dfrac{1}{m}\\left[\\displaystyle\\sum_{i=1}^{m} y^{(i)}cost_{1}(\\theta^{T}f^{(i)}) + (1-y^{(i)})cost_{0}(\\theta^{T}f^{(i)})\\right] + \\dfrac{1}{2}\\displaystyle\\sum_{j=1}^{n}\\theta_j^2$\n",
    "<br/><br/>\n",
    "Parameters : Large $C$ or small $\\sigma^{2}$, low bias and high variance. Small $C$ or large $\\sigma^{2}$, high bias and low variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Clustering</h1>\n",
    "<br/>\n",
    "In supervised learning, training set : ${(x^{(1)}, y^{(1)}) \\dots (x^{(m)}, y^{(m)})}$\n",
    "<br/>\n",
    "In unsupervised learning, training set : ${x^{(1)} \\dots x^{(m)}}$\n",
    "<br/>\n",
    "<h2>K-means algorithm</h2>\n",
    "<br/>\n",
    "Input : K(number of clusters) and training set ${x^{(1)} \\dots x^{(m)}}$\n",
    "<br/>\n",
    "Randomly initialize centroids : $\\mu_{1} \\dots \\mu_{K}$\n",
    "<br/>\n",
    "Repeat\n",
    "<br/>\n",
    "for 1 to m\n",
    "<br/>\n",
    "$c^{(i)}$ = index (from 1 to K) of cluster centroid closest to $x^{(i)}$\n",
    "<br/>\n",
    "for 1 to K\n",
    "<br/>\n",
    "$\\mu_{k}$ = average of points assigned to cluster $k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Principle Component Analysis</h1>\n",
    "<br/>\n",
    "Demension reduction technique\n",
    "<br/>\n",
    "Reduce from $n$ dimension to $k$ dimension : Find $k$ vectors $u^{(1)} \\dots u^{(k)}$ onto which to project the data, in order to minimize the error\n",
    "<br/>\n",
    "<br/>\n",
    "Training Set : $x^{(1)} \\dots x^{(m)}$\n",
    "<br/>\n",
    "Preprocessing - feature scaling and mean normalization\n",
    "<br/>\n",
    "Compute \"covariance matrix\"\n",
    "<br/>\n",
    "$\\Sigma = \\dfrac{1}{m}\\displaystyle\\sum_{i=1}^{n}(x^{(i)})(x^{(i)})^{T}$\n",
    "<br/>\n",
    "Compute \"eigenvectors of matrix $\\Sigma$\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Anomaly Detection</h1>\n",
    "<br/>\n",
    "Anamoly detection algorithm\n",
    "<br/>\n",
    "1. Choose feature $x_{i}$ that you think is anamoly\n",
    "<br/>\n",
    "2. Fit parameters $\\mu_{1} \\dots \\mu_{n}$ and $\\sigma_{1}^{2} \\dots \\sigma_{n}^{2}$ of Gaussian distribution\n",
    "<br/>\n",
    "$\\mu_{j} = \\dfrac{1}{m}\\displaystyle\\sum_{i=1}^{m}x_{j}^{(i)}$, $\\sigma_{j}^{2} = \\dfrac{1}{m}\\displaystyle\\sum_{i=1}^{m}\\left(x_{j}^{(i)}-\\mu_{j}\\right)^{2}$\n",
    "<br/>\n",
    "3. Compute $p(x) = \\displaystyle\\prod_{j=1}^{n}p(x_{j};\\mu_{j},\\sigma_{j}^{2})$ \n",
    "<br/>\n",
    "4. Anamoly if $p(x) < \\epsilon$\n",
    "<br/><br/>\n",
    "Anamoly detection vs supervised learning\n",
    "<br/>\n",
    "- very small number of positive examples / large number of positive and negative examples\n",
    "<br/>\n",
    "- anamolies are very unique from one another / positive examples are similar to one another\n",
    "<br/>\n",
    "- Ex. fraud detection / Ex. email spam classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Recommender Systems</h1>\n",
    "<br/>\n",
    "<h2>Problem formulation</h2>\n",
    "<br/>\n",
    "$r(i,j) = 1$ if user $i$ rated movie $j$ (0 otherwise)\n",
    "<br/>\n",
    "$y^{(i,j)}$ = rating by user $j$ on movie $i$ (if defined)\n",
    "<br/>\n",
    "$\\theta^{(j)}$ = parameter vector for user $j$\n",
    "<br/>\n",
    "$x^{(i)}$ = feature vector for movie $i$\n",
    "<br/>\n",
    "For user $j$, movie $i$, predicted rating : $\\left(\\theta^{(j)}\\right)^{T}(x^{(i)})$\n",
    "<br/>\n",
    "$m^{(j)}$ = number of movies rated by user $j$\n",
    "<br/>\n",
    "<h2>Optimization objective</h2>\n",
    "<br/>\n",
    "To learn $\\theta^{(j)}$ (pamater for user $j$)\n",
    "<br/>\n",
    "$\\displaystyle\\min_{\\theta^{(j)}} \\dfrac{1}{2}\\displaystyle\\sum_{i:r(i,j)=1}\\left(\\left(\\theta^{(j)}\\right)^{T}x^{(i)} - y^{(i,j)}\\right)^2 + \\dfrac{\\lambda}{2}\\displaystyle\\sum_{k=1}^{n}\\left(\\theta_{k}^{(j)}\\right)^{2}$\n",
    "<br/>\n",
    "<br/>\n",
    "Thus, to learn $\\theta^{(1)} \\dots \\theta^{(n_{u})}$ \n",
    "<br/>\n",
    "$\\displaystyle\\min_{\\theta^{(1)} \\dots \\theta^{(n_{u})}} \\dfrac{1}{2}\\displaystyle\\sum_{i:r(i,j)=1}\\left(\\left(\\theta^{(j)}\\right)^{T}x^{(i)} - y^{(i,j)}\\right)^2 + \\dfrac{\\lambda}{2}\\displaystyle\\sum_{j=1}^{n_{u}}\\displaystyle\\sum_{k=1}^{n}\\left(\\theta_{k}^{(j)}\\right)^{2}$\n",
    "<br/>\n",
    "<h2>Gradient update</h2>\n",
    "<br/>\n",
    "$\\theta_{k}^{(j)} := \\theta_{k}^{(j)} - \\alpha\\displaystyle\\sum_{i:r(i,j)=1}\\left(\\left(\\theta^{(j)}\\right)^{T}x^{(i)} - y^{(i,j)}\\right)x_{k}^{(i)}$ for $k = 0$\n",
    "<br/>\n",
    "$\\theta_{k}^{(j)} := \\theta_{k}^{(j)} - \\alpha\\left[\\displaystyle\\sum_{i:r(i,j)=1}\\left(\\left(\\theta^{(j)}\\right)^{T}x^{(i)} - y^{(i,j)}\\right)x_{k}^{(i)} + \\lambda\\theta_{k}^{(j)}\\right]$ for $k \\ne 1$\n",
    "<br/>\n",
    "<h2>Similarly</h2>\n",
    "<br/>\n",
    "To learn $x^{(i)}$ \n",
    "<br/>\n",
    "$\\displaystyle\\min_{x^{(i)}} \\dfrac{1}{2}\\displaystyle\\sum_{i:r(i,j)=1}\\left(\\left(\\theta^{(j)}\\right)^{T}x^{(i)} - y^{(i,j)}\\right)^2 + \\dfrac{\\lambda}{2}\\displaystyle\\sum_{k=1}^{n}\\left(x_{k}^{(i)}\\right)^{2}$\n",
    "<br/>\n",
    "<br/>\n",
    "Thus, to learn $x^{(1)} \\dots x^{(n_{m})}$ \n",
    "<br/>\n",
    "$\\displaystyle\\min_{x^{(1)} \\dots x^{(n_{m})}} \\dfrac{1}{2}\\displaystyle\\sum_{i:r(i,j)=1}\\left(\\left(\\theta^{(j)}\\right)^{T}x^{(i)} - y^{(i,j)}\\right)^2 + \\dfrac{\\lambda}{2}\\displaystyle\\sum_{j=1}^{n_{m}}\\displaystyle\\sum_{k=1}^{n}\\left(x_{k}^{(i)}\\right)^{2}$\n",
    "<br/>\n",
    "<br/>\n",
    "$x_{k}^{(i)} := x_{k}^{(i)} - \\alpha\\displaystyle\\sum_{i:r(i,j)=1}\\left(\\left(\\theta^{(j)}\\right)^{T}x^{(i)} - y^{(i,j)}\\right)\\theta_{k}^{(j)}$ for $k = 0$\n",
    "<br/>\n",
    "$x_{k}^{(i)} := x_{k}^{(i)} - \\alpha\\left[\\displaystyle\\sum_{i:r(i,j)=1}\\left(\\left(\\theta^{(j)}\\right)^{T}x^{(i)} - y^{(i,j)}\\right)\\theta_{k}^{(j)} + \\lambda x_{k}^{(i)}\\right]$ for $k \\ne 1$\n",
    "<h2>Collaborative filtering</h2>\n",
    "<br/>\n",
    "Guess $\\theta -> x -> \\theta -> x \\dots $\n",
    "<br/>\n",
    "1. Initialize $x^{(1)} \\dots x^{(n_{m})}, \\theta^{(1)} \\dots \\theta^{(n_{u})}$ to samll random values\n",
    "<br/>\n",
    "2. Minimize $\\theta^{(1)} \\dots \\theta^{(n_{u})}$ and $x^{(1)} \\dots x^{(n_{m})}$ simultaneously\n",
    "<br/>\n",
    "$\\displaystyle\\min_{x^{(1)} \\dots x^{(n_{m})}, \\theta^{(1)} \\dots \\theta^{(n_{u})}} \\dfrac{1}{2}\\displaystyle\\sum_{i:r(i,j)=1}\\left(\\left(\\theta^{(j)}\\right)^{T}x^{(i)} - y^{(i,j)}\\right)^2 + \\dfrac{\\lambda}{2}\\displaystyle\\sum_{j=1}^{n_{m}}\\displaystyle\\sum_{k=1}^{n}\\left(x_{k}^{(i)}\\right)^{2} + \\dfrac{\\lambda}{2}\\displaystyle\\sum_{j=1}^{n_{u}}\\displaystyle\\sum_{k=1}^{n}\\left(\\theta_{k}^{(j)}\\right)^{2}$\n",
    "<br/>\n",
    "<br/>\n",
    "$x_{k}^{(i)} := x_{k}^{(i)} - \\alpha\\left[\\displaystyle\\sum_{i:r(i,j)=1}\\left(\\left(\\theta^{(j)}\\right)^{T}x^{(i)} - y^{(i,j)}\\right)\\theta_{k}^{(j)} + \\lambda x_{k}^{(i)}\\right]$\n",
    "<br/>\n",
    "$\\theta_{k}^{(j)} := \\theta_{k}^{(j)} - \\alpha\\left[\\displaystyle\\sum_{i:r(i,j)=1}\\left(\\left(\\theta^{(j)}\\right)^{T}x^{(i)} - y^{(i,j)}\\right)x_{k}^{(i)} + \\lambda\\theta_{k}^{(j)}\\right]$\n",
    "<br/>\n",
    "3. For a user with parameters $\\theta$ and a movie with (learned) features $x$, predict a star rating of $\\theta^{T}x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Large Scale Machine Learning</h1>\n",
    "<br/>\n",
    "<h2>Batch Gradient Descent</h2>\n",
    "<br/>\n",
    "$h_{\\theta}(x) = \\displaystyle\\sum_{j=0}^{n}\\theta_{j}x_{j}$\n",
    "<br/>\n",
    "$J_{train}(\\theta) = \\dfrac{1}{2m}\\displaystyle\\sum_{i=1}^{m}\\left(h_{\\theta}(x^{(i)})-y^{(i)}\\right)^{2}$\n",
    "<br/>\n",
    "Repeat {\n",
    "<br/>\n",
    "$\\theta_{j} := \\theta_{j} - \\alpha\\dfrac{1}{m}\\displaystyle\\sum_{i=1}^{m}\\left(h_{\\theta}(x^{(i)})-y^{(i)}\\right)x_{(j)}^{(i)}$ for every $j = 0 \\dots n$\n",
    "<br/>\n",
    "}\n",
    "<br/>\n",
    "For large $m$, computation is very expensive - use all $m$ examples in each iteration\n",
    "<br/>\n",
    "<br/>\n",
    "<h2>Stochastic Gradient Descent</h2>\n",
    "<br/>\n",
    "For the same objective function, randomly shuffle dataset\n",
    "<br/>\n",
    "Repeat {\n",
    "<br/>\n",
    "for $i := 1 \\dots m$ {\n",
    "<br/>\n",
    "$\\theta_{j} := \\theta_{j} - \\alpha\\left(h_{\\theta}(x^{(i)})-y^{(i)}\\right)x_{(j)}^{(i)}$ for every $j = 0 \\dots n$\n",
    "<br/>\n",
    "}\n",
    "<br/>\n",
    "}\n",
    "<br/>\n",
    "Use $1$ example in each iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Photo OCR</h1>\n",
    "<br/>\n",
    "1. Text detection\n",
    "<br/>\n",
    "2. Character segmentation\n",
    "<br/>\n",
    "3. Character classification\n",
    "<br/>\n",
    "<h2>Ceiling analysis</h2>\n",
    "<br/>\n",
    "What part of pipeline should you spend most time to improve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
