{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f946625-5bb6-4d3f-a8c6-339f271102d3",
   "metadata": {},
   "source": [
    "# Note\n",
    "- Shorten the sentence.\n",
    "    - Ex. due to the fact that -> because\n",
    "- Turn into numbers.\n",
    "    - Ex. most, many, frequent -> 30%, 85%\n",
    "- Explain acronym in the beginnnig."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b23537-9439-4ec0-9ffb-01443824d94d",
   "metadata": {},
   "source": [
    "# Achievements\n",
    "\n",
    "ML infrastructure\n",
    "- S: Whole team was manually constructing five hundred yaml files to provision infrastructure for machine learnig use cases. 95% of information we provided were duplicate. \n",
    "- T: Automate the manul work as much as possible.\n",
    "- A: I came up with using one config file where we supply the minimum five percent information. Then I wrote 9 scripts in python to automatically generate all those yaml files based on that one config file. \n",
    "- R: Use case infra provisioning, which used to take 4-6 weeks were reduced to less than 2 weeks.\n",
    "\n",
    "DevOps pipeline\n",
    "- S: When users made mistakes in pipeline configurations, they had to go back to the beginning to raise a new Pull Request. There was no way update deployment configuration for inflight releases. This caused delay between three days to weeks in deployments.\n",
    "- T: Allow users to update pipeline variables w/o going back to the beginning.\n",
    "- A: We could not allowed users to update variables in the tool itself because audit claimed that the source of truth must comes from GIT. I came up with a way to allow users to invoke a separate release which targeted the original release to update its deployment variables. This design passed the audit because that separate release will be triggered based GIT, which we treat as source of truth.\n",
    "- R: User feedback was their deployment time got reduced by three to five times on average because they didn't have to go back to the beginning after releazing the mistakes in the middle of the releases.\n",
    "\n",
    "# Made the current system better\n",
    "\n",
    "- S: The kubernetes cluster we use for model training/inference was not robust in general because it lacked monitoring and security.\n",
    "- T: We wanted monitoring and security integrated with K8s clusters.\n",
    "- A: I used helm charts to deploy datadog and kube-state-metric for monitoring and collecting metrics, and Kubernetes Reboot Daemon to automatically reboot Kubernetes nodes after the cloud provider applied the patches on the VMs.\n",
    "- R: The time to notice CPU & memory outage was cut by one day on average and Engineers were able to solve problems before users can even notice. The time and effort to reboot Kubernetes nodes manually by checking the condition inside the machines have been eliminated.\n",
    "\n",
    "# Caculated risk and take\n",
    "\n",
    "- S: We were deploying new infrastructure for a project whose load-testing was failing in non-production. We had different infrastructure t-shirt sizes between non-prod and prod. \n",
    "- T: We wanted to completed the prod deployment within the given timeline.\n",
    "- A: I suggested we do production deployment knowing that it is an internal application, user size is limited to the number of IT employees in the bank, which was eight thousand. The production t-shirt size was twice bigger in CPU and memory.\n",
    "- R: We reviewed that we had to increase the t-shrit size in non-prod so that it exactly matches the production despite the cost, in order to closely reflect what would happen in reality even during non-prod testing. \n",
    "\n",
    "# Build something from scratch\n",
    "\n",
    "- S: Team had created jobs manually in Jenkins where scripts were embedded within the jobs. Each job was different, making it hard to manage Continuous Integration for the whole line of business.\n",
    "- T: We wanted systematic approach to setup the pipeline for all projects.\n",
    "- A: I wrote multibranch pipeline using groovy language from scratch so that every project would use the same codebase for CI. And the codebase is stored is one place and updated in one place rather than each job being managed differently.\n",
    "- R: Each project in LOB significatly reduced time to onboard to CI. From two weeks to one day because we established a streamlined way to configure the hooks from code repo to Jenkins and how to configure pipeline for each project in a very systemtic way.\n",
    "\n",
    "# Disaggreement with team member\n",
    "\n",
    "- S: I updated helm chart for deploying resources in K8s for the real time machine learning usecases. My team member claimed that I should update all use cases that used that helm chart immediately.\n",
    "- T: We wanted to come up with a conclusion which we could agree upon.\n",
    "- A: I reasoned that there wouldn't be any difference between updating the chart version now or the next time we do the deployment. My team member still was not happy with that. I explained the time and effort for doing so, and the benefits of use cases having the lastest updated chart wouldn't exceeds the cost of doing it.\n",
    "- R: He came to an agreement and we ended up saving a lot of time.\n",
    "\n",
    "# Had to pursuade the team\n",
    "\n",
    "- S: The whole DevOps team were the users of the pipeline. Because we didn't know the internal mechanics of the pipeline,  we had to rely on support whenever we hit any issues. And the supports were very limited mostly due to short staffing in cloud engineering. \n",
    "- T: I wanted the team to innersource to pipeline development.\n",
    "- A: I asked the team to select several individuals to inner-source to cloud engineering to work as pipeline developers. This would raise the knowledge of the team as a whole and we would be much better enabled to resolve any pipeline issues by ourselves. \n",
    "- R: I was selected as one of the developers to inner-source to the pipeline. I gained lot of knowledge working as a developer. When we were facing any pipeline issues, it usually took a week to resolve it. Now that I knew the internal mechanics, most issues were resolved in less than a day.\n",
    "\n",
    "# Had to go beyond my responsibility\n",
    "\n",
    "- S: I was inner-sourcing in cloud engineering, working as a developer for the enterprise pipeline. There were individuals hired to support the users of the pipeline. This support was done through an internal forum where users would ask questions and support team would answer questions. Because support engineers were not too knowledgable about the pipeline, users were complaining about the quality of support.\n",
    "- T: I wanted to provide much better support for the users.\n",
    "- A: I voluntarily started answering the question on the forum whenever I found time after finishing my development work. The forum was used by everyone in the enterprise, although the alignment was that support engineer working for a LOB would only support users in that LOB. But I supported all users of the pipeline regardless where they come from.\n",
    "- R: I did that to build trust, because in an enterprise settings, we often need things from each others. And helping other LOBs requires time commitment. I believe life is give and taken, so by giving first I could receive help from others. And after I joined layer6 AI, I got a lot of support from engineers from other LOBs when I was facing new challenges.\n",
    "\n",
    "# Most challening experience\n",
    "\n",
    "- S: In the first week of joining Layer6 AI, I was tasked to on-board infrastructure for a new use case. This is one major task for this team. I was given two weeks, which is the same time given to all other members.\n",
    "- T: To provision all machines leanring infra for the new use case.\n",
    "- A: I studied the on-boarding document very carefully. Many places in the document did not make sense, so I reached out to team members asking questions and correcting them. I noted down places for improvement, especially what we could automate. The pipeline to provision the infrastructure was failing in many different places, and rather than waiting for support engineers to fix, I investigated the codebase myself to resolve issues because I was under the time cruch.\n",
    "- R: It tooks two weeks and two days to finish the infrasturcture on-boarding. Team said well down for it was first time for me to do it and they starting showing me trust.\n",
    "\n",
    "# Navigate through a difficult situation\n",
    "\n",
    "- S: I was given a very tight deadline (about 2 weeks) to develop and deploy pipeline for automating the provisioning of infrastructure on private cloud. The release date was already communicated to the users in the Enterpise and cloud engineering could not find the right candidate until they gave it to me. \n",
    "- T: Build and release VMC provision feature in the pipeline in two weeks.\n",
    "- A: I gathered all the requirements from the stakeholders. There were lots. I realized it is impractical to target all of them and I suggested the team to deliver the requirements in phases. First of all, I would finish the infra provisioning for various middlewares (jboss, springboot, nodejs) including the core configuration of those infrastructure because these were most critical. Then, I would target allowing users to provide their own custom configuration for the infrastructure in the second release. Then, the ability to cleanup any unused infrasture or deployment would come in third release. Besides I worked long hours each day for that sprint.\n",
    "- R: The team was still able to deliver their initial promise to end users. The end users got what they expected and before they realized they were missing something, we filled those gaps in the subsequent releses. \n",
    "\n",
    "# Made tradeoffs in your projects\n",
    "\n",
    "- S: Log4j issue hit Layer6 AI hard affecting most containers running in production environment. The issue came during the weekend, so we had limited responses from the users. If we kept the containers running, we would have serious security risks of RCE. If we just shutdown containers, it could impact businesses.  \n",
    "- T: Remediate log4j situation as soon as possible.\n",
    "- A: I tried to reach out to all users. But seeing people didn't respond for it was a weekend, I took an intiative to tell my team that we should just to go ahead and shutdown all the containers. It was because I determined the risk of RCE far outweighted the loss of business outputs in the next several days.\n",
    "- R: I took 2-3 days (including the weekend) to patch the containers and re-stood them up. Although that caused some business impacts, I still feel it was the right decision to make since Layer6 AI was not exploited by the vulnerability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b9a769-d060-44f1-b6bf-942d0bd19db5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
