<!DOCTYPE html>

<html lang="en">

<head>

<!-- Metadata -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="Seungmoon's DevOps Engineering Blog">
<meta name="author" content="Seungmoon Rieh">
<meta name="keywords" content="Python, Groovy, Kubernetes, Docker, Jenkins, Terraform, Bash">

<!-- Title and image -->
<title>Seungmoon Rieh</title>
<link href="img/seungmoonrieh.jpg" rel="icon">

<!-- CSS -->
<link href="css/bootstrap.min.css" rel="stylesheet">
<link href="css/monokai-sublime.css" rel="stylesheet">
<link href="css/site.css" rel="stylesheet">

<!-- JavaScript -->
<script src="js/jquery.min.js"></script>
<script src="js/bootstrap.bundle.min.js"></script>
<script src="js/highlight.pack.js"></script>
<script type="text/javascript" src="js/include_html.js"></script>
<script type="text/javascript" src="js/site.js"></script>

</head>

<body>

<include src="header.html"></include>

<div class="container">
<div class="row">
<div class="col-md-12">
<h1 class="my-4">Software Engineering</h1>

<!-- Java BEGIN -->
<div class="card mb-4" id="java">
  <div class="card-body">
    <h2 class="card-title">Java</h2>
    <p class="card-text"></p>
    <ul class="list-unstyled mb-0">
      <li><a href="#java-1">Search for JAR</a></li>
      <li><a href="#java-2">Work with WAR</a></li>
      <li><a href="#java-3">JDK</a></li>
      <li><a href="#java-4">Maven</a></li>
      <li><a href="#java-5">Gradle</a></li>
    </ul>
  </div>
</div>

<div class="card mb-4" id="java-1">
  <div class="card-body">
    <h2 class="card-title">Search for JAR</h2>
    <p class="card-text"></p>
    <h3 class="card-title">In linux process</h3>
<pre><code class="bash">ps faux # Shows parent and child processes.
ps aux | grep java # Shows Java processes.
ps aux | grep java | awk '{print $2}' # Only show process ids.
lsof -p &lt;pid&gt; # Show everything about this process.</code></pre>

    <h3 class="card-title">Inside a jar</h3>
<pre><code class="bash">unzip &lt;name&gt;.jar && cd &lt;name&gt;
grep -rnw ./ -e '&lt;jar file to find&gt;'</code></pre>
  </div>
  <div class="card-footer text-muted">

  </div>
</div>

<div class="card mb-4" id="java-2">
  <div class="card-body">
    <h2 class="card-title">Work with WAR</h2>
    <p class="card-text"></p>
<pre><code class="bash"># Extract a WAR file.
jar -xvf &lt;name&gt;.war

# Create a WAR file.
jar -cvf &lt;name&gt;.war &lt;directory&gt;</code></pre>
  </div>
  <div class="card-footer text-muted">

  </div>
</div>

<div class="card mb-4" id="java-3">
  <div class="card-body">
    <h2 class="card-title">JDK</h2>
    <p class="card-text"></p>
    <ul>
      <li>JRE (Java Runtime Environment): JVM and "java" command line tool.</li>
      <li>JDK (Java Development Kit): JRE + javac (compiler) + javadoc (documentation generator) + jdb (debugger).</li>
      <li>Only JDK is available since Java 9.</li>
    </ul>
<pre><code class="bash"># Check JDK location.
which java

# Check Java version.
java -version
# Output will depend on Oracle JDK vs OpenJDK

# Check available Java in the system.
update-alternatives --config java</code></pre>

<pre><code class="bash"># Update JAVA_HOME to the output of "which java".
export JAVA_HOME=$(dirname $(dirname $(readlink $(readlink $(whichi java)))))
# Update PATH variable.
export PATH=$PATH:$JAVA_HOME/bin
# Update Classpath.
export CLASSPATH=.:$JAVA_HOME/lib:$JAVA_HOME/jre/lib</code></pre>

    <h3 class="card-title">Uninstall Java</h3>
<pre><code class="bash"># Oracle JDK
sudo update-alternatives --remove "java"
sudo update-alternatives --remove "javac"
sudo update-alternatives --remove "javaws"
sudo rm -r /usr/lib/jvm/jdk&lt;version&gt;

# OpenJDK
sudo apt-get purge --auto-remove openjdk*</code></pre>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://novicestuffs.wordpress.com/2017/04/25/how-to-uninstall-java-from-linux/">How To Uninstall JDK From Linux ?</a>
  </div>
</div>

<div class="card mb-4" id="java-4">
  <div class="card-body">
    <h2 class="card-title">Maven</h2>
    <p class="card-text"></p>
<pre><code class="bash"># Build.
mvn package

# Build without testing.
mvn package -Dmaven.test.skip</code></pre>
  </div>
  <div class="card-footer text-muted">

  </div>
</div>

<div class="card mb-4" id="java-5">
  <div class="card-body">
    <h2 class="card-title">Gradle</h2>
    <p class="card-text"></p>
<pre><code class="bash"></code></pre>
  </div>
  <div class="card-footer text-muted">

  </div>
</div>
<!-- Java END -->


<!-- Hadoop BEGIN -->
<div class="card mb-4" id="hadoop">
  <div class="card-body">
    <h2 class="card-title">Hadoop</h2>
    <p class="card-text"></p>
    <ul class="list-unstyled mb-0">
      <li><a href="#hadoop-1">Hadoop basic</a></li>
      <li><a href="#hadoop-2">Start with Hadoop</a></li>
    </ul>
  </div>
</div>

<div class="card mb-4" id="hadoop-1">
  <div class="card-body">
    <h2 class="card-title">Hadoop basic</h2>
    <p class="card-text"></p>

    <h3 class="card-title">HDFS (Hadoop Distributed File System)</h3>
    <ul>
      <li>Scalability and reliability.</li>
      <li>Spread files to differnt nodes for parallel access.</li>
      <li>Partitioning and replication for fault tolerance. (node failure doesn't cause files to become unaccessible)</li>
      <li>NameNode for metadata (1 node per cluster), DataNode for block storage (each node per cluster)</li>
    </ul>

    <h3 class="card-title">YARN (Yet Another Resource Negotiator)</h3>
    <ul>
      <li>Resource manager for Hadoop.</li>
      <li>While node manager is in charge of each machine, resource manager decides who gets what.</li>
      <li>Allows applications to run on the same Hadoop cluster.</li>
    </ul>

    <h3 class="card-title">MapReduce</h3>
    <ul>
      <li>Programming model for Hadoop ecosystem.</li>
      <li>Relies on YARN.</li>
      <li>Not good for frequently changing data.</li>
      <li>Consists of mapping, shuffling, and reducing.</li>
      <li>Delegates large tasks to lots of nodes and aggregates the result.</li>
    </ul>

    <h3 class="card-title">Weakness</h3>
    <ul>
      <li>Small datasets.</li>
      <li>Advanced algorithms.</li>
      <li>Infrastrutural replacement.</li>
      <li>Task level parallelism.</li>
      <li>Random data access.</li>
      <li>Only map-reduce based computation.</li>
      <li>No interactive shell.</li>
      <li>Native support for Java only.</li>
      <li>Relies on reading data from HDFS, which become a bottleneck.</li>
    </ul>

    <h3 class="card-title">Three layers of Hadoop</h3>
    <ul>
      <li>Data management and storage.</li>
      <li>Data integration and processing.</li>
      <li>Coordination and workflow management.</li>
    </ul>
  </div>
  <div class="card-footer text-muted">

  </div>
</div>

<div class="card mb-4" id="hadoop-2">
  <div class="card-body">
    <h2 class="card-title">Start with Hadoop</h2>
    <p class="card-text"></p>
<pre><code class="bash"># First, open etc/hadoop/hadoop-env.sh and add JAVA_HOME environment variable.
# If permission error, do the following.
cd ~/.ssh && ssh-keygen -t rsa -p""
cat id_rsa.pub >> authorized_keys

# Start Hadoop service.
sbin/start-dfs.sh

# If needed, add to the path.
export PATH=$PATH:$HADOOP_HOME/bin

# Note that copying/pasting hadoop commands into the terminal may not work.
# Check hadoop version.
hadoop version

# Create a directory /tmp/test
hadoop fs -mkdir -p /tmp/test
hadoop fs -ls /tmp/test
hadoop fs -du -s /tmp/test
hadoop fs -df -h

# Create a file.
hadoop fs -touchz /tmp/test/file

# Empty out the trash.
hadoop fs -expunge</code></pre>
  </div>
  <div class="card-footer text-muted">

  </div>
</div>
<!-- Hadoop END -->


<!-- Spark BEGIN -->
<div class="card mb-4" id="spark">
  <div class="card-body">
    <h2 class="card-title">Spark</h2>
    <p class="card-text"></p>
    <ul class="list-unstyled mb-0">
      <li><a href="#spark-1">Start basic</a></li>
      <li><a href="#spark-2">Start with Spark</a></li>
    </ul>
  </div>
</div>

<div class="card mb-4" id="spark-1">
  <div class="card-body">
    <h2 class="card-title">Start basic</h2>
    <p class="card-text"></p>
    <h3 class="card-title">Spark</h3>
    <ul>
      <li>Resilent distributed datasets (RDD)</li>
      <li>Driver program creates RDD.</li>
      <li>Create RDD -> apply transformation -> lazily evaluated and action performed.</li>
      <ul>
        <li>Transformations will wait until actions are performed.</li>
        <li>Errors may show up in action stage, not in transformation stage.</li>
      </ul>
    </ul>

    <h3 class="card-title">Narrow transformations</h3>
    <ul>
      <li>Take place in worker nodes locally.</li>
      <li>map: apply function to each element of RDD.</li>
      <li>flatMap: map then flatten output.</li>
      <li>filter: keep only elements where function is true.</li>
      <li>coalesce: reduce number of partitions.</li>
    </ul>

    <h3 class="card-title">Wide transformations</h3>
    <ul>
      <li>Processing depends on data residing in multiple partitions distributed across worker nodes,</li>
      <li>groupByKey: (K, V) pairs => (K, list of all V)</li>
    </ul>

    <h3 class="card-title">Actions</h3>
    <ul>
      <li>collect: copy all elements to the driver.</li>
      <li>take(n): copy first n elements.</li>
      <li>reduce(func): aggregate elements with func.</li>
      <li>saveAsTextFile(filename): save to local file or HDFS.</li>
    </ul>

    <h3 class="card-title">Spark SQL</h3>
    <ul>
      <li>Enables querying structured and unstructured data through Spark.</li>
      <li>APIs for Scala, Java, Python to convert result into RDDs.</li>
      <li>Deploy business intelligence tools over Spark.</li>
    </ul>

    <h3 class="card-title">Data frames</h3>
    <ul>
      <li>Looks just like tables in relational databases.</li>
      <li>RDDs can be converted to data frames.</li>
    </ul>

    <h3 class="card-title">Spark streaming</h3>
    <ul>
      <li>sources: Kafka, Flume, HDFS, S3, etc.</li>
    </ul>

    <h3 class="card-title">Spark MLLib</h3>
    <ul>
      <li>machine learning library for Spark</li>
    </ul>

    <h3 class="card-title">Spark GraphX</h3>
    <ul>
      <li>API for graph computation.</li>
      <li>triplets: views that logically join vertex and edge properties.</li>
    </ul>
  </div>
  <div class="card-footer text-muted">

  </div>
</div>

<div class="card mb-4" id="spark-2">
  <div class="card-body">
    <h2 class="card-title">Start with Spark</h2>
<pre><code class="bash">bin/spark-shell
# Read a file from local file system.
val data = sc.textFile("tmp/test/file")
val num = Array(1,2,3,4,5,6,7,8,9)
val NewData = sc.parallelize(num)
NewData.count()
# Type Ctrl-D to exit.</code></pre>
  </div>
  <div class="card-footer text-muted">

  </div>
</div>
<!-- Spark END -->

</div> <!-- /.col-md-12 -->
</div> <!-- /.row -->
</div> <!-- /.container -->

<include src="footer.html"></include>

</body>

</html>