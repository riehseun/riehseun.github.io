<!DOCTYPE html>

<html lang="en">

<head>

<!-- Metadata -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="">
<meta name="author" content="Seungmoon Rieh">
<meta name="keywords" content="">

<!-- Title and image -->
<title>Seungmoon Rieh</title>
<link href="/img/seungmoonrieh.jpg" rel="icon">

<!-- CSS -->
<link href="/css/bootstrap.min.css" rel="stylesheet">
<link href="/css/monokai-sublime.css" rel="stylesheet">
<link href="/css/site.css" rel="stylesheet">

<!-- JavaScript -->
<script src="/js/jquery.min.js" type="text/javascript"></script>
<script src="/js/bootstrap.bundle.min.js" type="text/javascript"></script>
<script src="/js/highlight.pack.js" type="text/javascript"></script>
<script src="/js/include_html.js" type="text/javascript"></script>
<script src="/js/mathjax/tex-chtml.js" type="text/javascript"></script>
<script src="/js/site.js" type="text/javascript"></script>

</head>

<body>

<include src="/header.html"></include>

<div class="container">
<div class="row">
<div class="col-md-12">
<h1 class="my-4">Platform Engineering</h1>

<!-- Kubernetes BEGIN -->
<div class="card mb-4" id="kubernetes">
  <div class="card-body">
    <h2 class="card-title">Kubernetes</h2>

    <ul class="list-unstyled mb-0">
      <li><a href="#kubernetes-1">Kubernetes basic</a>
      <li><a href="#kubernetes-2">Kubectl</a></li>
      <li><a href="#kubernetes-5">Helm</a></li>
      <li><a href="#kubernetes-6">Skaffold</a></li>
      <li><a href="#kubernetes-7">Datadog and kube state metric</a></li>
      <li><a href="#kubernetes-8">Kured</a></li>
      <li><a href="#kubernetes-9">Istio</a></li>
      <li><a href="#kubernetes-10">Kubernetes common issues</a></li>
      <br>

      <li><a href="#kubernetes-21">Namespace</a> - virtual clusters supported by the same physical cluster</li>
      <li><a href="#kubernetes-22">Deployment</a> - declarative updates for Pod and ReplicaSet including rollouts and rollbacks</li>
      <li><a href="#kubernetes-23">ReplicaSet</a> - creates Pods (should not be manipulated, Deployment should be used instead)</li>
      <li><a href="#kubernetes-24">StatefulSet</a> - similar to Deployment, but provides uniqueness and ordering of Pods</li>
      <li><a href="#kubernetes-25">DaemonSet</a> - exactly one Pod per Node. Deleting it clean up Pods</li>
      <li><a href="#kubernetes-26">Job</a> - create short living Pods. Deleting it clean up Pods</li>
      <li><a href="#kubernetes-27">Cron Job</a> - create Jobs on repeating schedule</li>
      <li><a href="#kubernetes-28">Pod</a> - smallest deployable unit</li>
      <ul>
        <li><a href="#kubernetes-101">Init containers</a></li>
        <li><a href="#kubernetes-102">Pod Topology Spread Constraints</a></li>
        <li><a href="#kubernetes-103">Multi-container pod design</a></li>
        <li><a href="#kubernetes-104">Disruptions</a></li>
        <li><a href="#kubernetes-105">Application Resource Requirement</a></li>
        <li><a href="#kubernetes-106">LivenesProbe and ReadinessProbe</a></li>
        <li><a href="#kubernetes-107">Service Account</a></li>
        <li><a href="#kubernetes-108">SecurityContext</a></li>
        <li><a href="#kubernetes-109">Labels, selectors, and annotations</a></li>
        <li><a href="#kubernetes-110">Node affinity</a></li>
        <li><a href="#kubernetes-111">Pod affinity</a></li>
      </ul>
      <br>

      <li><a href="#kubernetes-31">Ingress Controller</a> - specification of Ingress</li>
      <li><a href="#kubernetes-32">Ingress</a> - provides external access to Service (Need ingress controller)</li>
      <li><a href="#kubernetes-33">Service</a> - manages traffic to Pods</li>
      <li><a href="#kubernetes-34">Network Policy</a> - denie/allows traffic to/from Pods</li>
      <li><a href="#kubernetes-35">CoreDNS</a></li>
      <br>

      <li><a href="#kubernetes-41">Volumes</a></li>
      <li><a href="#kubernetes-42">Persistent Volumes</a> - piece of storage, simialr to Node</li>
      <li><a href="#kubernetes-45">Volume Snapshot</a> - request for a snapshot</li>
      <br>

      <li><a href="#kubernetes-51">ConfigMap</a> - configuration data separate from app code. Must be in the same namespace with Pod</li>
      <li><a href="#kubernetes-52">Secret</a> - similar to ConfigMap but specifically for confidential data</li>
      <li><a href="#kubernetes-53">RBAC</a>
      <br>
    </ul>
  </div>
  <div class="card-footer text-muted">

  </div>
</div>

<div class="card mb-4" id="kubernetes-1">
  <div class="card-body">
    <h2 class="card-title">Kubernetes basic</h2>
    <ul>
      <li>Orchestrator of cloud-native microservices applications</li>
      <li>Kubernetes abstracts runtime via container runtime interface (CRI)</li>
      <ul>
        <li>Container runtime code exists outside of Kubernetes</li>
        <li>Container runtime code interacts with Kubernetes in standardized way</li>
      </ul>
      <li><strong>containerd</strong> is most commonly used runtime</li>
      <ul>
        <li>It is stripped-down version of Docker with just the stuff that Kubernetes needs</li>
      </ul>
    </ul>

    <h3 class="card-title">Control plane</h3>
    <ul>
      <li>Running 3 or 5 replicated masters in an HA configuration is recommended</li>
      <li>kube-apiserver</li>
      <ul>
        <li>Exposes Kubernetes API</li>
        <li>All communication, between all components, must go through the API server</li>
      </ul>
      <li>Cluster store</li>
      <ul>
        <li>based on <strong>etcd</strong>, a popular distributed database</li>
        <li>Stores cluster data</li>
        <li>Should run between 3-5 etcd replicas for high availability</li>
      </ul>
      <li>kube-scheduler</li>
      <ul>
        <li>Assigns Pods to Nodes</li>
      </ul>
      <li>kube-controller-manager</li>
      <ul>
        <li>Node controller - notice and respond when Nodes are down</li>
        <li>Replication controller - ensures correct number of Pods are running</li>
        <li>Endpoint controller - populates Endpoint object</li>
        <li>Service account & token controller - creates accounts and API tokens for new namespaces</li>
      </ul>
      <li>cluster-controller-manager</li>
      <ul>
        <li>Cloud specific control logic</li>
      </ul>
    </ul>

    <h3 class="card-title">Node component</h3>
    <ul>
      <li>kubelet</li>
      <ul>
        <li>Registers the node with the cluster</li>
        <li>Watches the API server for new work assignments</li>
      </ul>
      <li>kube-proxy</li>
      <ul>
        <li>Local cluster network rules</li>
      </ul>
      <li>Container runtime</li>
      <ul>
        <li>Ex. cri-containerd</li>
      </ul>
    </ul>

    <h3 class="card-title">DNS</h3>
    <ul>
      <li>Kubernetes cluster has an internal DNS service</li>
      <li>This DNS service has a static IP address</li>
    </ul>

    <h3 class="card-title">Kubernetes manifest</h3>
    <ul>
      <li>apiVersion</li>
      <ul>
        <li>StorageClass objects are defined <code>storage.k8s.io/v1</code></li>
        <li>Pods are defined <code>v1</code></li>
      </ul>
      <li>kind</li>
      <ul>
        <li>Type of object being deployed</li>
      </ul>
      <li>metadata</li>
      <ul>
        <li>Define name, label, namepsace</li>
      </ul>
      <li>spec</li>
      <ul>
        <li>Define container(s)</li>
      </ul>
    </ul>

    <h3 class="card-title">Kubernetes security</h3>
    <ul>
      <li>Spoofing</li>
      <ul>
        <li>Pretending to be someone</li>
        <li>Kubernetes components communicate each other using TLS</li>
        <li>Certificate is issued by the internal Kubernetes CA</li>
      </ul>
      <li>Tampering</li>
      <ul>
        <li>Changing something in a malicious way</li>
        <li>Can enforce readonly to certain directory</li>
<pre><code class="yaml">apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  securityContext:
    readOnlyRootFilesystem: true
    allowedHostPaths:
      - pathPrefix: /path
        readOnly: true</code></pre>
      </ul>
      <li>Repudiation</li>
      <ul>
        <li>Non-repudiation is proving certain actions were carried out by certain individuals</li>
        <li>API server has audit logs</li>
      </ul>
      <li>Information Disclosure</li>
      <ul>
        <li>Secrets can be encrypted</li>
        <li>Encryption key is stored in nodes but storing it outside the cluster is recommended</li>
      </ul>
      <li>Denial of Service</li>
      <ul>
        <li>Can limit number of pods</li>
<pre><code class="yaml">apiVersion: v1
kind: ResourceQuota
metadata:
  name: my-resourcequota
spec:
  hard:
    pods: 100</code></pre>
      </ul>
      <li>Elevation of Privilege</li>
      <ul>
        <li>Gaining higher access than what is granted</li>
        <li>Can use RBAC</li>
      </ul>
      <li>User in Pod</li>
      <ul>
<pre><code class="yaml">apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  securityContext:  # Applies to all containers in this Pod
    runAsUser: 1000 # Non-root user
  containers:
  - name: my-container
    image: my-image
    securityContext:
      runAsUser: 2000  # Overrides the Pod setting</code></pre>
      </ul>
      <li>Prevent privilege escalation by containers</li>
      <ul>
<pre><code class="yaml">apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  containers:
  - name: my-pod
    image: my-image
    securityContext:
      allowPrivilegeEscalation: false</code></pre>
      </ul>
    </ul>

    <h3 class="card-title">Autoscaler</h3>
    <ul>
      <li>Horizontal Pod Autoscaler</li>
      <ul>
        <li>Dynamically scales Pods in a Deployment based on demand</li>
      </ul>
      <li>Cluster Autoscaler</li>
      <ul>
        <li>Dynamically scales nodes based on demand</li>
      </ul>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://www.educative.io/courses/the-kubernetes-course">Learn Kubernetes: A Deep Dive</a>
  </div>
</div>

<div class="card mb-4" id="kubernetes-2">
  <div class="card-body">
    <h2 class="card-title">Kubectl</h2>
    <ul>
      <li>Should have only 1 minor version difference from the cluster version</li>
      <li>Converts commands into JSON payload required by API server</li>
    </ul>

    <h3 class="card-title">Cluster</h3>

<pre><code class="bash">kubectl cluster-info</code></pre>

    <h3 class="card-title">Config</h3>
    <ul>
      <li>Configuration is stored at <code>$HOME/.kube/config</code></li>
    </ul>

<pre><code class="bash">kubectl config view
kubectl config view | grep namespace

kubectl config set-cluster &lt;cluster_name&gt;

kubectl config get-contexts
kubectl config current-context
kubectl config use-context &lt;context_name&gt;
kubectl config set-context --current --namespace=&lt;namespace&gt;
kubectl config set-context --current --user=&lt;user&gt;</code></pre>

    <h3 class="card-title">Namespace</h3>

<pre><code class="bash"># List namespace
kubectl get ns

# If "Unable to connect to the server: x509: certificate signed by unknown authority"
kubectl get ns --insecure-skip-tls-verify

# Delete namespace
kubectl delete namespace &lt;namespace&gt;

# Delete a namespace label
kubectl label namespaces &lt;namespace&gt; &lt;label&gt;-

# Find all resources in a namespace
kubectl api-resources --verbs=list --namespaced -o name | xargs -n 1 kubectl get --show-kind --ignore-not-found -n &lt;namespace&gt;</code></pre>

<!-- <pre><code class="bash"># Inspace all namespaces.
kubectl get pods --all-namespaces
kubectl get rolebinding --all-namespaces
kubectl get clusterrolebinding --all-namespaces
kubectl get serviceaccount --all-namespaces
kubectl get rolebindings,clusterrolebindings \
  --all-namespaces  \
  -o custom-columns='KIND:kind,NAMESPACE:metadata.namespace,NAME:metadata.name,SERVICE_ACCOUNTS:subjects[?(@.kind=="ServiceAccount")].name'
</code></pre> -->

    <h3 class="card-title">Image and container</h3>

<pre><code class="bash"># List all images
kubectl get pods --all-namespaces -o jsonpath="{.items[*].spec.containers[*].image}" | tr -s '[[:space:]]' '\n' | sort | uniq -c

# Find containers in a pod
kubectl get pods &lt;pod_name&gt; -n &lt;namespace&gt; -o jsonpath='{.spec.containers[*].name}'

# Get image used in deployment or daemonset
kubectl get deployments/ds -n &lt;namespace&gt; -o yaml | grep image

# Debug init container
kubectl get pod &lt;pod_name&gt; --template '{{.status.initContainerStatuses}}'
kubectl logs &lt;pod_name&gt;; -c &lt;init_container_name&gt;</code></pre>

    <h3 class="card-title">Pod</h3>

<pre><code class="bash"># Get all pods in current namespace
kubectl get pod

# View Pod-Node assignment
kubectl get pod -o wide

# List pods matching specific label
kubectl get pods -l app=myapp

# List pods matching specific node
kubectl get pods --all-namespaces -o wide --field-selector spec.nodeName=&lt;node_name&gt;

# View desired state and current observed state
kubectl get pod &lt;pod_name&gt; -o=yaml

# View overview of object
kubectl describe pod &lt;pod_name&gt;

# Check pod status periodically
watch -n &lt;nubmer_of_seconds&gt; kubectl get pod

# Exec into a pod
kubectl exec -it &lt;pod_name&gt; -- /bin/bash

# Exec into a pod (In windows using Git Bash)
echo alias kubectl="/path/to/kubectl.exe" >> ~/.bashrc
winpty kubectl exec -it &lt;pod_name&gt; -- sh

# Temporarily disable pod
kubectl scale --replicas=0 deployment/&lt;deployment_name&gt;

# Rasons for Pod failure
kubectl get pod &lt;pod_name&gt; -o go-template="{{range .status.containerStatuses}}{{.lastState.terminated.message}}{{end}}"

# Port forward to access app runnig inside a pod/deployment from localhost
kubectl port-forward &lt;pod_name&gt; &lt;localhost_port&gt;:&lt;container_port&gt;</code></pre>

    <h3 class="card-title">Deployment</h3>

<pre><code class="bash"># Check Deployment rollout status
kubectl rollout status deployment/&lt;deployment_name&gt;

# Check the labels generated for each Pod
kubectl get pods --show-labels

# To update an iamge, run one of the following ("--record" flag makes K8s document revision history)
kubectl --record deployment.apps/nginx-deployment set image deployment.v1.apps/&lt;deployment_name&gt; &lt;image_name&gt;=&lt;image_name&gt;:&lt;new_image_version&gt;
kubectl set image deployment/&lt;deployment_name&gt; &lt;image_name&gt;=&lt;image_name&gt;:&lt;new_image_version&gt; --record</code></pre>

    <h4 class="card-title">Deployment rollback</h4>

<pre><code class="bash"># Check the revisions of Deployment
kubectl rollout history deployment.v1.apps/&lt;deployment_name&gt;

# To see the details of each revision
kubectl rollout history deployment.v1.apps/&lt;deployment_name&gt; --revision=&lt;revision_number&gt;

# Rollback to a specific version
kubectl rollout undo deployment.v1.apps/&lt;deployment_name&gt; --to-revision=&lt;revision_number&gt;</code></pre>

    <h4 class="card-title">Deployment scaling</h4>
    <ul>
      <li>Deployment Controller balances additional replicas in the existing ReplicaSets</li>
    </ul>

<pre><code class="bash"># Scale a Deployment
kubectl scale deployment.v1.apps/&lt;deployment_name&gt; --replicas=&lt;number_of_replicas&gt;

# Setup autoscaler for Deployment and choose the minimum and maximum number of Pods
kubectl autoscale deployment.v1.apps/&lt;deployment_name&gt; --min=&lt;min_number_of_replicas&gt; --max=&lt;max_number_of_replicas&gt; --cpu-percent=&lt;percentage_of_cpu&gt;

# Rollback to a specific version
kubectl rollout undo deployment.v1.apps/&lt;deployment_name&gt; --to-revision=&lt;min_number_of_replicas&gt;</code></pre>

    <h4 class="card-title">Pause and resume Deployment</h4>
    <ul>
      <li>Can apply multiple fixes in between pausing and resuming without triggering unnecessary rollouts</li>
    </ul>

<pre><code class="bash"># To pause the Deployment
kubectl rollout pause deployment.v1.apps/&lt;deployment_name&gt;

# Can update the image
kubectl set image deployment.v1.apps/&lt;deployment_name&gt; --revision=&lt;revision_number&gt;

# Can update the resources
kubectl set resources deployment.v1.apps/&lt;deployment_name&gt; -c=nginx --limits=cpu=&lt;cpu&gt;,memory=&lt;memory&gt;

# To resume the Deployment
kubectl rollout resume deployment.v1.apps/&lt;deployment_name&gt;</code></pre>

    <h3 class="card-title">PV/PVC</h3>

<pre><code class="bash"># Show PVC on all namespaces
kubectl get pvc -A

# Show PVC on current namespace
kubectl get pvc

# Update PV Reclaim Policy (To Retain)
kubectl patch pv &lt;pv_name&gt; -p '{"spec":{"persistentVolumeReclaimPolicy":"Retain"}}'</code></pre>

    <h3 class="card-title">Log</h3>

<pre><code class="bash"># Inspect pod logs
kubectl logs &lt;pod_name&gt; &lt;container_name&gt;

# Inspect previous pod logs
kubectl logs &lt;pod_name&gt; &lt;container_name&gt; -p</code></pre>

    <h3 class="card-title">Node</h3>

<pre><code class="bash">kubectl get nodes --show-labels

# Drain and delete a node
kubectl drain &lt;node_name&gt; --ignore-daemonsets --delete-local-data
kubectl delete node &lt;node_name&gt;

# This could be preferred over draining a node
kubectl cordon &lt;node_name&gt;
kubectl uncordon &lt;node_name&gt;

# Check node capacity
kubectl get nodes -o yaml | egrep '\sname:|cpu:|memory:'
kubectl get nodes -o json | jq '.items[] | {name: .metadata.name, cap: .status.capacity}'</code></pre>

    <h4 class="card-title">How to exec into a container as root</h4>

<pre><code class="bash"># 1. Find the node on which the pod is running
kubectl get pods -o wide -n &lt;namespace&gt;

# 2. Describe the pod
kubectl describe pod &lt;pod_name&gt; -n &lt;namespace&gt;

# 3. Get the container ID

# 4. SSH into the node

# 5. Exec into the pod
docker exec -u root -it &lt;container_id&gt;</code></pre>

    <h3 class="card-title">Endpoints</h3>

<pre><code class="bash">kubectl get endpoints -n &lt;namespace&gt;</code></pre>

    <h3 class="card-title">ConfigMap</h3>

<pre><code class="bash"># Create ConfigMap from a file
kubectl create cm &lt;configmap_name&gt;--from-file &lt;filename&gt;</code></pre>

    <h3 class="card-title">Secrets</h3>

<pre><code class="bash"># Export certificate and key
kubectl get secret -n &lt;namespace&gt; &lt;secret_name&gt; -o go-template='{{index .data "&lt;your_cert&gt;.crt" | base64decode }}' > &lt;your_cert&gt;.crt
kubectl get secret -n &lt;namespace&gt; &lt;secret_name&gt; -o go-template='{{index .data "&lt;your_cert&gt;.key" | base64decode }}' > &lt;your_cert&gt;.key</code></pre>

    <h3 class="card-title">Auth</h3>

<pre><code class="bash">kubectl auth can-i get pod --as=system:serviceaccount:&lt;namespace&gt;&lt;service_account_name&gt; -n &lt;namespace&gt;</code></pre>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://kubernetes.io/docs/reference/kubectl/cheatsheet">kubectl Cheat Sheet</a>
  </div>
</div>

<div class="card mb-4" id="kubernetes-5">
  <div class="card-body">
    <h2 class="card-title">Helm</h2>
    <p class="card-text">Helps packaging K8s manifest files</p>
    <ul>
      <li><code>Chart.yaml</code> - chart information such as apiVersion, chart name, chart version</li>
      <li><code>values.yaml</code> - configuration values for the chart</li>
      <li><code>templates/</code> - k8s manifest files</li>
    </ul>

    <h3 class="card-title">Helm limitation</h3>
    <ul>
      <li>When users manually changes resources deployed by Helm, Helm does not know the change and the next Helm deployment will fail</li>
      <li>Uninstalling helm chart does not delete PVC. Those need to be cleaned up separately</li>
    </ul>

    <h3 class="card-text">Helm command</h3>

<pre><code class="bash"># Lint helm values yaml
helm lint -f &lt;values.yaml&gt;

# Dry run
helm install -n &lt;namespace&gt; &lt;release_name&gt; --dry-run --debug . -f &lt;values.yaml&gt;

# Deploy
helm install -n &lt;namespace&gt; &lt;release_name&gt; . -f &lt;values.yaml&gt;

# Upgrade
helm upgrade -n &lt;namespace&gt; &lt;release_name&gt; . -f &lt;values.yaml&gt;

# Undeploy
helm uninstall -n &lt;namespace&gt; &lt;release_name&gt;

# List helm releases
helm list -n &lt;namespace&gt;
helm ls --all-namespaces

# Check helm release
helm status &lt;release_name&gt;

# Get helm values of previous deployment
helm get values &lt;release_name&gt; -n &lt;namespace&gt;

# Check helm history
helm history &lt;release_name&gt; -n &lt;namespace&gt;

# Rollback to previous version
helm rollback &lt;release_name&gt; -n &lt;namespace&gt; &lt;version&gt;</code></pre>

    <h3 class="card-text">Helm syntax</h3>

<pre><code class="yaml"># Lowercase the result, then wrap it in double quotes
value: {{ include "mytpl" . | lower | quote }}

# Being a new line and indent 8 spaces
{{ Values.myValue | nindent 8 }}</code></pre>

    <h3 class="card-text">Conditions</h3>

<pre><code class="yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ .Release.Name }}-configmap
data:
  myvalue: "Hello World"
  drink: {{ .Values.favorite.drink | default "tea" | quote }}
  food: {{ .Values.favorite.food | upper | quote }}*
**{{- if eq .Values.favorite.drink "coffee" }}
  mug: "true"*
**{{- end }}</code></pre>

<pre><code class="yaml"># Result
apiVersion: v1
kind: ConfigMap
metadata:
  name: clunky-cat-configmap
data:
  myvalue: "Hello World"
  drink: "coffee"
  food: "PIZZA"
  mug: "true"</code></pre>

    <h3 class="card-text">Variable scoping</h3>

<pre><code class="yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ .Release.Name }}-configmap
data:
  myvalue: "Hello World"
  {{- with .Values.favorite }}
  drink: {{ .drink | default "tea" | quote }}
  food: {{ .food | upper | quote }}
  {{- end }}

# Use "$" to access objects from parent scope.
{{- with .Values.favorite }}
drink: {{ .drink | default "tea" | quote }}
food: {{ .food | upper | quote }}
release: {{ $.Release.Name }}
{{- end }}</code></pre>

    <h3 class="card-text">Loop</h3>

<pre><code class="yaml"># Assume "values.yaml"
favorite:
  drink: coffee
  food: pizza
pizzaToppings:
  - mushrooms
  - cheese
  - peppers
  - onions</code></pre>

<pre><code class="yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ .Release.Name }}-configmap
data:
  myvalue: "Hello World"
  {{- with .Values.favorite }}
  drink: {{ .drink | default "tea" | quote }}
  food: {{ .food | upper | quote }}
  toppings: |-
    {{- range $.Values.pizzaToppings }}
    - {{ . | title | quote }}
    {{- end }}
  {{- end }}</code></pre>

<pre><code class="yaml"># Result
apiVersion: v1
kind: ConfigMap
metadata:
  name: edgy-dragonfly-configmap
data:
  myvalue: "Hello World"
  drink: "coffee"
  food: "PIZZA"
  toppings: |-
    - "Mushrooms"
    - "Cheese"
    - "Peppers"
    - "Onions"</code></pre>

    <h3 class="card-text">Predefined values</h3>

<pre><code class="bash">Release.Name
Release.Namespace
Release.Service # Always "Helm"
Chart</code></pre>

    <h3 class="card-text">Common issues</h3>

<pre><code class="bash"># Error: rendered manifests contain a resource that already exists
# Reason #1: deployment was initially done by helm, but deleted by kubectl
# Solution:
kubectl get crd
kubectl get delete &lt;crd1&gt; &lt;crd2&gt; &lt;crd3&gt; ...

# Reason #2: resources created manually but getting upgraded by helm
# Solution:
kubectl patch namespace &lt;namespace&gt; --patch-file &lt;updated_yaml_to_use&gt;
kubectl annotate &lt;kind&gt; &lt;name&gt; meta.helm.sh/release-namespace=&lt;helm_will_tell_you_what_to_put_here&gt; # For example, kind can be "namespace", name can be "namespace name".
kubectl annotate &lt;kind&gt; &lt;name&gt; meta.helm.sh/release-name=&lt;helm_will_tell_you_what_to_put_here&gt;</code></pre>

<pre><code class="bash"># "updated_yaml_to_use" should look like
apiVersion: v1
kind: Namespace
metadata:
  name: &lt;namespace_name&gt;
  labels:
    app.kubernetes.io/part-of: {{ .Release.Name }}
    app.kubernetes.io/instance: {{ .Release.Name }}
    app.kubernetes.io/managed-by: {{ .Release.Service }}
    helm.sh/chart: {{ .Chart.Name }}-{{ .Chart.Version }}</code></pre>

<pre><code class="bash"># Error: UPGRADE FAILED: another operation (kill/upgrade/rollback) is in progress
# Reason: "helm status &lt;release_name&gt;" or "helm history &lt;release_name&gt;" will show that the last helm install is still pending
# Solution:
helm uninstall &lt;release_name&gt;
</code></pre>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://helm.sh/docs/topics/charts">Charts</a>
  </div>
</div>

<div class="card mb-4" id="kubernetes-6">
  <div class="card-body">
    <h2 class="card-title">Skaffold</h2>
    <p class="card-text">Simple pipeline for CI/CD targeting K8s</p>

<pre><code class="bash"># Run skaffold
skaffold run -f &lt;skaffold.yaml&gt;</code></pre>

<pre><code class="bash"># This is equivalent to
# kubectl port-forward deployment/my-deployment 8080:9000
portForward:
- resourceType: deployment
  resourceName: my-deployment
  namespace: my-namespace
  port: 8080
  localPort: 9000</code></pre>

<pre><code class="bash"># Build docker locally
build:
  artifacts:
  - image: gcr.io/k8s-skaffold/example
  local:
    useDockerCLI: false
    useBuildkit: false</code></pre>

<pre><code class="bash"># Split registry, repository, tag
build:
  artifacts:
  - image: gcr.io/k8s-skaffold/example
deploy:
  helm:
    releases:
      - name: my-chart
        chartPath: helm
        artifactOverrides:
          imageKey: gcr.io/k8s-skaffold/example
        imageStrategy:
          helm:
            explicitRegistry: true</code></pre>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://skaffold.dev/docs/references/yaml">skaffold.yaml</a>
  </div>
</div>

<div class="card mb-4" id="kubernetes-7">
  <div class="card-body">
    <h2 class="card-title">Datadog and kube state metric</h2>

    <h3 class="card-title">Datadog</h3>
    <ul>
      <li>Collects logs from K8s infrastructure (Nodes, API server, etc)</li>
      <li>Agent - K8s DeamonSet. Collects logs from each node</li>
      <li>Cluster agent - K8s Deployment. Monitors API server. Deployed in a set of 3, 5, ... to achieve redundancy</li>
    </ul>

    <h3 class="card-title">KSM</h3>
    <ul>
      <li>Listens to K8s API server and generates metrics</li>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://github.com/kubernetes/kube-state-metrics">kube-state-metrics</a>
  </div>
</div>

<div class="card mb-4" id="kubernetes-8">
  <div class="card-body">
    <h2 class="card-title">Kured (Kubernetes REbook Daemon)</h2>
    <ul>
      <li>DaemonSet that performs safe node reboots when needed (Indicated by package management system)</li>
      <li>Only one node reboots at a time</li>
      <li>Drains nodes before reboot</li>
      <li>Check if <code>/var/run/reboot-required</code> every 6 minutes by default</li>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://github.com/weaveworks/kured">kured - Kubernetes Reboot Daemon</a>
  </div>
</div>

<div class="card mb-4" id="kubernetes-9">
  <div class="card-body">
    <h2 class="card-title">Istio</h2>

    <h3 class="card-title">Service mesh</h3>
    <ul>
      <li>Automatically encrypt traffic between microservices</li>
      <li>Provide application network telemetry and observability</li>
      <li>Provide advanced traffic control</li>
    </ul>

    <h3 class="card-title">Istio</h3>
    <ul>
      <li>Injected into applications as sidecar containers</li>
      <li>Sets up rules to intercept all network traffic entering and exiting the Pod</li>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://www.educative.io/courses/the-kubernetes-course">Learn Kubernetes: A Deep Dive</a>
  </div>
</div>

<div class="card mb-4" id="kubernetes-10">
  <div class="card-body">
    <h2 class="card-title">Kubernetes common issues</h2>

    <h3 class="card-title">CrashLoopBackedOff &#38; Back-off restarting failed containers</h3>
    <ul>
      <li>If liveness probe is defined, see if it succeeds</li>
      <li>Check resource memory</li>
      <li>Check <code>command</code> and <code>args</code> to see if container is set to exit soon</li>
    </ul>

    <h3 class="card-title">How to remove a namespace that is stuck on terminating</h3>

<pre><code class="bash">kubectl get namesapce &lt;stuck_namespace&gt; -o json > temp.json

# Update "temp.json" such that "finalizers" array is empty. For example,
"spec": {
  "finalizers": [

  ]
}

kubectl replace --raw "/api/v1/namespaces/&lt;stuck_namespace&gt;/finalize" -f ./temp.json</code></pre>

    <h3 class="card-title">Kubectl command fails</h3>
    <ul>
      <li>Error from server (InternalError): an error on the server ("") has prevented the request from succeeding</li>
      <li>check proxy setting</li>
    </ul>
  </div>
  <div class="card-footer text-muted">

  </div>
</div>

<div class="card mb-4" id="kubernetes-21">
  <div class="card-body">
    <h2 class="card-title">Namespace</h2>
    <ul>
      <li>Pods in different namespaces can still communicate each other</li>
      <ul>
        <li>Pod can use short name if Service is in the same namespace</li>
        <li>Pod must use FQDN if Service is in a different namespace</li>
      </ul>
      <li>Every cluster has subnets and every namespace partitions this subnets</li>
    </ul>

<pre><code class="yaml">apiVersion: v1
kind: Namespace
metadata:
  name: my-namespace
  labels:
    name: my-namespace
  ...</code></pre>
  </div>
  <div class="card-footer text-muted">

  </div>
</div>

<div class="card mb-4" id="kubernetes-22">
  <div class="card-body">
    <h2 class="card-title">Deployments</h2>
    <ul>
      <li>Provides declarative updates for Pods</li>
      <li>Provides rolling update (zero downtime) by creating new ReplicaSets whenever change is made</li>
      <li>Provides rollbacks by maintaining old ReplicaSets</li>
      <li>Deployment Controller change an actual state to a desired state at a controlled rate</li>
      <li>Deployment ensures that at least 75% of Pods are up while they are being updated</li>
      <li>It also ensures that at most 125% of the desired number of Pods are up</li>
    </ul>

    <h3 class="card-title">Create a Deployment</h3>

<pre><code class="yaml">apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-deployment  # Deployment named ".metadata.name" is created
  labels:
    app: app-name
spec:
  replicas: 3  # Three replicated Pods. If this field does not exist, it will default to 1
  selector:  # Required field for "spec". This specifies the label selector of Pod targeted by this Deployment
    matchLabels:
      app: app-name  # How Deployment finds which Pods to manage
  minReadySeconds: 10  # Wait 10 seconds between each Pod being updated
  strategy:
    type: RollingUpdate
    rollingUpdate:
      # In this setting, rolling update will updates two Pods at a time
      maxUnavailable: 1  # Never have more than 1 Pod below the desired state
      maxSurge: 1  # Never have more than 1 Pod above the desired state
  template:  # Required field for "spec". This is a Pod template, which has the same schema as Pod
    metadata:
      labels:
        app: app-name  # Pod label. This must match ".spec.selector"
    spec:
      containers:
      - name: container-name
        image: image-name:1.0.0
        ports:
        - containerPort: 80</code></pre>

    <h3 class="card-title">kubectl get deployments</h3>

    <img class="img-fluid" class="card-img-top" src="/platform-engineering/platform-engineering/kubernetes12.png" style="width: 600px; height: 100px" alt="Card image cap">

    <ul>
      <li>NAME - names of Deployments in the namespace</li>
      <li>READY - how many replicas of the application are available to users</li>
      <li>UP TO DATE - number of replicas updated to achieve the desired state</li>
      <li>AVAILABLE - how many replicas of the application are available to users</li>
      <li>AGE - amount of time the appliation has been running</li>
    </ul>

    <h3 class="card-title">Rollover</h3>
    <ul>
      <li>Everytime a new Deployment is observed by Deployment Controller, a ReplicaSet is created to bring up the desired Pods</li>
      <li>If Deployment is updated, the existing ReplicaSet that control Pods whose labels match <code>.spec.selector</code> but whose template does not match <code>.spec.template</code> are scaled down</li>
      <li>Eventually, new ReplicaSet is scaled to <code>.spec.replicas</code> and all old ReplicaSets is scaled to 0</li>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment">Deployment</a>
  </div>
</div>

<div class="card mb-4" id="kubernetes-23">
  <div class="card-body">
    <h2 class="card-title">ReplicaSet</h2>
    <ul>
      <li>Generally, ReplicaSet should not be manipulated. Rather, Deployment should be used</li>
      <li>ReplicaSet is mapped to Pod by Pod's metadata.ownerReferences field</li>
      <li>Runs a background reconciliation loop, constantly checking whether the right number of Pod are present</li>
    </ul>

    <h3 class="card-title">Scale-down a ReplicaSet</h3>
    <ul>
      <li>Pending Pods are scaled-down first</li>
      <li>If controller.kubernetes.io/pod-deletion-cost annotation is set, Pods with lower value are scaled-down second</li>
      <li>Pods on Nodes with more replicas are scaled-down thrid</li>
      <li>Pods created recently are scaled-down fourth</li>
    </ul>

    <h3 class="card-title">kubectl get replicaset</h3>

    <img class="img-fluid" class="card-img-top" src="/platform-engineering/platform-engineering/kubernetes13.png" style="width: 600px; height: 100px" alt="Card image cap">

    <ul>
      <li>NAME - names of ReplicaSets in the namespace</li>
      <li>DESIRED - desired number of replicas in the application</li>
      <li>CURRENT - how many applications are currently running</li>
      <li>READY - how many replicas of the application are available to users</li>
      <li>AGE - amount of time the appliation has been running</li>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://kubernetes.io/docs/concepts/workloads/controllers/replicaset">ReplicaSet</a>
  </div>
</div>

<div class="card mb-4" id="kubernetes-24">
  <div class="card-body">
    <h2 class="card-title">StatefulSet</h2>
    <ul>
      <li>Predictable and persistent Pod names, DNS hostnames, volume bindings</li>
      <li>Create one Pod at a time and always wait for previous Pods to be running and ready before creating the next</li>
      <ul>
        <li>This is the same for delete</li>
      </ul>
      <li>When a StatefulSet Pod is created, any volumes it needs are created at the same time and named in a way to connect them to the right Pod</li>
    </ul>

    <h3 class="card-title">Use cases</h3>
    <ul>
      <li>Clustered apps that store data cannot have multiple replicas go down at the same time</li>
    </ul>

    <h3 class="card-title">Limitations</h3>
    <ul>
      <li>Retention policy of PersistentVolumes must be defined in the StorageClass</li>
      <li>Deleting StatefulSet does not delete volumns associated with it</li>
      <ul>
        <li>When scaling down, Pod is gone but PVC is still alive</li>
        <li>When scaling up, Pod is attached to surviving PVC</li>
      </ul>
      <li>StatefulSet requires Headless Service for the network identity of the Pods</li>
      <li>StatefulSet does not guarantee on the termination of Pods when Statefulset gets deleted. This means Pods do not get terminated in order</li>
    </ul>

    <h3 class="card-title">Headless service</h3>
    <ul>
      <li>Creates predictable and stable DNS entries for every Pod that matches the StatefulSets label selector</li>
      <li>Service must set the value <code>clusterIP</code> to <code>None</code></li>
    </ul>

    <h3 class="card-title">Example</h3>
    <ul>
      <li>Three Pods will be deployed in the order web-0, web-1, web-2</li>
      <li>web-1 will not be deployed until web-0 is Running and Ready</li>
      <li>When scaling down, web-1 will not be terminated until web-2 is fully shutdown</li>
    </ul>

<pre><code class="yaml">apiVersion: v1
kind: Service
metadata:
  name: my-headless-service
  labels:
    app: my-headless-service
spec:
  ports:
  - port: 80
    name: web
  clusterIP: None  # Only difference between headless service and regular service is this field
  selector:
    app: my-headless-service

---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: my-statefulset
spec:
  selector:
    matchLabels:
      app: my-statefulset  # Has to match .spec.template.metadata.labels
  serviceName: my-headless-service  # Has to be the name of headless service
  replicas: 3  # by default is 1.
  template:
    metadata:
      labels:
        app: my-statefulset  # Has to match .spec.selector.matchLabels
    spec:
      terminationGracePeriodSeconds: 10
      containers:
      - name: my-container
        image: my-image
        ports:
        - containerPort: 80
        volumeMounts:
        - name: my-volumemount
          mountPath: /path/to/mount
  volumeClaimTemplates:
  - metadata:
      name: my-volumemount
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: my-storageclass
      resources:
        requests:
          storage: 1Gi</code></pre>

    <h3 class="card-title">Recreate Pod from Statefulset with new volume</h3>

<pre><code class="bash"># The terminal will get stuck after issuing this command
kubectl delete pvc &lt;pvc_name&gt;

# Do this on a different terminal
kubectl delete statefulset &lt;statefulset_name&gt; --cascade=false
kubectl delete pod &lt;pod_name&gt;
kubectl apply -f &lt;statefulset_yaml&gt;</code></pre>

  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset">StatefulSet</a> | <a href="https://www.educative.io/courses/the-kubernetes-course">Learn Kubernetes: A Deep Dive</a>
  </div>
</div>

<div class="card mb-4" id="kubernetes-25">
  <div class="card-body">
    <h2 class="card-title">DaemonSet</h2>
    <ul>
      <li>A Pod exists in each node</li>
      <li>Useful for monitoring and logging Pods</li>
    </ul>

<pre><code class="yaml">apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: my-daemonset
  namespace: kube-system
  labels:
    k8s-app: logging
spec:
  selector:
    matchLabels:
      name: my-daemonset
  template:
    metadata:
      labels:
        name: my-daemonset
    spec:
      tolerations:
      # this toleration is to have the daemonset runnable on master nodes
      # remove it if your masters can't run pods
      - key: node-role.kubernetes.io/master
        operator: Exists
        effect: NoSchedule
      containers:
      - name: my-container
        image: my-image
        resources:
          limits:
            memory: 200Mi
          requests:
            cpu: 100m
            memory: 200Mi
        volumeMounts:
        - name: varlog
          mountPath: /var/log
        - name: varlibdockercontainers
          mountPath: /var/lib/docker/containers
          readOnly: true
      terminationGracePeriodSeconds: 30
      volumes:
      - name: varlog
        hostPath:
          path: /var/log
      - name: varlibdockercontainers
        hostPath:
          path: /var/lib/docker/containers</code></pre>

  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://kubernetes.io/docs/concepts/workloads/controllers/daemonset">DaemonSet</a>
  </div>
</div>

<div class="card mb-4" id="kubernetes-26">
  <div class="card-body">
    <h2 class="card-title">Jobs</h2>
    <ul>
      <li>A Job creates one more more Pods</li>
      <li>It reliably runs one Pod to completion</li>
      <li>Deleting a Job will clean up Pods it created</li>
      <li>Suspending a Job will delete its active Pods</li>
      <li>A Job is better than bare Pod because it can automatically replace failed Pod with new one</li>
      <li>While Replication Controller manages Pods that are not expected to terminate, Job manages Pods that are expected to terminate</li>
    </ul>

<pre><code class="yaml">apiVersion: batch/v1
kind: Job
metadata:
  name: pi
spec:
  template:
    spec:
      containers:
      - name: pi
        image: perl
        command: ["perl",  "-Mbignum=bpi", "-wle", "print bpi(2000)"]
      restartPolicy: Never
  backoffLimit: 4</code></pre>

<pre><code class="bash"># To list all the Pods that belong to a Job
run pods=$(kubectl get pods --selector=job-name=pi --output=jsonpath='{.items[*].metadata.name}') && echo $pods"</code></pre>

    <h3 class="card-title">Three types of tasks suitable to run as a Job</h3>
    <ul>
      <li>Non-parallel Jobs - normally only one Pod is started. Job is complete as soon as its Pod terminates successfully. Can leave both <code>.spec.completions</code> and <code>.spec.parallelism</code> unset (they will default to 1)</li>
      <li>Parallel Jobs with a fixed completion count - Job is complete when there is one successful Pod for each value in the range 1 to <code>.spec.completions</code></li>
      <li>Parallel Jobs with a work queue - when any Pod from the Job terminates with success, no new Pods are created. Once at least one Pod is terminated with success and all Pods are terminated, Job succeeds. Must leave <code>.spec.completions</code> unset and set <code>.spec.parallelism</code> to a non-negative integer</li>
    </ul>

    <p class="card-text">Requested parallelism <code>.spec.parallelism</code> is set to 1 if not specified. Setting it to 0 makes Job effective paused</p>
    <ul>
      <li>Fixed completion count Jobs - actual number of Pods running in parallel will not exceed the number of remaining completions. Higher value of <code>.spec.parallelism</code> is ignored</li>
      <li>Work queue Jobs - no new Pods are started after any Pod has succeeded</li>
    </ul>

    <h3 class="card-title">Pod and container failure</h3>
    <ul>
      <li>If container fails and <code>.spec.template.spec.restartPolicy = "OnFailure"</code>, Pod stays on the node but container re-runs. You can avoid this by <code>.spec.template.spec.restartPolicy = "Never"</code></li>
      <li>If Pod fails, then Job Controller starts a new Pod</li>
      <li><code>.spec.backoffLimit</code> is specifiy number fo retries before marking Job as failure (Default is 6)</li>
    </ul>

    <h3 class="card-title">Job termination and clean up</h3>

<pre><code class="bash"># Delete the Job, all the Pods created by that Job are deleted too.
kubectl get jobs
kubectl delete jobs/&lt;job_name&gt;</code></pre>

    <p class="card-text">Setting <code>.spec.activeDeadlineSeconds</code> will make Job fail and terminate all running Pods once <code>activeDeadlineSeconds</code> is reached</p>

<pre><code class="yaml">apiVersion: batch/v1
kind: Job
metadata:
  name: pi-with-timeout
spec:
  backoffLimit: 5
  activeDeadlineSeconds: 100
  template:
    spec:
      containers:
      - name: pi
        image: perl
        command: ["perl",  "-Mbignum=bpi", "-wle", "print bpi(2000)"]
      restartPolicy: Never</code></pre>

  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://kubernetes.io/docs/concepts/workloads/controllers/job">Jobs</a>
  </div>
</div>

<div class="card mb-4" id="kubernetes-27">
  <div class="card-body">
    <h2 class="card-title">Cron Job</h2>
    <ul>
      <li>It can create Jobs on a repeating schedule or any individual tasks</li>
    </ul>

<pre><code class="yaml">apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: hello
spec:
  schedule: "*/1 * * * *"
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: hello
            image: busybox
            imagePullPolicy: IfNotPresent
            command:
            - /bin/sh
            - -c
            - date; echo Hello from the Kubernetes cluster
          restartPolicy: OnFailure</code></pre>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs">CronJob</a>
  </div>
</div>

<div class="card mb-4" id="kubernetes-28">
  <div class="card-body">
    <h2 class="card-title">Pods</h2>
    <ul>
      <li>The smallest deployable unit</li>
      <li>Similar to Docker containers with shared namespace and volumn</li>
      <li>Pod gets created by resources such as Deployment, Job, or StatefulSet</li>
      <li>Controller for those resources handles Pod replication, rollout, and failure</li>
      <li>Controllers create Pod from Pod Template</li>
      <li>Every container in a Pod share the same IP address and port range. These containers can communicate to each other using localhost</li>
      <li>Any container in a Pod can enable privileged mode to use OS admin level capabilities</li>
      <li>Static Pods are managed directly by kubelet without API server. Kubelet though will create mirror Pods on API server for each static Pod</li>
      <li>Pods are created, assinged a unique ID (UUID), and scheduled to nodes. They can never be rescheduled to different nodes</li>
    </ul>

    <h3 class="card-title">Pod Lifecycle</h3>
    <img class="img-fluid" class="card-img-top" src="/platform-engineering/platform-engineering/kubernetes14.png" style="width: 500px; height: 300px;" alt="Card image cap">
    <ul>
      <li><strong>Pending</strong> - containers have not been setup yet</li>
      <li><strong>Running</strong> - Pod is bounded to a node. Containers are created but still running</li>
      <li><strong>Succeeded</strong> (<strong>Completed</strong> when <code>restartPolicy:Never</code>) - containers are terminated with success</li>
      <li><strong>Failed</strong> (<strong>CrashLoopBackoff</strong> when Pod fails or exits unexpectedly) - at least one container is terminated with failure</li>
      <li><strong>Unknown</strong> - Pod status cannot be obtained. Most often error communicating with the node</li>
    </ul>

    <h3 class="card-title">Container lifecycle</h3>
    <ul>
      <li><strong>Waiting</strong> - running operations to complete startup</li>
      <li><strong>Running</strong> - executing without issues</li>
      <li><strong>Terminated</strong> - either ran to completion or failed</li>
    </ul>

    <h3 class="card-title">Container restart policy</h3>
    <ul>
      <li><code>spec</code> of Pod has <code>restartPolicy</code>, which has <code>Always</code>, <code>OnFailure</code>, <code>Never</code>. Default is <code>Always</code>.</li>
    </ul>

    <h3 class="card-title">Pod condition</h3>
    <ul>
      <li><strong>PodScheduled</strong> - Pod is scheduled to a node</li>
      <li><strong>ContainersReady</strong> - all containers in Pod are ready</li>
      <li><strong>Initialized</strong> - all init containers are started</li>
      <li><strong>Ready</strong> - Pod can serve requests</li>
    </ul>

    <h3 class="card-title">Pod Termination</h3>
    <ul>
      <li>Kubelet tool to delete Pod, with default graceful period of 30 seconds</li>
      <li>Control plane removes shutting-down Pods from Endpoints</li>
      <li>Resources no longer treat shutting-down Pods valid</li>
      <li>When the grace period expires, kubelet triggeres forcible shutdown (Contrainer runtime sends SIGKILL to any running processes in containers)</li>
      <li>API server deletes Pod's object</li>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://kubernetes.io/docs/concepts/workloads/pods">Pods | <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle">Pod Lifecycle</a> | <a href="https://www.educative.io/path/become-a-kubernetes-professional">Become a Kubernetes Professional</a>
  </div>
</div>

<div class="card mb-4" id="kubernetes-101">
  <div class="card-body">
    <h2 class="card-title">Init containers</h2>
    <p class="card-text">Specialized containers that run before app containers in Pod</p>
    <ul>
      <li>Init containers always run to completion</li>
      <li>Each init container must succeed before next one can run</li>
      <li>If init container fails, kubelet repeatly restarts the container</li>
      <li>Init containers do not support lifecycle, livenessProbe, readinessProbe, startupProbe because they must run to completion before Pod can be ready</li>
      <li>Init containers can have custom code and no need to use FROM</li>
      <li>Init containers can be given access to Secret (Unlike app containers)</li>
      <li>If Pod restarts, all init containers must run again</li>
      <li>Init container code must be idempotent (Because they can be re-run)</li>
    </ul>

<pre><code class="yaml">apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
spec:
  containers:
  - name: myapp-container
    image: busybox:1.28
    command: ['sh', '-c', 'echo The app is running! && sleep 3600']
  initContainers:
  - name: init-myservice
    image: busybox:1.28
    command: ['sh', '-c', "until nslookup myservice.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for myservice; sleep 2; done"]
  - name: init-mydb
    image: busybox:1.28
    command: ['sh', '-c', "until nslookup mydb.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for mydb; sleep 2; done"]</code></pre>

    <p class="card-text">Init containters would be waiting to discover Services named myservice and mydb</p>

<pre><code class="yaml">---
apiVersion: v1
kind: Service
metadata:
  name: myservice
spec:
  ports:
  - protocol: TCP
    port: 80
    targetPort: 9376
---
apiVersion: v1
kind: Service
metadata:
  name: mydb
spec:
  ports:
  - protocol: TCP
    port: 80
    targetPort: 9377</code></pre>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://kubernetes.io/docs/concepts/workloads/pods/init-containers">Init Containers</a>
  </div>
</div>

<div class="card mb-4" id="kubernetes-102">
  <div class="card-body">
    <h2 class="card-title">Pod Topology Spread Constraints</h2>

    <img class="img-fluid" class="card-img-top" src="/platform-engineering/platform-engineering/kubernetes2.png" style="width: 700px; height: 300px;" alt="Card image cap">

<pre><code class="yaml">kind: Pod
apiVersion: v1
metadata:
  name: mypod
  labels:
    foo: bar
spec:
  topologySpreadConstraints:
  - maxSkew: 1
    topologyKey: zone
    whenUnsatisfiable: DoNotSchedule
    labelSelector:
      matchLabels:
        foo: bar
  containers:
  - name: pause
    image: k8s.gcr.io/pause:3.1</code></pre>

    <p class="card-text">If a new Pod goes to Zone A, then the skew will be 3-1=2, which will exceed the maxSkew of 1. Thus, it can only go to Zone B such that</p>

    <img class="img-fluid" class="card-img-top" src="/platform-engineering/platform-engineering/kubernetes3.png" style="width: 700px; height: 700px;" alt="Card image cap">

    <img class="img-fluid" class="card-img-top" src="/platform-engineering/platform-engineering/kubernetes4.png" style="width: 700px; height: 300px;" alt="Card image cap">

<pre><code class="yaml">kind: Pod
apiVersion: v1
metadata:
  name: mypod
  labels:
    foo: bar
spec:
  topologySpreadConstraints:
  - maxSkew: 1
    topologyKey: zone
    whenUnsatisfiable: DoNotSchedule
    labelSelector:
      matchLabels:
        foo: bar
  - maxSkew: 1
    topologyKey: node
    whenUnsatisfiable: DoNotSchedule
    labelSelector:
      matchLabels:
        foo: bar
  containers:
  - name: pause
    image: k8s.gcr.io/pause:3.1</code></pre>

    <p class="card-text">A new Pod can only go to Zone B to meet the maxSkew of 1 in the first constraint. However at the same time, it can only go to Node 2 to meet the maxSkew of 1 in the second constraint. Because whenUnsatisfiable is DoNotSchedule in both constraints, new Pod cannot be scheduled (It would be scheduled if whenUnsatisfiable is ScheduleAnyway)</p>
    <img class="img-fluid" class="card-img-top" src="/platform-engineering/platform-engineering/kubernetes5.png" style="width: 700px; height: 700px;" alt="Card image cap">

<pre><code class="yaml">kind: Pod
apiVersion: v1
metadata:
  name: mypod
  labels:
    foo: bar
spec:
  topologySpreadConstraints:
  - maxSkew: 1
    topologyKey: zone
    whenUnsatisfiable: DoNotSchedule
    labelSelector:
      matchLabels:
        foo: bar
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: zone
            operator: NotIn
            values:
            - zoneC
  containers:
  - name: pause
    image: k8s.gcr.io/pause:3.1</code></pre>

    <p class="card-text">This will exclude Zone C from the constraint such that a new Pod goes to Zone B rather than Zone C</p>

    <h4 class="card-title">Cluster-Level Default Constraints</h4>

<pre><code class="yaml">apiVersion: kubescheduler.config.k8s.io/v1beta1
kind: KubeSchedulerConfiguration

profiles:
  - pluginConfig:
      - name: PodTopologySpread
        args:
          defaultConstraints:
            - maxSkew: 1
              topologyKey: topology.kubernetes.io/zone
              whenUnsatisfiable: ScheduleAnyway
          defaultingType: List</code></pre>

    <ul>
      <li>Pod Affinity - can place any number of Pods into qualifying topology domains</li>
      <li>Pod Anti-Affinity - can only place one Pod into a single topology domain</li>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints">Pod Topology Spread Constraints</a>
  </div>
</div>

<div class="card mb-4" id="kubernetes-103">
  <div class="card-body">
    <h2 class="card-title">Multi-container pod design</h2>
    <ul>
      <li>Each pod can have multiple containers (Which would run on the same node)</li>
      <li>Communication between containers is faster and securer, and containers can share volumns and file systems</li>
    </ul>

    <h4 class="card-title">Sidecar</strong></h4>
    <ul>
      <li>Enhance/extend existing functionality of container</li>
      <li>For example, an app container can stream logs to a particular location while the sidecar container mounts the logs to some other directory</li>
    </ul>

<pre><code class="yaml">apiVersion: v1
kind: Pod
metadata:
  name: sidecar-pod
spec:
  volumes:
  - name: logs
    emptyDir: {}
  containers:
  - name: app-container
    image: alpine
    command: ["/bin/sh"]
    args: ["-c", "while true; do date >> /var/log/app.log; sleep 2;done"]
    volumeMounts:
    - name: logs
      mountPath: /var/log
  - name: log-exporter-sidecar
    image: nginx
    ports:
      - containerPort: 80
    volumeMounts:
    - name: logs
      mountPath: /usr/share/nginx/html</code></pre>
    <p class="card-text">"app-container" streams logs to /var/log/app.log while "log-exporter-sidecar" mounts those logs into /usr/share/nginx/html</p>

    <h4 class="card-title">Ambassador</h4>
    <ul>
      <li>Serves as a proxy to external worlds (This for for legacy apps, ConfigMap should be used for new apps)</li>
      <li>For example, when connecting to a DB server and that server config changes across different environments, the ambassador container can act as a TCP proxy to the database, which can be connected via localhost. The sysadmin can use config maps and secrets with the proxy container to inject the correct connection and auth information</li>
    </ul>

<pre><code class="yaml">apiVersion: v1
kind: Pod
metadata:
  name: ambassador-pod
  labels:
    app: ambassador-app
spec:
  volumes:
  - name: shared
    emptyDir: {}
  containers:
  - name: app-container-poller
    image: yauritux/busybox-curl
    command: ["/bin/sh"]
    args: ["-c", "while true; do curl 127.0.0.1:81 > /usr/share/nginx/html/index.html; sleep 10; done"]
    volumeMounts:
    - name: shared
      mountPath: /usr/share/nginx/html
  - name: app-container-server
    image: nginx
    ports:
      - containerPort: 80
    volumeMounts:
    - name: shared
      mountPath: /usr/share/nginx/html
  - name: ambassador-container
    image: bharamicrosystems/nginx-forward-proxy
    ports:
      - containerPort: 81</code></pre>

    <p class="card-text">"app-container-poller" call on port 81 and send stuff to /usr/share/nginx/html/index.html. "app-container-server" listens on  port 80. These two containers share the same mount point. Lastly, "ambassador-container" listens on port 81, so that when users curl on 80 they get response from html page</p>

    <h4 class="card-title">Adaptor</h4>
    <ul>
      <li>Help standarized heterogeneous system</li>
      <li>For example, when there are multiple applications running on separate containers that are outputing logs in different formats, the adaptor container can standardize logs</li>
    </ul>

    <h4 class="card-title">Service mesh</h4>
    <ul>
      <li>A proxy container is inserted into every application Pod</li>
      <li>Proxy container handles all network traffic entering and leaving the Pod</li>
    </ul>

<pre><code class="yaml">apiVersion: v1
kind: Pod
metadata:
  name: adapter-pod
  labels:
    app: adapter-app
spec:
  volumes:
  - name: logs
    emptyDir: {}
  containers:
  - name: app-container
    image: alpine
    command: ["/bin/sh"]
    args: ["-c", "while true; do date >> /var/log/app.log; sleep 2;done"]
    volumeMounts:
    - name: logs
      mountPath: /var/log
  - name: log-adapter
    image: alpine
    command: ["/bin/sh"]
    args: ["-c", "tail -f /var/log/app.log|sed -e 's/^/Date /' > /var/log/out.log"]
    volumeMounts:
    - name: logs
      mountPath: /var/log</code></pre>
    <p class="card-text">"app-container" outputs stream of dates in log file while "log-adapter" appends a word to those stream of dates</p>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://betterprogramming.pub/understanding-kubernetes-multi-container-pod-patterns-577f74690aee">Understanding Kubernetes Multi-Container Pod Patterns</a>
  </div>
</div>

<div class="card mb-4" id="kubernetes-104">
  <div class="card-body">
    <h2 class="card-title">Disruptions</h2>
    <p class="card-text">There are involuntary disruptions</p>
    <ul>
      <li>Hardware failure</li>
      <li>Kernal panic</li>
      <li>Cloud provider issue</li>
      <li>Network issue</li>
      <li>Pod eviction due to Node having out of resource</li>
    </ul>

    <p class="card-text">There are voluntary disruptions. Application owners can</p>
    <ul>
      <li>Delete the Deployment</li>
      <li>Update the Deployment, causing a restart</li>
      <li>Directly delete Pods by accident</li>
    </ul>

    <p class="card-text">Cluster admins can</p>
    <ul>
      <li>Drain a Node for repair or scale down</li>
      <li>Remove a Pod from a Node to fit in something else</li>
    </ul>

    <p class="card-text">Pod disruption budgets (PDB)</p>
    <ul>
      <li>Limits the number of Pods down simultaneously from voluntary disruptions</li>
    </ul>

    <p class="card-text">Consider the following scenario where Pod-a, Pod-b, Pod-c are subject to PDB (whose requirement is that at least 2 out of 3 Pods must be available) while Pod-x is not</p>

    <img class="img-fluid" class="card-img-top" src="/platform-engineering/platform-engineering/kubernetes6.png" style="width: 700px; height: 300px" alt="Card image cap">

    <p class="card-text">Now the cluster admin drains Node 1, which will cause Pod-a and Pod-x to start terminating</p>

    <img class="img-fluid" class="card-img-top" src="/platform-engineering/platform-engineering/kubernetes7.png" style="width: 700px; height: 300px" alt="Card image cap">

    <p class="card-text">Deployment notices that Pods are terminating, and to reinstate the desired state, it creates replacement Pods (Pod-d and Pod-y)</p>

    <img class="img-fluid" class="card-img-top" src="/platform-engineering/platform-engineering/kubernetes8.png" style="width: 700px; height: 300px" alt="Card image cap">

    <p class="card-text">The cluster admin now attempts to drain Node 2 and Node 3. However, the drain command will block because of PDB</p>

    <img class="img-fluid" class="card-img-top" src="/platform-engineering/platform-engineering/kubernetes9.png" style="width: 700px; height: 300px" alt="Card image cap">

    <p class="card-text">At this point, there are three availabe Pods that are subject to PDB</p>

    <img class="img-fluid" class="card-img-top" src="/platform-engineering/platform-engineering/kubernetes10.png" style="width: 700px; height: 300px" alt="Card image cap">

    <p class="card-text">The cluster admin now attempts to drain Node 2. Either one of Pod-b or Pod-d will be evicted but both cannot be eviced due to PDB. Assuming Pod-b got evicted, the Deployment will create a replacement Pod-e. But since there are not enough resources in Node 2 and 3, the drain will block</p>

    <img class="img-fluid" class="card-img-top" src="/platform-engineering/platform-engineering/kubernetes11.png" style="width: 700px; height: 300px" alt="Card image cap">
  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://kubernetes.io/docs/concepts/workloads/pods/disruptions">Disruptions</a>
  </div>
</div>

<div class="card mb-4" id="kubernetes-105">
  <div class="card-body">
    <h2 class="card-title">Application Resource Requirement</h2>
    <p class="card-text"><code>Mib</code> indicates the momory size based on 2's power</p>
    <p class="card-text">For example, a mebibyte is 1,048,576 (2<sup>20</sup>) bytes while a megabyte is 1,000,000 (10<sup>6</sup>) bytes</p>

<pre><code class="yaml">apiVersion: v1
kind: Pod
metadata:
  name: memory-demo
  namespace: mem-example
spec:
  containers:
  - name: memory-demo-ctr
    image: polinux/stress
    resources:
      limits:
        memory: "200Mi" # Containers cannot exceed this
      requests:
        memory: "100Mi" # Containers are gunaranteed to have this much
    command: ["stress"]
    args: ["--vm", "1", "--vm-bytes", "150M", "--vm-hang", "1"] # Attempt to allocate 150MiB of memory. Containers can exceed the memory requests as long as Node has memory available</code></pre>
    <p class="card-text">If containers allocate more memory than its limit, they will eventually terminate</p>

<pre><code class="yaml">apiVersion: v1
kind: Pod
metadata:
  name: cpu-demo
  namespace: cpu-example
spec:
  containers:
  - name: cpu-demo-ctr
    image: vish/stress
    resources:
      limits:
        cpu: "1"
      requests:
        cpu: "0.5"
    args:
    - -cpus
    - "2"</code></pre>
    <p class="card-text">If specify limit but no request, K8s automatically assigns CPU request that matches the limit</p>

    <h4 class="card-title">If no CPU/memory limit</h4>
    <ul>
      <li>Container can use all the CPU/memory in the Node (Until it invokes OOM killer)</li>
      <li>Or, container is running in namespace with a default CPU/memory limit</li>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://kubernetes.io/docs/tasks/configure-pod-container/assign-memory-resource">Assign Memory Resources to Containers and Pods</a> | <a href="https://kubernetes.io/docs/tasks/configure-pod-container/assign-cpu-resource">Assign CPU Resources to Containers and Pods</a>
  </div>
</div>

<div class="card mb-4" id="kubernetes-106">
  <div class="card-body">
    <h2 class="card-title">LivenesProbe and ReadinessProbe</h2>

    <h3 class="card-title">Container probe</h3>
    <ul>
      <li>Kubelet performs diagnostic on a container periodically (This is call Probe)</li>
      <li>Kubelet calls Handler, which is implemented by the container</li>
      <li>ExecAction Handler - executes a command inside container. Diagnostic successful if command exits with 0</li>
      <li>TCPSocketAction Handler - TCP check on IP address on specified port. Diagnostic successful if port is open</li>
      <li>HTTPGetAction Handler - HTTP GET check on IP address on specified port and path. Diagnostic successful if 200 &le; response &lt; 400</li>
    </ul>

    <h3 class="card-title">livenessProbe</h3>
    <ul>
      <li>Indicates whether the container is running</li>
      <li>Similar to readiness probe, but probes periodically to make sure the pod is healthy and restarts it if not</li>
      <li>If liveness probe fails, kubelet kills the container, and container is subject to its restart policy</li>
    </ul>

    <h3 class="card-title">readinessProbe</h3>
    <ul>
      <li>Indicates whether the container is ready to respond to requests</li>
      <li>If readiness probe fails, endpoint controller removes Pod IP address from Service endpoints that match the Pod</li>
      <li>Used when container needs to load large data, configuration files</li>
    </ul>

    <h3 class="card-title">startupProbe</h3>
    <ul>
      <li>Indicates whether the application within the container has started</li>
      <li>If starup probe fails, kubelet kills the container, and container is subject to its restart policy</li>
      <li>Used when containers take long time to come into service</li>
    </ul>

<pre><code class="yaml">apiVersion: v1
kind: Pod
metadata:
  labels:
    test: liveness
  name: liveness-exec
spec:
  containers:
  - name: liveness
    image: k8s.gcr.io/busybox
    args:
    - /bin/sh
    - -c
    - touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600
    livenessProbe: # kubelet executes command "cat /tmp/healthy" in the target container. If 0 is returned, then container is healthy. Otherwise, kubelet kills the container and restarts it
      exec:
        command:
        - cat
        - /tmp/healthy
      initialDelaySeconds: 5 # kubelet should wait 5 seconds before performing the first probe
      periodSeconds: 5 # kubelet should perform liveness probe every 5 seconds</code></pre>

<pre><code class="yaml">apiVersion: v1
kind: Pod
metadata:
  labels:
    test: liveness
  name: liveness-http
spec:
  containers:
  - name: liveness
    image: k8s.gcr.io/liveness
    args:
    - /server
    livenessProbe: # kubelet sends HTTP GET request to the server running in the container and listening on port 8080. If status code between 200 and 400 is returned, then container is healthy. Otherwise, kubelet kills the container and restarts it
      httpGet:
        path: /healthz
        port: 8080
        httpHeaders:
        - name: Custom-Header
          value: Awesome
      initialDelaySeconds: 3 # kubelet should wait 3 seconds before performing the first probe
      periodSeconds: 3 # kubelet should perform liveness probe every 3 seconds</code></pre>

<pre><code class="yaml">apiVersion: v1
kind: Pod
metadata:
  name: goproxy
  labels:
    app: goproxy
spec:
  containers:
  - name: goproxy
    image: k8s.gcr.io/goproxy:0.1
    ports:
    - containerPort: 8080
    readinessProbe:
      tcpSocket:
        port: 8080
      initialDelaySeconds: 5
      periodSeconds: 10
    livenessProbe:
      tcpSocket:
        port: 8080
      initialDelaySeconds: 15
      periodSeconds: 20</code></pre>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes">Configure Liveness, Readiness and Startup Probes</a>
  </div>
</div>

<div class="card mb-4" id="kubernetes-107">
  <div class="card-body">
    <h2 class="card-title">Service Account</h2>
    <ul>
      <li>When creating Pod, when service account is not specified, it is automatically assigned <code>default</code> service account in the same namespace</li>
    </ul>

    <h3 class="card-text">Opt-out of automatic service account assignment</h3>

<pre><code class="yaml">apiVersion: v1
kind: ServiceAccount
metadata:
  name: build-robot
  namespace: my-namespace
  automountServiceAccountToken: false
...</code></pre>

    <h3 class="card-text">Opt-out of automatic service account assignment for a specific Pod</h3>

<pre><code class="yaml">apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  serviceAccountName: build-robot
  automountServiceAccountToken: false
  ...</code></pre>

  <h3 class="card-text">Manually create service account</h3>

<pre><code class="yaml">apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-service-account
  namespace: my-namespace
secrets:
- name: my-service-account-secret</code></pre>

    <h3 class="card-text">Manually create service account API token</h3>

<pre><code class="yaml">apiVersion: v1
kind: Secret
metadata:
  name: my-service-account-secret
  annotations:
    kubernetes.io/service-account.name: my-service-account
type: kubernetes.io/service-account-token</code></pre>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account">Configure Service Accounts for Pods</a>
  </div>
</div>

<div class="card mb-4" id="kubernetes-108">
  <div class="card-body">
    <h2 class="card-title">SecurityContext</h2>

    <p class="card-text">Defines privilege and access control for Pod and Container</p>
<pre><code class="yaml">apiVersion: v1
kind: Pod
metadata:
  name: security-context-demo
spec:
  securityContext:
    runAsUser: 1000  # All processes run with user ID 1000 in containers (If omitted, defaults to root(0))
    runAsGroup: 3000  # Any file created in containers is owned by user 1000 and group 3000
    fsGroup: 2000  # All processes of containers are also part of supplementary group 2000
  volumes:
  - name: sec-ctx-vol
    emptyDir: {}
  containers:
  - name: sec-ctx-demo
    image: busybox
    command: [ "sh", "-c", "sleep 1h" ]
    volumeMounts:
    - name: sec-ctx-vol
      mountPath: /data/demo
    securityContext:
      allowPrivilegeEscalation: false</code></pre>

<pre><code class="yaml">apiVersion: v1
kind: Pod
metadata:
  name: security-context-demo-2
spec:
  securityContext:
    runAsUser: 1000
  containers:
  - name: sec-ctx-demo-2
    image: gcr.io/google-samples/node-hello:1.0
    securityContext:
      runAsUser: 2000  # This overrides setting made at Pod level
      allowPrivilegeEscalation: false</code></pre>

<pre><code class="yaml">apiVersion: v1
kind: Pod
metadata:
  name: security-context-demo-4
spec:
  containers:
  - name: sec-ctx-4
    image: gcr.io/google-samples/node-hello:1.0
    securityContext:
      capabilities:
        add: ["NET_ADMIN", "SYS_TIME"]  # Linux capabilities</code></pre>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://kubernetes.io/docs/tasks/configure-pod-container/security-context">Configure a Security Context for a Pod or Container</a>
  </div>
</div>

<div class="card mb-4" id="kubernetes-109">
  <div class="card-body">
    <h2 class="card-title">Labels, selectors, and annotations</h2>
    <ul>
      <li>Labels are key/value pairs enabling users to map their own structures to system objects (for example, Pods) in loosely coupled fashion</li>
      <li>Labels do not need to be unique</li>
    </ul>
<pre><code class="yaml">"metadata": {
  "labels": {
    "key1" : "value1",
    "key2" : "value2"
  }
}</code></pre>

    <p class="card-text">Example, Pods with two labels <code>environment: production</code> and <code>app: nginx</code></p>
<pre><code class="yaml">apiVersion: v1
kind: Pod
metadata:
  name: label-demo
  labels:
    environment: production
    app: nginx
spec:
  containers:
  - name: nginx
    image: nginx:1.14.2
    ports:
    - containerPort: 80</code></pre>

    <p class="card-text">Selectors are equality-based allows filtering by label keys and values while set-based allows filtering keys according to a set of values. For example,</p>

<pre><code class="bash">kubectl get pods -l environment=production,tier=frontend  # Equality based
kubectl get pods -l 'environment in (production),tier in (frontend)'  # Set based</code></pre>

    <p class="card-text">Service and Replication Controller only support equality-based selector</p>

<pre><code class="yaml">selector:
  component: redis</code></pre>

    <p class="card-text">Job, Deployment, ReplicaSet, DaemonSet also support set-based selector</p>

<pre><code class="yaml">selector:
matchLabels:
  component: redis
matchExpressions:
  - {key: tier, operator: In, values: [cache]}
  - {key: environment, operator: NotIn, values: [dev]}</code></pre>

    <p class="card-text">Annotations - allows attaching arbitrary non-identifying metadata to objects (While Labels are used to select objects, annotations are for recording metadata)</p>

<pre><code class="yaml">apiVersion: v1
kind: Pod
metadata:
  name: annotations-demo
  annotations:
    imageregistry: "https://hub.docker.com/"
spec:
  containers:
  - name: nginx
    image: nginx:1.14.2
    ports:
    - containerPort: 80</code></pre>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/labels">Labels and Selectors</a> | <a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations">Annotations</a>
  </div>
</div>

<div class="card mb-4" id="kubernetes-110">
  <div class="card-body">
    <h2 class="card-title">Node affinity</h2>
    <ul>
      <li>Works like <code>nodeSelector</code> but is more expressive</li>
      <li><code>requiredDuringSchedulingIgnoredDuringExecution</code> - pod must be assgined to nodes with matching label</li>
      <li><code>preferredDuringSchedulingIgnoredDuringExecution</code> - pod preferably assgined to nodes with matching label</li>
    </ul>

<pre><code class="yaml">spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: &lt;key_of_label&gt;
            operator: In
            values:  # This is OR relationship
            - &lt;value1_of_label&gt;
            - &lt;value2_of_label&gt;</code></pre>

<pre><code class="yaml">spec:
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 1
        preference:
          matchExpressions:
          - key: &lt;key_of_label&gt;
            operator: In
            values:
            - &lt;value_of_label&gt;</code></pre>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node">Assigning Pods to Nodess</a>
  </div>
</div>

<div class="card mb-4" id="kubernetes-111">
  <div class="card-body">
    <h2 class="card-title">Pod affinity</h2>
    <ul>
      <li>Constrain Pods against labels on other Pods</li>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node">Assigning Pods to Nodess</a>
  </div>
</div>

<div class="card mb-4" id="kubernetes-31">
  <div class="card-body">
    <h2 class="card-title">Ingress Controller</h2>
    <ul>
      <li>Application that configures load balancer</li>
      <li>Actually load balancer can be appplication running in cluster or external cloud load balancer</li>
      <li>Different load balancer requires different Ingress Controller implementation</li>
    </ul>

    <h2 class="card-title">Enable Ingress Controller</h2>

<pre><code class="bash">kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.3.0/deploy/static/provider/cloud/deploy.yaml
kubectl get pods -n=ingress-nginx</code></pre>

  </div>
  <div class="card-footer text-muted">

  </div>
</div>

<div class="card mb-4" id="kubernetes-32">
  <div class="card-body">
    <h2 class="card-title">Ingress</h2>
    <ul>
      <li>Sits in front of services</li>
      <li>Depending on the host and its path the ingress receives, sends the request to different services</li>
    </ul>

<pre><code class="yaml">apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: my-ingress
  annotations:
    kubernetes.io/ingress.class: "nginx"
    ingress.kubernetes.io/ssl-redirect: "false"
    nginx.ingress.kubernetes.io/ssl-redirect: "false"
spec:
  rules:
  - http:
      paths:
      - path: /
        pathType: ImplementationSpecific
        backend:
          service:
            name: my-service
            port:
              number: 80</code></pre>
  </div>
  <div class="card-footer text-muted">

  </div>
</div>

<div class="card mb-4" id="kubernetes-33">
  <div class="card-body">
    <h2 class="card-title">Service</h2>
    <ul>
      <li>Service is an abstraction for logical set of Pods</li>
      <li>Has static IP, DNS, and port</li>
      <li>The set of Pods targeted by Service is determined by selector</li>
      <li>Each Service gets an associated Endpoints object, which is a dynamic list of all healthy Pods that match Services label selector</li>
      <li>Service label can be used to do blue-green deployment of Pods by switching traffics to different Pods</li>
    </ul>

<pre><code class="yaml">apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  selector:
    app: my-app  # Pods must have all the labels in the Service for the Service to send traffics to Pods
  ports:
    - protocol: TCP
      port: 80  # Servie listens to this port for the internal request
      targetPort: 80  # Application listens to this port inside the pod. By default and for convenience, the "targetPort" is set to the same value as the "port" field</code></pre>

    <h3 class="card-title">Service without selectors</h3>
    <ul>
      <li>Ex. External database in production, but your own database in test environment</li>
      <li>Ex. Point Service to another Service in different namespace</li>
    </ul>

<pre><code class="yaml"># Service needs to be manually mapped to network address and port
apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  ports:
    - protocol: TCP
      port: 80
      targetPort: 9376</code></pre>

    <h3 class="card-title">Virtual IPs and service proxies</h3>
    <ul>
      <li>Every Node runs <code>kube-proxy</code>, which implementes virtual IP</li>
      <li>ConfigMap is used to configure <code>kube-proxy</code></li>
    </ul>

    <h3 class="card-title">iptables proxy mode</h3>
    <ul>
      <li><code>kube-proxy</code> watches for Kubernetes control plane for addition and removal of Service and Endpoint objects</li>
      <li>It installs iptable rules and redirect traffics to Service's backend sets (For Service) or backend Pod (For EndPoint)</li>
      <li>It chooses backend at random</li>
    </ul>

    <h3 class="card-title">Publishing Service</h3>
    <ul>
      <li><code>ClusterIP</code></li>
      <ul>
        <li>Default <code>ServiceTypes</code></li>
        <li>Expose Service on cluster-internal IP</li>
        <li>Service beomes only reachable from within the cluster</li>
      </ul>
      <li><code>NodePort</code></li>
      <ul>
        <li>Exposes Service on each Node's IP at a static port</li>
        <li><code>ClusterIP</code> is automatically created</li>
        <li>Traffic from outside the cluster can hit any node in the cluster</li>
      </ul>
      <li><code>LoadBalancer</code></li>
      <ul>
        <li>Exposes Service externally using cloud provider's load balancer</li>
        <li><code>ClusterIP</code> and <code>NodePort</code> are automatically created</li>
      </ul>
    </ul>

    <h4 class="card-title">NodePort</h4>

<pre><code class="yaml">apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  type: NodePort
  selector:
    app: my-app
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
      nodePort: 30007  # Servie listens to this port for the external request</code></pre>

    <h4 class="card-title">LoadBalancer</h4>

<pre><code class="yaml">apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  selector:
    app: my-app
  ports:
    - protocol: TCP
      port: 80
      targetPort: 9376
  clusterIP: 10.0.171.239
  type: LoadBalancer
status:
  loadBalancer:
    ingress:
    - ip: 192.0.2.127</code></pre>

    <h3 class="card-title">Connecting Applications with Services</h3>
    <ul>
      <li>Docker: containers can talk to other containers only if they are on the same machine</li>
      <ul>
        <li>Containers must be allowed ports on machine's own IP address</li>
      </ul>
      <li>Kubernetes: Pods can talk to other Pods regardless of Nodes</li>
      <ul>
        <li>Every Pod gets cluster-private IP address and all Pods in a cluster can see each other</li>
      </ul>
    </ul>

    <h3 class="card-title">Service discovery</h3>
    <ul>
      <li>Kubernetes runs an internal DNS service, which is implemented by <code>kube-dns</code> Service and <code>coredns</code> Deployment</li>
      <li>Any time a new Service object is observed, DNS record is created so that Service name can be resolved to its ClusterIP</li>
      <li>The hostname has the form <code>&lt;service_name&gt;.&lt;namespace&gt;</code> (Its FQDN will be <code>&lt;service_name&gt;.&lt;namespace&gt;.svc.cluster.local</code>)</li>
      <li><code>/etc/resolve.conf</code> inside each pod contains the search domain and IP address to send traffic to (Appending search domain to a short name, like Service name, converts the short name to FQDN)</li>
      <li><code>kube-proxy</code>, which is a system service running on every node, capture traffic destined for Service's ClusterIP and redirects it to the IP addresses of Pods that match the Services label selector</li>
    </ul>

    <h3 class="card-title">Service discovery debugging</h3>

<pre><code class="bash">kubectl get deploy -n kube-system -l k8s-app=kube-dns

kubectl get pods -n kube-system -l k8s-app=kube-dns

kubectl logs &lt;coredns_pod_name&gt; -n kube-system

# ClusterIP address for the kube-dns Service should match the IP address in the /etc/resolv.conf files of all containers running on the cluster
kubectl get svc kube-dns -n kube-system

# kube-dns Endpoints should have the IP addresses of the coredns Pods listening on port 53 TCP and UDP
kubectl get ep -n kube-system -l k8s-app=kube-dns

# Run DNS pod and run nslookup
kubectl run -it dnsutils image gcr.io/kubernetes-e2e-test-images/dnsutils:1.3</code></pre></li>

    <h3 class="card-title">Securing the Service</h3>

<pre><code class="bash"># Create a public private key pair
openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout /d/tmp/mycert.key -out /d/tmp/mycert.crt -subj "/CN=my-app/O=my-app"
# Convert the keys to base64 encoding
cat /d/tmp/mycert.crt | base64
cat /d/tmp/mycert.key | base64

make keys KEY=/tmp/mycert.key CERT=/tmp/mycert.crt
kubectl create secret tls mysecret --key /tmp/mycert.key --cert /tmp/mycert.crt
kubectl create configmap myconfigmap --from-file=default.conf</code></pre>

<pre><code class="yaml">apiVersion: "v1"
kind: Secret
metadata:
  name: my-secret
  namespace: my-namespace
type: kubernetes.io/tls
data:
  tls.crt: &lt;encrypted key output from above&gt;
  tls.key: &lt;encrypted key output from above&gt;</code></pre>

<pre><code class="yaml">apiVersion: v1
kind: Service
metadata:
  name: my-app
  labels:
    run: my-app
spec:
  type: NodePort
  ports:
  - port: 8080
    targetPort: 80
    protocol: TCP
    name: http
  - port: 443
    protocol: TCP
    name: https
  selector:
    run: my-app
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-app
spec:
  selector:
    matchLabels:
      run: my-app
  replicas: 1
  template:
    metadata:
      labels:
        run: my-app
    spec:
      volumes:
      - name: secret-volume
        secret:
          secretName: mysecret
      - name: configmap-volume
        configMap:
          name: myconfigmap
      containers:
      - name: my-container
        image: my-image:1.0
        ports:
        - containerPort: 443
        - containerPort: 80
        volumeMounts:
        - mountPath: /path/to/mount/secret
          name: secret-volume
        - mountPath: /path/to/mount/configmap
          name: configmap-volume</code></pre>

    <h3 class="card-title">Can reach the server from any node</h3>

<pre><code class="bash">kubectl get pods -o yaml | grep -i podip
    podIP: 10.244.3.5
node $ curl -k https://10.244.3.5</code></pre>

    <h3 class="card-title">Setup Pod such that</h3>

<pre><code class="yaml">apiVersion: apps/v1
kind: Deployment
metadata:
  name: curl-deployment
spec:
  selector:
    matchLabels:
      app: curl-pod
  replicas: 1
  template:
    metadata:
      labels:
        app: curl-pod
    spec:
      volumes:
      - name: secret-volume
        secret:
          secretName: my-secret
      containers:
      - name: curl-pod
        command:
        - sh
        - -c
        - while true; do sleep 1; done
        image: my-image
        volumeMounts:
        - mountPath: /path/to/mount/secret
          name: secret-volume</code></pre>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://kubernetes.io/docs/concepts/services-networking/service">Service</a> | <a href="https://kubernetes.io/docs/concepts/services-networking/connect-applications-service">Connecting Applications with Services</a> | Reference: <a href="https://www.educative.io/courses/the-kubernetes-course">Learn Kubernetes: A Deep Dive</a>
  </div>
</div>

<div class="card mb-4" id="kubernetes-34">
  <div class="card-body">
    <h2 class="card-title">Network Policy</h2>
    <p class="card-text"></p>
    <ul>
      <li>Pods are non-isolated by default and accept traffics from any source</li>
      <li>Pods become isolated by Network Policy. they reject any connections that are not allowed by any NetworkPolicy</li>
    </ul>

    <h3 class="card-title">Pod IP</h3>
    <ul>
      <li>When a cluster is created, an IP address range is given to the cluster</li>
      <li>Subset of that IP is given to the nodes</li>
      <li>Pod gets IP from the IPs reserved in that node</li>
    </ul>

    <h3 class="card-title">Ingress Vs. Egress</h3>
    <ul>
      <li>Ingress - imcoming traffic to Pods</li>
      <li>Egress - outgoing traffic from Pods</li>
    </ul>

<pre><code class="yaml">apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: test-network-policy
  namespace: default
spec:
  podSelector:  # Empty podSelector selects all pods in the namespace
    matchLabels:
      role: db  # Selects pods with the label "role=db"
  policyTypes:
    - Ingress
    - Egress
  ingress:
    - from:
        - ipBlock:
            cidr: 172.17.0.0/16
            except:
            - 172.17.1.0/24
        - namespaceSelector:
            matchLabels:
              project: myproject
        - podSelector:
            matchLabels:
              role: frontend
      ports:
      - protocol: TCP
        port: 6379
  egress:
    - to:
        - ipBlock:
            cidr: 10.0.0.0/24
      ports:
        - protocol: TCP
          port: 5978</code></pre>

    <h3 class="card-title">Default deny all ingress traffic</h3>

<pre><code class="yaml">---
apiVersion: networking.k8s.io/v1
  kind: NetworkPolicy
metadata:
name: default-deny-ingress
spec:
  podSelector: {}
  policyTypes:
  - Ingress</code></pre>

    <h3 class="card-title">Default allow all ingress traffic</h3>

<pre><code class="yaml">---
apiVersion: networking.k8s.io/v1
  kind: NetworkPolicy
metadata:
name: allow-all-ingress
spec:
  podSelector: {}
  ingress:
  - {}
  policyTypes:
  - Ingress</code></pre>

    <h3 class="card-title">Default deny all egress traffic</h3>

<pre><code class="yaml">---
apiVersion: networking.k8s.io/v1
  kind: NetworkPolicy
metadata:
name: default-deny-egress
spec:
  podSelector: {}
  policyTypes:
  - Egress</code></pre>

    <h3 class="card-title">Default allow all egress traffic</h3>

<pre><code class="yaml">---
apiVersion: networking.k8s.io/v1
  kind: NetworkPolicy
metadata:
name: allow-all-egress
spec:
  podSelector: {}
  egress:
  - {}
  policyTypes:
  - Egress</code></pre>

    <h3 class="card-title">Default allow all egress traffic</h3>

<pre><code class="yaml">---
apiVersion: networking.k8s.io/v1
  kind: NetworkPolicy
metadata:
name: default-deny-all
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress</code></pre>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://kubernetes.io/docs/concepts/services-networking/network-policies">Network Policies</a>
  </div>
</div>

<div class="card mb-4" id="kubernetes-35">
  <div class="card-body">
    <h2 class="card-title">CoreNDS</h2>

    <h3 class="card-title">How to use custom url for the service</h3>
    <ul>
      <li>The default url for Service is <code>&lt;service_name&gt;.&lt;namespace_name&gt;</code></li>
      <li>CoreDNS ConfigMap cannot directly be modified</li>
      <li>Instead, another ConfigMap <code>coredns-custom</code> should be created under <code>kube-system</code> namespaces</li>
      <pre><code class="yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: coredns-custom
  namespace: kube-system
data:
  test.override: |
    rewrite new_url old_url</code></pre>
      <li>Then the following commands can be issued to apply the change</li>
      <pre><code class="bash">kubectl apply -f &lt;configmap-coredns-custom.yaml&gt;
kubectl get configmaps -n kube-system coredns-custom -o yaml
kubectl delete pod -n kube-system -l k8s-app=kube-dns</code></pre>
    </ul>
  </div>
  <div class="card-footer text-muted">

  </div>
</div>

<div class="card mb-4" id="kubernetes-41">
  <div class="card-body">
    <h2 class="card-title">Volumes</h2>
    <ul>
      <li>Volumes mount at specific path within the image</li>
      <li>ConfigMap allows injecting configration data into Pod</li>
    </ul>

    <h3 class="card-title">Configmap volume</h3>
    <ul>
      <li><code>my-configmap</code> ConfigMap is mounted as a volume at path <code>/etc/config/log_level</code></li>
    </ul>

<pre><code class="yaml">apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  containers:
    - name: my-container
      image: my-image
      volumeMounts:
        - name: my-config-volume
          mountPath: /etc/config
  volumes:
    - name: my-config-volume
      configMap:
        name: my-configmap
        items:
          - key: log_level
            path: log_level</code></pre>

    <h3 class="card-title">emptyDir volume</h3>
    <ul>
      <li><code>emptyDir</code> is created when Pod is assigned to Node</li>
      <li>When Pod is removed from Node, data is deleted permanently</li>
    </ul>

<pre><code class="yaml">apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  containers:
  - name: my-container
    image: my-image
    volumeMounts:
    - name: my-cache-volume
      mountPath: /cache
  volumes:
  - name: my-cache-volume
    emptyDir: {}</code></pre>

    <h3 class="card-title">Dynamic Volume Provisioning</h3>
    <ul>
      <li>To enable dynamic provisioning, cluster admin must pre-create StorageClass object for users</li>
      <li>Users request dynamically provisioned storage by including a storage class in their <code>PersistentVolumeClaim</code>. When this claim is deleted, the volume gets destroyed</li>
      <li>Cluster admin can make Claims to use dynamic provisioning by default. This is done by marking a specific StorageClass as default by adding <code>storageclass.kubernetes.io/is-default-class</code> annotation to it</li>
    </ul>

<pre><code class="yaml"> # Create storage class "slow" that provisions persistent disks like standard disk
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: slow
provisioner: kubernetes.io/gce-pd
parameters:
  type: pd-standard</code></pre>

<pre><code class="yaml"> # Create storage class "fast" that provisions persistent disks like SSD
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: fast
provisioner: kubernetes.io/gce-pd
parameters:
  type: pd-ssd</code></pre>

<pre><code class="yaml">apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: claim1
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: fast
  resources:
    requests:
      storage: 30Gi</code></pre>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://kubernetes.io/docs/concepts/storage/volumes">Volumes</a> | <a href="https://kubernetes.io/docs/concepts/storage/dynamic-provisioning">Dynamic Volume Provisioning</a>
  </div>
</div>

<div class="card mb-4" id="kubernetes-42">
  <div class="card-body">
    <h2 class="card-title">Persistent Volumes</h2>
    <ul>
      <li>PersistentVolume (PV) is a piece of storage in a cluster. It is similar to Node</li>
      <li>PersistentVolumeClaim (PVC) a request for storage by a user. It is similar to Pod</li>
      <li>PVC comsume PV resources. While Pod can request CPU and memory, PVC can request specific size and access mode</li>
    </ul>

    <h3 class="card-title">Binding</h3>
    <ul>
      <li>If PV was dynamically provisioned for a PVC, those PV and PVC will bind together</li>
      <li>Otherwise, users will get at least what they asked for but volumes maybe at the excess</li>
    </ul>

    <h3 class="card-title">Storage Object in Use Protection</h3>
    <ul>
      <li>If user deletes PVC, deletion is postponed until PVC is not in use by any Pods</li>
      <li>If admin deletes PV, deletion is postponed until PV is not bound to PVC</li>
    </ul>

    <h3 class="card-title">Reserving PV</h3>

<pre><code class="yaml">apiVersion: v1
kind: PersistentVolume
metadata:
  name: foo-pv
spec:
  storageClassName: ""
  claimRef:
    name: foo-pvc
    namespace: foo
...</code></pre>

    <h3 class="card-title">PV</h3>
    <ul>
      <li><code>ReadWriteOnce</code>: PV can only be bound as R/W by a single PVC</li>
      <li><code>ReadWriteMany</code>: PV can be bound as R/W by multiple PVCs</li>
      <li><code>ReadOnlyMany</code>: PV can be bound as R/O by multiple PVCs</li>
      <li><code>Delete</code>: deletes PV and data stored on the associated external storage</li>
      <li><code>Retain</code>: keep PV and data stored on the associated external storage</li>
    </ul>

<pre><code class="yaml">apiVersion: v1
kind: PersistentVolume
metadata:
  name: my-pv
spec:
  capacity:
    storage: 5Gi
  volumeMode: Filesystem  # Filesystem - default, Block - raw block device
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Delete
  storageClassName: slow
  mountOptions:
    - hard
    - nfsvers=4.1
  nfs:
    path: /tmp
    server: 172.17.0.2</code></pre>

    <h3 class="card-title">PVC</h3>

<pre><code class="yaml">apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-pvc
spec:
  accessModes:
    - ReadWriteOnce
  volumeMode: Filesystem
  resources:
    requests:
      storage: 8Gi
  storageClassName: slow
  selector:
    matchLabels:  # Volumn must have a label with this value
      name: my-pv
    matchExpressions:  # A list of requirements
      - {key: environment, operator: In, values: [dev]}</code></pre>

    <h3 class="card-title">Claims as Volumns</h3>
    <ul>
      <li>Pods access storage by using Claim as volume</li>
      <li>Claim must exist in the same namespace as Pod</li>
      <li>The cluster finds Claim in Pods's namespace and uses it to get PV</li>
    </ul>

<pre><code class="yaml">apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  containers:
    - name: my-container
      image: my-image
      volumeMounts:
      - name: my-volumemount
        mountPath: /path/to/mount
        readOnly: True  # This overwrites access mode in PV
  volumes:
    - name: my-volumemount
      persistentVolumeClaim:
        claimName: my-pvc
        readOnly: True  # This overwrites access mode in PV</code></pre>

    <h3 class="card-title">Storage Class</h3>
    <ul>
      <li>There is no need to create PV manually with Storage Class</li>
      <li><code>provisioner</code> tells Kubernetes which plugin to use</li>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes">Persistent Volumes</a>
  </div>
</div>

<div class="card mb-4" id="kubernetes-45">
  <div class="card-body">
    <h2 class="card-title">Volume Snapshots</h2>
    <ul>
      <li><code>VolumeSnapshotContent</code> - snapshot taken from a volumn</li>
      <li><code>VolumeSnapshot</code> - request for a snapshot by a user</li>
      <li><code>VolumeSnapshot</code> is only available for CSI (Container Storage Interface) drivers</li>
    </ul>

    <h3 class="card-title">VS</h3>

<pre><code class="yaml">apiVersion: snapshot.storage.k8s.io/v1
kind: VolumeSnapshot
metadata:
  name: new-snapshot-test
spec:
  volumeSnapshotClassName: csi-hostpath-snapclass
  source:
    persistentVolumeClaimName: pvc-test # name of PVC data source for the snapshot</code></pre>

    <h3 class="card-title">VSC</h3>

<pre><code class="yaml">apiVersion: snapshot.storage.k8s.io/v1
kind: VolumeSnapshotContent
metadata:
  name: snapcontent-72d9a349-aacd-42d2-a240-d775650d2455
spec:
  deletionPolicy: Delete
  driver: hostpath.csi.k8s.io
  source:
    volumeHandle: ee0cfb94-f8d4-11e9-b2d8-0242ac110002  # Unique identifier creatd on the storage (Returned by CSI driver druing volume creation)
  volumeSnapshotClassName: csi-hostpath-snapclass
  volumeSnapshotRef:
    name: new-snapshot-test
    namespace: default
    uid: 72d9a349-aacd-42d2-a240-d775650d2455</code></pre>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://kubernetes.io/docs/concepts/storage/volume-snapshots">Volume Snapshots</a>
  </div>
</div>

<div class="card mb-4" id="kubernetes-51">
  <div class="card-body">
    <h2 class="card-title">ConfigMap</h2>
    <ul>
      <li>Map of key-value pairs</li>
      <li>Provides configuration data, which is separate from application code. Data stored in configMap cannot exceed 1MB</li>
    </ul>

    <h3 class="card-title">ConfigMap and Pod</h3>
    <ul>
      <li>ConfigMap and Pod must be in the same namespace. There are four ways to use ConfigMap</li>
      <ul>
        <li>Container commands (And args)</li>
        <li>Environment variables</li>
        <li>Add a file in read-only volume</li>
        <li>Code inside Pod that uses K8s API to read ConfigMap</li>
      </ul>
    </ul>

<pre><code class="yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: my-configmap
data:
  # property-like keys; each key maps to a simple value
  player_initial_lives: "3"
  ui_properties_file_name: "user-interface.properties"

  # file-like keys
  game.properties: |
    enemy.types=aliens,monsters
    player.maximum-lives=5
  user-interface.properties: |
    color.good=purple
    color.bad=yellow
    allow.textmode=true</code></pre>

<pre><code class="yaml">apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  containers:
    - name: my-container
      image: my-image
      command: ["sleep", "3600"]
      env:
        # Define the environment variable
        - name: PLAYER_INITIAL_LIVES  # Notice that the case is different here from the key name in the ConfigMap
          valueFrom:
            configMapKeyRef:
              name: my-configmap  # The ConfigMap this value comes from
              key: player_initial_lives  # The key to fetch
        - name: UI_PROPERTIES_FILE_NAME
          valueFrom:
            configMapKeyRef:
              name: my-configmap
              key: ui_properties_file_name
        - name: ENV_NAME
          value: env_value
      volumeMounts:
      - name: my-volumemount
        mountPath: "/config"
        readOnly: true
  volumes:
    # You set volumes at the Pod level, then mount them into containers inside that Pod
    - name: my-volumemount
      configMap:
        # Provide the name of the ConfigMap you want to mount
        name: my-configmap
        # An array of keys from the ConfigMap to create as files
        items:
        - key: "game.properties"
          path: "game.properties"
        - key: "user-interface.properties"
          path: "user-interface.properties"</code></pre>

    <h3 class="card-title">Using ConfigMap as file/volume</h3>
    <ul>
      <li>Create a ConfigMap</li>
      <li>Update Pod to add a volume under <code>.spec.volumes[]</code> whose name can be anything. Make this field <code>spec.volumes[].configMap.name</code> reference ConfigMap object</li>
      <li>Add <code>.spec.containers[].volumeMounts[]</code> to each container that needs configMap. Set <code>.spec.containers[].volumeMounts[].readOnly = true</code>. Specify <code>.spec.containers[].volumeMounts[].mountPath</code> to your ConfigMap location</li>
      <li>Look for ConfigMap from the image. Each key in ConfigMap <code>data</code> becomes filename under <code>mountPath</code></li>
    </ul>
<pre><code class="yaml">apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  containers:
  - name: my-container
    image: my-image
    volumeMounts:
    - name: my-volumemount
      mountPath: /path/to/mount
      readOnly: true
  volumes:
  - name: my-volumemount
    configMap:
      name: my-configmap</code></pre>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://kubernetes.io/docs/concepts/configuration/configmap">ConfigMap</a>
  </div>
</div>

<div class="card mb-4" id="kubernetes-52">
  <div class="card-body">
    <h2 class="card-title">Secret</h2>
    <ul>
      <li>Similar to ConfigMap but specifically for confidential data</li>
      <li>Never written to physical storage</li>
      <li>Stored encrypted in the master node</li>
      <li>Secret can be used in three ways</li>
      <ul>
        <li>File in a volumn mounted on containers</li>
        <li>Container envinronment variable</li>
        <li>By Kubelet when pulling images for Pod</li>
      </ul>
      <li>Encoding scheme is base64 by default</li>
      <ul>
        <li><code>data</code> field requires secret to be base64 encoded</li>
        <li><code>stringData</code> field allow raw value secret</li>
      </ul>
    </ul>

    <h3 class="card-title">Opaque Secret</h3>

<pre><code class="yaml">apiVersion: v1
kind: Secret
metadata:
  name: my-opaque-secret
type: Opaque
stringData:
  username: my-username
  password: my-password</code></pre>

    <h3 class="card-title">Service account token Secret</h3>

<pre><code class="yaml">apiVersion: v1
kind: Secret
metadata:
  name: my-sa-secret
  annotations:
    kubernetes.io/service-account.name: "my-sa-name"  # Existing service account name
type: kubernetes.io/service-account-token
data:
  # You can include additional key value pairs as you do with Opaque Secrets
  extra: YmFyCg==</code></pre>

    <h3 class="card-title">Docker config Secret</h3>
    <ul>
      <li>~/.dockercfg is legacy, ~/.docker/config.json is the new format</li>
    </ul>

<pre><code class="yaml">apiVersion: v1
kind: Secret
metadata:
  name: secret-dockercfg
type: kubernetes.io/dockercfg
data:
  .dockercfg: | # This would be ".dockerconfigjson" for ~/.docker/config.json
          "&lt;base64 encoded ~/.dockercfg file&gt;"  # Or &lt;base64 encoded ~/.docker/config.json&gt;</code></pre>

    <h3 class="card-title">Basic authentication Secret</h3>

<pre><code class="yaml">apiVersion: v1
kind: Secret
metadata:
  name: my-basic-auth-secret
type: kubernetes.io/basic-auth
stringData:
  username: my-username
  password: my-password</code></pre>

    <h3 class="card-title">SSH authentication Secret</h3>

<pre><code class="yaml">apiVersion: v1
kind: Secret
metadata:
  name: secret-ssh-auth
type: kubernetes.io/ssh-auth
data:
  # The data is abbreviated in this example
  ssh-privatekey: |
        MIIEpQIBAAKCAQEAulqb/Y ...</code></pre>

    <h3 class="card-title">TLS secrets</h3>

<pre><code class="yaml">apiVersion: v1
kind: Secret
metadata:
  name: secret-tls
type: kubernetes.io/tls
data:
  # The data is abbreviated in this example
  tls.crt: |
        MIIC2DCCAcCgAwIBAgIBATANBgkqh ...
  tls.key: |
        MIIEpgIBAAKCAQEA7yn3bRHQ5FHMQ ...</code></pre>

    <h3 class="card-title">Create Secret from a file</h3>

<pre><code class="bash">kubectl create secret generic my-secret --from-file=./&lt;my_secret_file&gt;</code></pre>

    <h3 class="card-title">Editing Secret</h3>

<pre><code class="bash">kubectl edit secrets mysecret</code></pre>

    <h3 class="card-title">Using Secret as File</h3>
    <ul>
      <li>Useful when, for example, mounting JKS files into a directory of a container</li>
    </ul>

<pre><code class="yaml">apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  containers:
  - name: my-pod
    image: my-image
    volumeMounts:  # Add this to each container that needs Secret
    - name: my-secret-name
      mountPath: "/etc/foo"  # Should be an unused directory where you want Secret to appear
      readOnly: true
  volumes:
  - name: my-secret-name
    secret:
      secretName: my-secret  # Name of Secret object
      defaultMode: 0400  # Default is 0644 if not specified. All files created by Secret volumn mount will have 0400
      items:
      - key: username
        path: my-group/my-username  # "username" Secret is stored in "/etc/foo/my-group/my-username" instead of "/etc/foo/username"
        mode: 0777 # Files in /etc/foo/my-group/my-username will have 0777.</code></pre>

    <h3 class="card-title">Using Secret as environment variables</h3>
    <ul>
      <li>Updating Secret will not update environment variables in the containers unless containers are restarted</li>
    </ul>

<pre><code class="yaml">apiVersion: v1
kind: Pod
metadata:
  name: secret-env-pod
spec:
  containers:
  - name: mycontainer
    image: redis
    env:
      - name: SECRET_USERNAME
        valueFrom:
          secretKeyRef:
            name: my-secret
            key: username
      - name: SECRET_PASSWORD
        valueFrom:
          secretKeyRef:
            name: my-secret
            key: password
  restartPolicy: Never</code></pre>

    <h3 class="card-title">Immutable Secret</h3>
    <ul>
      <li>Prevents accidental deletion/update of Secret</li>
    </ul>

<pre><code class="yaml">apiVersion: v1
kind: Secret
metadata:
  ...
data:
  ...
immutable: true</code></pre>

    <h3 class="card-title">Risk</h3>
    <ul>
      <li>Secret data is stored etcd of API server. Admin should enable encryption-at-rest for cluster data and limit access to etcd</li>
      <li>Secret written as base64 in manifest files must not be shared or checked-in (Base64 encoding is not an encryption method and is the same as plain text)</li>
      <li>Users who can create Pod using the Secret can also see the Secret</li>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://kubernetes.io/docs/concepts/configuration/secret">Secret</a>
  </div>
</div>

<div class="card mb-4" id="kubernetes-53">
  <div class="card-body">
    <h2 class="card-title">RBAC</h2>
    <ul>
      <li>To enable RBAC, start the API server with such that:</li>
<pre><code class="bash">kube-apiserver --authorization-mode=Example,RBAC --other-options --more-options</code></pre>
      <li>RBAC API declares four objects:</li>
      <ul>
        <li>Role</li>
        <li>ClusterRole</li>
        <li>RoleBinding</li>
        <li>CLusterRoleBinding</li>
      </ul>
    </ul>

    <h3 class="card-title">Role</h3>
    <ul>
      <li>Role sets permission with a particular namespace</li>
    </ul>
<pre><code class="yaml">apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: pod-reader
  namespace: default
rules:
- apiGroups: [""]  # "" indicates the core API group
  resources: ["pods"]
  verbs: ["get", "watch", "list"]</code></pre>

    <h3 class="card-title">ClusterRole</h3>
    <ul>
      <li>ClusterRole is a cluster-wide, non-namespaced resource</li>
    </ul>
<pre><code class="yaml">apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: secret-reader  # "namespace" omitted since ClusterRoles are not namespaced
rules:
  - apiGroups: [""]
    resources: ["secrets"]  # At the HTTP level, the name of the resource for accessing Secret objects is "secrets"
    verbs: ["get", "watch", "list"]</code></pre>

    <h3 class="card-title">RoleBidning</h3>
    <ul>
      <li>RoleBinding grants permissions defined in a Role to users</li>
    </ul>
<pre><code class="yaml">apiVersion: rbac.authorization.k8s.io/v1
# This role binding allows "dave" to read secrets in the "development" namespace
# You need to already have a ClusterRole named "secret-reader"
kind: RoleBinding
metadata:
  name: read-secrets
  # The namespace of the RoleBinding determines where the permissions are granted
  # This only grants permissions within the "development" namespace
  namespace: development
subjects:
  - kind: User
    name: dave  # Name is case sensitive
    apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: secret-reader
  apiGroup: rbac.authorization.k8s.io</code></pre>

    <h3 class="card-title">ClusterRoleBinding</h3>
    <ul>
      <li>ClusterRoleBinding grants permission across the whole cluster</li>
    </ul>
<pre><code class="yaml">apiVersion: rbac.authorization.k8s.io/v1
# This cluster role binding allows anyone in the "manager" group to read secrets in any namespace
kind: ClusterRoleBinding
metadata:
  name: read-secrets-global
subjects:
  - kind: Group
    name: manager  # Name is case sensitive
    apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: secret-reader
  apiGroup: rbac.authorization.k8s.io</code></pre>

    <h3 class="card-title">Aggregated ClusterRole</h3>
    <ul>
      <li>Aggregate serveral ClusterRoles into one ClusterRole</li>
    </ul>
<pre><code class="yaml">apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: aggregate-cron-tabs-edit
  labels:
    # Add these permissions to the "admin" and "edit" default roles
    rbac.authorization.k8s.io/aggregate-to-admin: "true"
    rbac.authorization.k8s.io/aggregate-to-edit: "true"
rules:
  - apiGroups: ["stable.example.com"]
    resources: ["crontabs"]
    verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: aggregate-cron-tabs-view
  labels:
    # Add these permissions to the "view" default role
    rbac.authorization.k8s.io/aggregate-to-view: "true"
rules:
  - apiGroups: ["stable.example.com"]
    resources: ["crontabs"]
    verbs: ["get", "list", "watch"]</code></pre>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://kubernetes.io/docs/reference/access-authn-authz/rbac">Using RBAC Authorization</a>
  </div>
</div>

<!-- <div class="card mb-4" id="kubernetes-?">
  <div class="card-body">
    <h2 class="card-title">Kubernetes objects and API</h2>

    <ul>
      <li>Persistent entities that represent the cluster desired state.</li>
      <li>Each object has <code>spec</code> and <code>status</code></li>
    </ul>

    <h3 class="card-title">Standard API terminology</h3>
    <p class="card-text">Most K8s resource types are objects, which have unique name to allow idempotent creation (virtual types may not have unique name, for example "permission check")</p>
    <ul>
      <li>Resource type - name used in the URLs (pods, namespaces, services)</li>
      <li>Kind - JSON representation of resource types</li>
      <li>Collection - list of instances of a resource type</li>
      <li>Resource - single instance of the resource type</li>
    </ul>
    <p class="card-text">All resource types are either <strong>cluster-scoped</strong> or <strong>namespace-scoped</strong>. namespace-scoped resource types will be deleted when the namespace is deleted</p>
    <p class="card-text">cluster-scoped</p>
    <ul>
      <li>GET /apis/GROUP/VERSION/RESOURCETYPE</li>
      <li>GET /apis/GROUP/VERSION/RESOURCETYPE/NAME</li>
    </ul>
    <p class="card-text">namespace-scoped</p>
    <ul>
      <li>GET /apis/GROUP/VERSION/namespaces/NAMESPACE/RESOURCETYPE</li>
      <li>GET /apis/GROUP/VERSION/namespaces/NAMESPACE/RESOURCETYPE/NAME</li>
      <li>GET /apis/GROUP/VERSION/namespaces/NAMESPACE/RESOURCETYPE/NAME</li>
    </ul>
    <p class="card-text">A namespace is a cluster-scoped resource type. Retrive all namespaces with "GET /api/v1/namespaces" and particular namespace with "GET /api/v1/namespaces/NAME"</p>
    <p class="card-text">K8s uses "list" to return a collection of resource and "get" to return a single resource</p>
    <p class="card-text">Some resources have sub-resource(s)</p>
    <ul>
      <li>GET /apis/GROUP/VERSION/RESOURCETYPE/NAME/SUBRESOURCE</li>
      <li>GET /apis/GROUP/VERSION/namespaces/NAMESPACE/RESOURCETYPE/NAME/SUBRESOURCE</li>
    </ul>

    <h3 class="card-title">Efficient detection of changes</h3>
    <p class="card-text"><strong>watch</strong> - detects incremental changes in cluster state. Use "resourceVersion" to store the state of resources</p>
    <ul>
      <li>GET /api/v1/namespaces/test/pods - list all pods in given namespace</li>
      <li>GET /api/v1/namespaces/test/pods?watch=1&resourceVersion=10245 - starting resource version 10245, receive notifications for create/delete/update as JSON</li>
    </ul>
    <p class="card-text">K8s server can only store history for a limted time. Clusters using etcd3 preserve changes for the last 5 mins by default. Clients are expected to handle http status code "410 Gone"</p>
    <p class="card-text"><strong>bookmarks</strong> - marks that all changes up to given "resourceVersion" has already been sent. (in an attempt to mitigate the short history window problem)</p>
    <ul>
      <li>GET /api/v1/namespaces/test/pods?watch=1&resourceVersion=10245&allowWatchBookmarks=true</li>
    </ul>

    <h3 class="card-title">Retrieving large results sets in chunks</h3>
    <p class="card-text">Break single large collection requests into small chunks by parameters "limit" and "continue"</p>
    <ul>
      <li>GET /api/v1/pods?limit=500 - retrive all pods in cluster, up to 500</li>
      <li>GET /api/v1/pods?limit=500&continue=ENCODED_CONTINUE_TOKEN - continue from the previous call to get 501-1000 pods</li>
      <li>GET /api/v1/pods?limit=500&continue=ENCODED_CONTINUE_TOKEN_2 - continue from the previous call to get last set of pods</li>
    </ul>

    <h3 class="card-title">Receiving resources as Tables</h3>
    <ul>
      <li>GET /api/v1/pods<br>
          Accept: application/json;as=Table;g=meta.k8s.io;v=v1beta1<br>
          - retrive all pods in cluster in table format</li>
    </ul>
    <p class="card-text">Because there are resource types that don't support Table response, client should handle both Table/non-Table case by using content-type</p>
    <ul>
      <li>Accept: application/json;as=Table;g=meta.k8s.io;v=v1beta1, application/json</li>
    </ul>

    <h3 class="card-title">Receiving resources as Protobuf</h3>
    <p class="card-text">This is for better performance at scale</p>
    <ul>
      <li>GET /api/v1/pods
          Accept: application/vnd.kubernetes.protobuf<br>
          - retrive all pods in cluster in Protobuf format</li>
      <li>POST /api/v1/namespaces/test/pods
          Content-Type: application/vnd.kubernetes.protobuf
          Accept: application/json
          - create a pod with Protobuf encoded data, but receive response in JSON</li>
    </ul>
    <p class="card-text">Similar to Table response, multiple content-types are needed in the "Accept" header to support resource types that don't have Protobuf support</p>
    <ul>
      <li>Accept: application/vnd.kubernetes.protobuf, application/json</li>
    </ul>

    <h3 class="card-title">Resource deletion</h3>
    <p class="card-text">Takes place in two phases 1. finalization 2. removal. Finalizers are removed in any order. Once the last finalizer is removed, the resource is removed from etcd.</p>

    <h3 class="card-title">Dry-run</h3>
    <p class="card-text">dry-run executes the request up until persisting objects in storage. The reponse body should be as close as possible to the actual run. Authorization of dry and non-dry runs are identical</p>
    <ul>
      <li>POST /api/v1/namespaces/test/pods?dryRun=All<br>
          Content-Type: application/json<br>
          Accept: application/json<br>
          - ALL: every stage runs normal except the final stage of persisting objects in storage</li>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/kubernetes-objects/">Understanding Kubernetes Objects</a> | <a href="https://kubernetes.io/docs/reference/using-api/api-concepts/">Kubernetes API Concepts</a>
  </div>
</div> -->

<!-- <div class="card mb-4" id="kubernetes-61">
  <div class="card-body">
    <h2 class="card-title">Container logging</h2>
    <h3 class="card-title">Pod with two sidecar containers</h3>
<pre><code class="yaml">apiVersion: v1
kind: Pod
metadata:
  name: counter
spec:
  containers:
  - name: count
    image: busybox
    args:
    - /bin/sh
    - -c
    - >
      i=0;
      while true;
      do
        echo "$i: $(date)" >> /var/log/1.log;
        echo "$(date) INFO $i" >> /var/log/2.log;
        i=$((i+1));
        sleep 1;
      done
    volumeMounts:
    - name: varlog
      mountPath: /var/log
  - name: count-log-1
    image: busybox
    args: [/bin/sh, -c, 'tail -n+1 -f /var/log/1.log']
    volumeMounts:
    - name: varlog
      mountPath: /var/log
  - name: count-log-2
    image: busybox
    args: [/bin/sh, -c, 'tail -n+1 -f /var/log/2.log']
    volumeMounts:
    - name: varlog
      mountPath: /var/log
  volumes:
  - name: varlog
    emptyDir: {}</code></pre>
    <p class="card-text">Access two separate log streams.</h3>
<pre><code class="bash">kubectl logs counter count-log-1
kubectl logs counter count-log-2</code></pre>

    <h3 class="card-title">Sidecar container with logging agent</h3>
<pre><code class="yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: fluentd-config
data:
  fluentd.conf: |
    &lt;source&gt;
      type tail
      format none
      path /var/log/1.log
      pos_file /var/log/1.log.pos
      tag count.format1
    &lt;/source&gt;

    &lt;source&gt;
      type tail
      format none
      path /var/log/2.log
      pos_file /var/log/2.log.pos
      tag count.format2
    &lt;/source&gt;

    &lt;match **&gt;
      type google_cloud
    &lt;/match&gt;</code></pre>

<pre><code class="yaml">apiVersion: v1
kind: Pod
metadata:
  name: counter
spec:
  containers:
  - name: count
    image: busybox
    args:
    - /bin/sh
    - -c
    - >
      i=0;
      while true;
      do
        echo "$i: $(date)" >> /var/log/1.log;
        echo "$(date) INFO $i" >> /var/log/2.log;
        i=$((i+1));
        sleep 1;
      done
    volumeMounts:
    - name: varlog
      mountPath: /var/log
  - name: count-agent
    image: k8s.gcr.io/fluentd-gcp:1.30
    env:
    - name: FLUENTD_ARGS
      value: -c /etc/fluentd-config/fluentd.conf
    volumeMounts:
    - name: varlog
      mountPath: /var/log
    - name: config-volume
      mountPath: /etc/fluentd-config
  volumes:
  - name: varlog
    emptyDir: {}
  - name: config-volume
    configMap:
      name: fluentd-config</code></pre>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://kubernetes.io/docs/concepts/cluster-administration/logging/">Logging Architecture</a>
  </div>
</div> -->

<!-- <div class="card mb-4" id="kubernetes-62">
  <div class="card-body">
    <h2 class="card-title">Monitoring</h2>
    <h3 class="card-title">Enable Node Problem Detector</h3>

<pre><code class="yaml">apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: node-problem-detector-v0.1
  namespace: kube-system
  labels:
    k8s-app: node-problem-detector
    version: v0.1
    kubernetes.io/cluster-service: "true"
spec:
  selector:
    matchLabels:
      k8s-app: node-problem-detector
      version: v0.1
      kubernetes.io/cluster-service: "true"
  template:
    metadata:
      labels:
        k8s-app: node-problem-detector
        version: v0.1
        kubernetes.io/cluster-service: "true"
    spec:
      hostNetwork: true
      containers:
      - name: node-problem-detector
        image: k8s.gcr.io/node-problem-detector:v0.1
        securityContext:
          privileged: true
        resources:
          limits:
            cpu: "200m"
            memory: "100Mi"
          requests:
            cpu: "20m"
            memory: "20Mi"
        volumeMounts:
        - name: log
          mountPath: /log
          readOnly: true
        - name: config # Overwrite the config/ directory with ConfigMap volume
          mountPath: /config
          readOnly: true
      volumes:
      - name: log
        hostPath:
          path: /var/log/
      - name: config # Define ConfigMap volume
        configMap:
          name: node-problem-detector-config</code></pre>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://kubernetes.io/docs/tasks/debug-application-cluster/monitor-node-health/">Monitor Node Health</a>
  </div>
</div> -->

<!-- <div class="card mb-4" id="kubernetes-63">
  <div class="card-body">
    <h2 class="card-title">Application inspection and debugging</h2>

    <h3 class="card-title">Debug using ephemeral container.</h3>
<pre><code class="bash">kubectl run &lt;POD-NAME&gt; --image=&lt;IMAGE-NAME-TO-PULL-FOR-THIS-POD&gt; --restart=Never
# Add debug container.
kubectl debug -it &lt;POD-NAME&gt; --image=&lt;IMAGE-NAME&gt; --target=&lt;POD-NAME&gt;</code></pre>

    <h3 class="card-title">Debug using copy of Pod.</h3>
<pre><code class="bash">kubectl run &lt;POD-NAME&gt; --image=&lt;IMAGE-NAME&gt; --restart=Never -- sleep 1d
kubectl debug &lt;POD-NAME&gt; -it --image=&lt;NEW-CONTAINER-NAME-FOR-DEBUGGING&gt; --share-processes --copy-to=&lt;POD-NAME&gt;-debug</code></pre>

    <h3 class="card-title">Copying Pod while changing its command.</h3>
<pre><code class="bash">kubectl run --image=&lt;IMAGE-NAME&gt; &lt;POD-NAME&gt; -- false
kubectl debug &lt;POD-NAME&gt; -it --copy-to=&lt;POD-NAME&gt; -debug --container=&lt;POD-NAME&gt;-- sh</code></pre>

    <h3 class="card-title">Copying Pod while changing container image.</h3>
<pre><code class="bash">kubectl run &lt;POD-NAME&gt; --image=&lt;IMAGE-NAME&gt; --restart=Never -- sleep 1d
kubectl debug &lt;POD-NAME&gt; --copy-to=&lt;POD-NAME&gt;-debug --set-image=*=&lt;IMAGE-NAME&gt;</code></pre>

    <h3 class="card-title">Debug via shell on Node.</h3>
<pre><code class="bash">kubectl debug node/mynode -it --image=&lt;IMAGE-NAME&gt;</code></pre>

    <h3 class="card-title">Debug Deployment</h3>
<pre><code class="bash"># Create Deployment.
kubectl create deployment &lt;DEPLOYMENT-NAME&gt;
# Scale Deployment to 3 replicas.
kubectl scale deployment &lt;DEPLOYMENT-NAME&gt; --replicas=3
# Confirm Pods are running.
kubectl get pods -l &lt;DEPLOYMENT-NAME&gt;
# Get list of Pod IP addresses.
kubectl get pods -l &lt;DEPLOYMENT-NAME&gt; -o go-template='{{range .items}}{{.status.podIP}}{{"\n"}}{{end}}'</code></pre>

    <h3 class="card-title">Debug Service</h3>
<pre><code class="bash"># From Pod within the same namespace.
nslookup &lt;SERVICE-NAME&gt;
nslookup &lt;SERVICE-NAME&gt;.default
nslookup &lt;SERVICE-NAME&gt;.default.svc.cluster.local
# Within the Pod, check.
cat /etc/resolv.conf
nslookup kubernetes.default
# Within the Node.
nslookup &lt;SERVICE-NAME&gt;.default.svc.cluster.local &lt;CLUSTER-DNS-SERVICE-IP&gt;
# Check if Service is defined correctly.
kubectl get service &lt;SERVICE-NAME&gt; -o json
# Check if Service has endpoint.
kubectl get pods -l app=&lt;SERVICE-NAME&gt;
# Check if kube-proxy is running.
ps auxw | grep kube-proxy</code></pre>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-application-introspection/">Application Introspection and Debugging | <a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-stateful-set/">Debug a StatefulSet</a> | <a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-init-containers/">Debug Init Containers</a> | <a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-pod-replication-controller/">Debug Pods and ReplicationControllers</a> | <a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-running-pod/">Debug Running Pods</a> | <a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-service/">Debug Services</a> | <a href="https://kubernetes.io/docs/tasks/debug-application-cluster/determine-reason-pod-failure/">Determine the Reason for Pod Failure</a>
  </div>
</div> -->

<!-- <div class="card mb-4" id="kubernetes-71">
  <div class="card-body">
    <h2 class="card-title">Create cluster with kubeadm</h2>
    <p class="card-text">Control plan is the node where <code>etcd</code> and <code>API server</code> run. Initialize the control plane node,</p>
<pre><code class="bash">kubeadm init &lt;args&gt;</code></pre>
<pre><code class="bash"># To make kubectl work for your non-root user
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
# If you are root user
export KUBECONFIG=/etc/kubernetes/admin.conf</code></pre>

    <h3 class="card-title">Install a Pod network add-on</h3>
<pre><code class="bash">kubectl apply -f &lt;add-on.yaml&gt;
# Confirm it is working by checking if CoreDNS is running.
kubectl get pods --all-namespaces</code></pre>

    <h3 class="card-title">Join a Node</h3>
<pre><code class="bash"># Get token.
kubeadm token list

# Get --discovery-token-ca-cert-hash.
openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2>/dev/null | \
openssl dgst -sha256 -hex | sed 's/^.* //'

kubeadm join --token &lt;token&gt; &lt;control-plane-host&gt;:&lt;control-plane-port&gt; --discovery-token-ca-cert-hash sha256:&lt;hash&gt;</code></pre>

    <h3 class="card-title">Remove a Node</h3>
<pre><code class="bash">kubectl drain &lt;node name&gt; --delete-emptydir-data --force --ignore-daemonsets
# Reset the state installed by kubeadm.
kubeadm reset
# Reset iptables.
iptables -F && iptables -t nat -F && iptables -t mangle -F && iptables -X
# Reset IPVS tables.
ipvsadm -C
kubectl delete node &lt;node name&gt;</code></pre>
    </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/">Creating a cluster with kubeadm</a>
  </div>
</div> -->

<!-- <div class="card mb-4" id="kubernetes-72">
  <div class="card-body">
    <h2 class="card-title">Create HA cluster with kubeadm</h2>

    <h3 class="card-title">Create a load balancer for kube-apiserver</h3>
    <p class="card-text">Place control plane nodes behind a TCP forwarding load balancer. Address of load balancer must match the address of kubeadm's <code>ControlPlaneEndpoint</code>. Then add control planes to the load balancer and test.</p>
<pre><code class="bash"># Connection refused error is expected since apiserver is not running yet. However, timeout means a real problem.
nc -v LOAD_BALANCER_IP PORT</code></pre>

    <h3 class="card-title">Option #1. Stacked control plane and etcd nodes</h3>
<pre><code class="bash"># 1. Initialize the control plane.
sudo kubeadm init --control-plane-endpoint "LOAD_BALANCER_DNS:LOAD_BALANCER_PORT" --upload-certs

# 2. Apply a CIN plugin. (For example, Weave Net)
kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')"

# 3. Verify control plane.
kubectl get pod -n kube-system -w

# 4. Join Nodes (Use outputs from step #1)</code></pre>

    <h3 class="card-title">Option #2. External etcd nodes</h3>
<pre><code class="bash"># 1. Setup etcd cluster.
# 1.1. Do this on every host where etcd should be running.
cat << EOF > /etc/systemd/system/kubelet.service.d/20-etcd-service-manager.conf
[Service]
ExecStart=
#  Replace "systemd" with the cgroup driver of your container runtime. The default value in the kubelet is "cgroupfs".
ExecStart=/usr/bin/kubelet --address=127.0.0.1 --pod-manifest-path=/etc/kubernetes/manifests --cgroup-driver=systemd
Restart=always
EOF

systemctl daemon-reload
systemctl restart kubelet

# 1.2. Ensure kubectl is running.
systemctl status kubelet

# 1.3. Create configuration file for kubeadm.
# Update HOST0, HOST1, and HOST2 with the IPs or resolvable names of your hosts.
export HOST0=10.0.0.6
export HOST1=10.0.0.7
export HOST2=10.0.0.8

# Create temp directories to store files that will end up on other hosts.
mkdir -p /tmp/${HOST0}/ /tmp/${HOST1}/ /tmp/${HOST2}/

ETCDHOSTS=(${HOST0} ${HOST1} ${HOST2})
NAMES=("infra0" "infra1" "infra2")

for i in "${!ETCDHOSTS[@]}"; do
HOST=${ETCDHOSTS[$i]}
NAME=${NAMES[$i]}
cat << EOF > /tmp/${HOST}/kubeadmcfg.yaml
apiVersion: "kubeadm.k8s.io/v1beta3"
kind: ClusterConfiguration
etcd:
  local:
      serverCertSANs:
      - "${HOST}"
      peerCertSANs:
      - "${HOST}"
      extraArgs:
          initial-cluster: ${NAMES[0]}=https://${ETCDHOSTS[0]}:2380,${NAMES[1]}=https://${ETCDHOSTS[1]}:2380,${NAMES[2]}=https://${ETCDHOSTS[2]}:2380
          initial-cluster-state: new
          name: ${NAME}
          listen-peer-urls: https://${HOST}:2380
          listen-client-urls: https://${HOST}:2379
          advertise-client-urls: https://${HOST}:2379
          initial-advertise-peer-urls: https://${HOST}:2380
EOF
done

# 1.4. Generate the certificate authority. (This will create two files /etc/kubernetes/pki/etcd/ca.crt and /etc/kubernetes/pki/etcd/ca.key)
kubeadm init phase certs etcd-ca

# 1.5. Create certificates for each member.

kubeadm init phase certs etcd-server --config=/tmp/${HOST2}/kubeadmcfg.yaml
kubeadm init phase certs etcd-peer --config=/tmp/${HOST2}/kubeadmcfg.yaml
kubeadm init phase certs etcd-healthcheck-client --config=/tmp/${HOST2}/kubeadmcfg.yaml
kubeadm init phase certs apiserver-etcd-client --config=/tmp/${HOST2}/kubeadmcfg.yaml
cp -R /etc/kubernetes/pki /tmp/${HOST2}/
# cleanup non-reusable certificates
find /etc/kubernetes/pki -not -name ca.crt -not -name ca.key -type f -delete

kubeadm init phase certs etcd-server --config=/tmp/${HOST1}/kubeadmcfg.yaml
kubeadm init phase certs etcd-peer --config=/tmp/${HOST1}/kubeadmcfg.yaml
kubeadm init phase certs etcd-healthcheck-client --config=/tmp/${HOST1}/kubeadmcfg.yaml
kubeadm init phase certs apiserver-etcd-client --config=/tmp/${HOST1}/kubeadmcfg.yaml
cp -R /etc/kubernetes/pki /tmp/${HOST1}/
find /etc/kubernetes/pki -not -name ca.crt -not -name ca.key -type f -delete

kubeadm init phase certs etcd-server --config=/tmp/${HOST0}/kubeadmcfg.yaml
kubeadm init phase certs etcd-peer --config=/tmp/${HOST0}/kubeadmcfg.yaml
kubeadm init phase certs etcd-healthcheck-client --config=/tmp/${HOST0}/kubeadmcfg.yaml
kubeadm init phase certs apiserver-etcd-client --config=/tmp/${HOST0}/kubeadmcfg.yaml
# No need to move the certs because they are for HOST0

# clean up certs that should not be copied off this host
find /tmp/${HOST2} -name ca.key -type f -delete
find /tmp/${HOST1} -name ca.key -type f -delete

# 1.6. Copy certificates and kubeadm configs.
USER=ubuntu
HOST=${HOST1}
scp -r /tmp/${HOST}/* ${USER}@${HOST}:
ssh ${USER}@${HOST}
USER@HOST $ sudo -Es
root@HOST $ chown -R root:root pki
root@HOST $ mv pki /etc/kubernetes/

# 1.7. Create the static pod manifests.
root@HOST0 $ kubeadm init phase etcd local --config=/tmp/${HOST0}/kubeadmcfg.yaml
root@HOST1 $ kubeadm init phase etcd local --config=/tmp/${HOST1}/kubeadmcfg.yaml
root@HOST2 $ kubeadm init phase etcd local --config=/tmp/${HOST2}/kubeadmcfg.yaml

# 2. Create a file called kubeadm-config.yaml.
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
kubernetesVersion: stable
controlPlaneEndpoint: "LOAD_BALANCER_DNS:LOAD_BALANCER_PORT"
etcd:
  external:
      endpoints:
      - https://ETCD_0_IP:2379
      - https://ETCD_1_IP:2379
      - https://ETCD_2_IP:2379
      caFile: /etc/kubernetes/pki/etcd/ca.crt
      certFile: /etc/kubernetes/pki/apiserver-etcd-client.crt
      keyFile: /etc/kubernetes/pki/apiserver-etcd-client.key

# Rest steps are similar to Option #1</code></pre>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/">Creating Highly Available clusters with kubeadm</a> | <a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/setup-ha-etcd-with-kubeadm/">Set up a High Availability etcd cluster with kubeadm</a>
  </div>
</div> -->

<!-- <div class="card mb-4" id="kubernetes-73">
  <div class="card-body">
    <h2 class="card-title">Upgrade kubeadm clusters</h2>

    <h3 class="card-title">Upgrade kubeadm</h3>
<pre><code class="bash"># Ubuntu
# replace x in 1.22.x-00 with the latest patch version
apt-get update && \
apt-get install -y --allow-change-held-packages kubeadm=1.22.x-00

# RHEL
# replace x in 1.22.x-0 with the latest patch version
yum install -y kubeadm-1.22.x-0 --disableexcludes=kubernetes

# Verify.
kubeadm version
kubeadm upgrade plan</code></pre>

    <h3 class="card-title">Drain the Node</h3>
<pre><code class="bash"># replace &lt;node-to-drain&gt; with the name of your node you are draining
kubectl drain &lt;node-to-drain&gt; --ignore-daemonsets</code></pre>

    <h3 class="card-title">Upgrade kubelet and kubectl</h3>
<pre><code class="bash"># Ubuntu
# replace x in 1.22.x-00 with the latest patch version
apt-get update && \
apt-get install -y --allow-change-held-packages kubelet=1.22.x-00 kubectl=1.22.x-00

# RHEL
# replace x in 1.22.x-0 with the latest patch version
yum install -y kubelet-1.22.x-0 kubectl-1.22.x-0 --disableexcludes=kubernetes

# Restart the kubelet.
sudo systemctl daemon-reload
sudo systemctl restart kubelet</code></pre>

    <h3 class="card-title">Uncardon the Node</h3>
<pre><code class="bash"># replace &lt;node-to-drain&gt; with the name of your node you are draining
kubectl uncordon &lt;node-to-drain&gt; --ignore-daemonsets</code></pre>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/">Upgrading kubeadm clusters</a>
  </div>
</div> -->

<!-- <div class="card mb-4" id="kubernetes-74">
  <div class="card-body">
    <h2 class="card-title">Operating etcd clusters</h2>
    <p class="card-text">etcd is storage for all cluster data. A five member etcd cluster is recommended for production. Access to etcd is equivalent to root permission to the cluster and ideally only API server should have it. </p>
<pre><code class="bash"># To start API server with five member etcd cluster.
etcd --listen-client-urls=http://$IP1:2379,http://$IP2:2379,http://$IP3:2379,http://$IP4:2379,http://$IP5:2379 --advertise-client-urls=http://$IP1:2379,http://$IP2:2379,http://$IP3:2379,http://$IP4:2379,http://$IP5:2379</code></pre>

    <h3 class="card-title">Replace a failed etcd member</h3>
<pre><code class="bash"># Assume three members: member1=http://10.0.0.1, member2=http://10.0.0.2, and member3=http://10.0.0.3. When member1 fails, replace it with member4=http://10.0.0.4.

# Get the member ID of the failed member1.
etcdctl --endpoints=http://10.0.0.2,http://10.0.0.3 member list
# This outputs something like.
8211f1d0f64f3269, started, member1, http://10.0.0.1:2380, http://10.0.0.1:2379
91bc3c398fb3c146, started, member2, http://10.0.0.2:2380, http://10.0.0.2:2379
fd422379fda50e48, started, member3, http://10.0.0.3:2380, http://10.0.0.3:2379

# Remove the failed member.
etcdctl member remove 8211f1d0f64f3269

# Add a new member.
etcdctl member add member4 --peer-urls=http://10.0.0.4:2380

# Start the newly added member
export ETCD_NAME="member4"
export ETCD_INITIAL_CLUSTER="member2=http://10.0.0.2:2380,member3=http://10.0.0.3:2380,member4=http://10.0.0.4:2380"
export ETCD_INITIAL_CLUSTER_STATE=existing
etcd [flags]

# Update t--etcd-servers flag for the Kubernetes API servers.
# Restart the Kubernetes API servers.</code></pre>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/">Operating etcd clusters for Kubernetes</a>
  </div>
</div> -->

<!-- Kubernetes END -->

</div> <!-- /.col-md-12 -->
</div> <!-- /.row -->
</div> <!-- /.container -->

<include src="/footer.html"></include>

</body>

</html>

