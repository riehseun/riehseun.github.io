<!DOCTYPE html>

<html lang="en">

<head>

<!-- Metadata -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="">
<meta name="author" content="Seungmoon Rieh">
<meta name="keywords" content="">

<!-- Title and image -->
<title>Seungmoon Rieh</title>
<link href="/img/seungmoonrieh.jpg" rel="icon">

<!-- CSS -->
<link href="/css/bootstrap.min.css" rel="stylesheet">
<link href="/css/monokai-sublime.css" rel="stylesheet">
<link href="/css/site.css" rel="stylesheet">

<!-- JavaScript -->
<script src="/js/jquery.min.js" type="text/javascript"></script>
<script src="/js/bootstrap.bundle.min.js" type="text/javascript"></script>
<script src="/js/highlight.pack.js" type="text/javascript"></script>
<script src="/js/include_html.js" type="text/javascript"></script>
<script src="/js/mathjax/tex-chtml.js" type="text/javascript"></script>
<script src="/js/site.js" type="text/javascript"></script>

</head>

<body>

<include src="/header.html"></include>

<div class="container">
<div class="row">
<div class="col-md-12">
<h1 class="my-4">Software Engineering</h1>

<!-- Web crawler BEGIN -->
<div class="card mb-4" id="web-crawler">
  <div class="card-body">
    <h2 class="card-title">Web crawler</h2>
    <ul class="list-unstyled mb-0">
      <li><a href="#web-crawler-1">Requirement</a></li>
      <li><a href="#web-crawler-2">Estimation</a></li>
      <li><a href="#web-crawler-3">High level design</a></li>
      <!-- <li><a href="#web-crawler-4">API</a></li> -->
      <!-- <li><a href="#web-crawler-5">Storage schema</a></li> -->
      <li><a href="#web-crawler-6">Detailed design</a></li>
      <li><a href="#web-crawler-7">Evaluation</a></li>
      <li><a href="#web-crawler-8">Follow up</a></li>
    </ul>
  </div>
</div>

<div class="card mb-4" id="web-crawler-1">
  <div class="card-body">
    <h2 class="card-title">Requirement</h2>

    <ul>
      <li>Business objective</li>
      <ul>
        <li>What does web crawler need to do? Search engine indexing</li>
        <li>What content types are in scpe? HTML only</li>
        <li>What about duplicate content? Duplicate content should be ignored</li>
      </ul>
      <li>Estimation</li>
      <ul>
        <li>How many web pages to crawl? 1B pages per month</li>
        <li>What is the size of each HTML page? 100KB including metadata</li>
        <li>How long does it take to traverse one page? 100ms</li>
        <li>Do we need to store HTML page? Yes for 5 years</li>
      </ul>
    </ul>

    <h3 class="card-title">Non-functional</h3>
    <ul>
      <li>Availability</li>
      <li>Reliability</li>
      <li>Scalability</li>
      <li>Performance</li>
      <li>Consistency</li>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: System Design Interview, Alex Xu
  </div>
</div>

<div class="card mb-4" id="web-crawler-2">
  <div class="card-body">
    <h2 class="card-title">Estimation</h2>

    <h3 class="card-title">Storage</h3>
    <ul>
      <li>1B per month * 100KB = 100TB per month</li>
      <li>100TB per month = 6PB for 5 years</li>
    </ul>

    <h3 class="card-title">Bandwidth</h3>
    <ul>
      <li>Incoming</li>
      <ul>
        <li>100TB * 8 / (86400 * 30) = 3Gbps</li>
      </ul>
    </ul>

    <h3 class="card-title">Server</h3>
    <ul>
      <li>100ms * 1B per month = 100Ms = 1157 days</li>
      <li>To crawl the web (one month of data) in one day, we need 1157 servers</li>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: System Design Interview, Alex Xu
  </div>
</div>

<div class="card mb-4" id="web-crawler-3">
  <div class="card-body">
    <h2 class="card-title">High level design</h2>

    <h3 class="card-title">Components</h3>

    <img class="img-fluid" class="card-img-top" src="/software-engineering/system-design/web-crawler1.png" alt="Card image cap">

    <ul>
      <li>Seed URLs</li>
      <ul>
        <li>Base URLs</li>
      </ul>
      <li>URL frontier (priority queue)</li>
      <ul>
        <li>Contains all the remaining URLs to download</li>
      </ul>
      <li>Service host</li>
      <ul>
        <li>Each worker dequeues URL from the URL frontier</li>
        <li>Each worker uses DNS resolver to acquire web page's IP address</li>
      </ul>
      <li>DNS resolver</li>
      <ul>
        <li>Custom DNS resolver is needed because DNS lookup is time consuming process</li>
        <li>Cache frequently used IP addresses within their TTL</li>
      </ul>
      <li>Content parser</li>
      <ul>
        <li>HTML is parsed and validated</li>
      </ul>
      <li>Duplicate HTML eliminator</li>
      <ul>
        <li>Checks if document is duplicate by checking the hash values of two web pages</li>
      </ul>
      <li>URL extractor</li>
      <ul>
        <li>Extracts links from HTML pages</li>
      </ul>
      <li>URL filter</li>
      <ul>
        <li>Exclude certain content types, file extensions, and blacklisted sites</li>
      </ul>
      <li>Duplicate URL eliminator</li>
      <ul>
        <li>Bloom filter or hash table is used</li>
      </ul>
      <li>URL storage</li>
      <ul>
        <li>Stores already visited URLs</li>
      </ul>
    </ul>

    <h3 class="card-title">Workflow</h3>
    <ul>
      <li>Add seed URLs to URL frontier</li>
      <li>Service host fetches URLs from URL frontier</li>
      <li>Service host gets IP addresses of URLs from DNS resolver and starts downloading</li>
      <li>Content parser validates HTML pages</li>
      <li>Duplicate HTML eliminator checks if HTML is already in storage</li>
      <li>Duplicate URL eliminator checks if URL is already in storage</li>
      <ul>
        <li>If URL is not duplicate, it is added to URL frontier</li>
      </ul>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: System Design Interview, Alex Xu
  </div>
</div>

<div class="card mb-4" id="web-crawler-6">
  <div class="card-body">
    <h2 class="card-title">Detailed design</h2>

    <h3 class="card-title">URL frontier (priority queue)</h3>

    <img class="img-fluid" class="card-img-top" src="/software-engineering/system-design/web-crawler2.png" alt="Card image cap">

    <ul>
      <li>Algorithm</li>
      <ul>
        <li>Use BFS</li>
        <li>DFS is not a good choice because depth can be very deep</li>
      </ul>
      <li>Priority</li>
      <ul>
        <li>Prioritizer computes priority of each URL</li>
        <li>Each queue is assigned a priority</li>
        <li>Queue selector has higher probability of picking queues with higher priority</li>
      </ul>
      <li>Politeness</li>
      <ul>
        <li>Download one page at a time from the same host</li>
        <li>Each thread in service host has a queue and download URLs from that queue</li>
        <li>Queue router ensures that each queue only contains URLs from the same host</li>
        <li>Mapping table maps host (ex. wikipedia.com) to queue</li>
        <li>Queue selector rotates worker threads that downloads webpages</li>
      </ul>
      <li>Freshness</li>
      <ul>
        <li>Recrawl is needed because webpages are constantly updated</li>
        <li>Recrawl URLs with higher priority first</li>
      </ul>
      <li>Storage</li>
      <ul>
        <li>Due to huge size of URLs, need to store URLs into a disk</li>
        <li>But, disk is slow, thus introduce buffers in memory</li>
        <ul>
          <li>Enqueue buffer - once filled, written to disk</li>
          <li>Dequeue buffer - keeps cache of URLs to be visited. It periodically reads from disk to fill the buffer</li>
        </ul>
      </ul>
    </ul>

    <h3 class="card-title">Service host</h3>
    <ul>
      <li>Robots.txt</li>
      <ul>
        <li>Specifies pages that are not allowed to be downloaded</li>
      </ul>
      <li>Performance optimization</li>
      <ul>
        <li>Crawl jobs are distributed into multiple servers, and each server runs multiple threads</li>
        <li>Response from DNS resolver is cached in Service host</li>
        <li>Place crawl servers close to website hosts in terms of geographical region</li>
        <li>Enforce maximum wait time from the host</li>
      </ul>
      <li>Robustness</li>
      <ul>
        <li>Use consistent hashing to distribute load among crawl jobs</li>
        <li>Save crawl states into disk (When failure happens, can resume from the saved state)</li>
      </ul>
    </ul>

    <h3 class="card-title">User experience during failure</h3>

    <h3 class="card-title">Potential bottlenecks</h3>

    <h3 class="card-title">10x scale</h3>
  </div>
  <div class="card-footer text-muted">
    Reference: System Design Interview, Alex Xu
  </div>
</div>

<div class="card mb-4" id="web-crawler-7">
  <div class="card-body">
    <h2 class="card-title">Evaluation</h2>

    <h3 class="card-title">Functional</h3>
    <ul>
      <li>Business objective</li>
      <ul>
        <li>HTML pages are downloaded and stored in HTML storage</li>
        <li>Duplicate contents are ignored by duplicate URL eliminator</li>
      </ul>
    </ul>

    <h3 class="card-title">Non-functional</h3>
    <ul>
      <li>Availability</li>
      <ul>
        <li>Replication</li>
        <ul>
          <li>Replicate each</li>
          <ul>
            <li></li>
          </ul>
          <li>Replicate entire system to different zone or geographical region</li>
        </ul>
        <li>Fail over</li>
        <ul>
          <li>Option 1. active-active</li>
          <ul>
            <li>Both actives manage traffic, spreading load between them</li>
          </ul>
          <li>Option 2. active-passive</li>
          <ul>
            <li>Active web servers, cache send heartbearts to passives</li>
            <li>If heartbeats stop, passives take over actives' IP and resume service</li>
          </ul>
        </ul>
      </ul>
      <li>Reliability</li>
      <ul>
        <li>Fault tolerance - </li>
        <li>Maintenability - </li>
      </ul>
      <li>Scalability</li>
      <ul>
        <li>Horizontal scaling</li>
        <li>Data partitioning</li>
        <ul>
          <li></li>
          <li></li>
        </ul>
      </ul>
      <li>Performance</li>
      <ul>
        <li>Peak load</li>
        <ul>
          <li></li>
          <li></li>
        </ul>
        <li></li>
        <li></li>
      </ul>
      <li>Consistency</li>
      <ul>
        <li></li>
        <li></li>
        <li></li>
      </ul>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: System Design Interview, Alex Xu
  </div>
</div>

<div class="card mb-4" id="web-crawler-8">
  <div class="card-body">
    <h2 class="card-title">Follow up</h2>
    <ul>
      <li>Explain how to handle dynamically generated links</li>
      <li>Explain how to filter out unwanted pages</li>
      <li>Explain how to perform data analytics</li>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: System Design Interview, Alex Xu
  </div>
</div>
<!-- Web crawler END -->

</div> <!-- /.col-md-12 -->
</div> <!-- /.row -->
</div> <!-- /.container -->

<include src="/footer.html"></include>

</body>

</html>