<!DOCTYPE html>

<html lang="en">

<head>


<!-- Metadata -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="Seungmoon's DevOps Engineering Blog">
<meta name="author" content="Seungmoon Rieh">
<meta name="keywords" content="Python, Groovy, Kubernetes, Docker, Jenkins, Terraform, Bash">

<title>Seungmoon Rieh</title>

<!-- Third Party CSS -->
<link href="../vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
<link href="../vendor/highlight/styles/monokai-sublime.css" rel="stylesheet">

<!-- CSS -->
<link href="../img/seungmoonrieh.jpg" rel="icon">
<link href="../css/site.css" rel="stylesheet">

<!-- Third Party JavaScript -->
<script src="../vendor/jquery/jquery.min.js"></script>
<script src="../vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
<script src="../vendor/highlight/highlight.pack.js"></script>

<!-- JavaScript -->
<script src="../js/site.js"></script>


</head>


<body>


<nav class="navbar navbar-expand-lg navbar-dark bg-dark fixed-top">
<div class="container">
<a class="navbar-brand" href="index.html">Seungmoon Rieh</a>
<button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
</button>
<div class="collapse navbar-collapse" id="navbarResponsive">
    <ul class="navbar-nav ml-auto">
        <li class="nav-item">
            <div class="btn-group">
                <button type="button" class="btn btn-success dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Deploy</button>
                <div class="dropdown-menu">
                    <a class="dropdown-item" href="#kubernetes">Kubernetes</a>
                </div>
            </div>
        </li>
    </ul>
</div> <!-- /.collapse navbar-collapse -->
</div> <!-- /.container -->
</nav> <!-- /.navbar navbar-expand-lg navbar-dark bg-dark fixed-top -->



<div class="container">
<div class="row">
<div class="col-md-12">
    <h1 class="my-4">Software Engineering</h1>



    <!-- Kubernetes BEGIN -->
    <div class="card mb-4" id="kubernetes">
        <div class="card-body">
            <h2 class="card-title">Kubernetes</h2>
            <ul class="list-unstyled mb-0">
                <!-- <li><a href="#kubernetes-0">What is Kubernetes</a></li> -->
                <li><a href="#kubernetes-1">1. Kubernetes API</a></li>
                <li><a href="#kubernetes-2">2. Pods</a></li>
                <li><a href="#kubernetes-3">3. Pod Lifecycle</a></li>
                <li><a href="#kubernetes-4">4. Init Containers</a></li>
                <li><a href="#kubernetes-5">5. Pod Topology Spread Constraints</a></li>
                <li><a href="#kubernetes-6">6. Disruptions</a></li>
                <li><a href="#kubernetes-7">7. Multi-container pod design</a></li>
                <li><a href="#kubernetes-8">8. Deployments</a></li>
                <li><a href="#kubernetes-9">9. ReplicaSet</a></li>
                <li><a href="#kubernetes-10">10. StatefulSet</a></li>
                <li><a href="#kubernetes-11">11. Job</a></li>
                <li><a href="#kubernetes-12">12. Cron Job</a></li>
                <li><a href="#kubernetes-13">13. Labels, Selectors, and Annotations</a></li>
                <li><a href="#kubernetes-14">14. Volumes</a></li>
                <li><a href="#kubernetes-15">15. Persistent Volumes</a></li>
                <li><a href="#kubernetes-16">16. Volume Snapshots</a></li>
                <li><a href="#kubernetes-17">17. Dynamic Volume Provisioning</a></li>

                <!-- <li><a href="#kubernetes-11">11. SecurityContext</a></li>
                <li><a href="#kubernetes-12">12. Managing Resources for Containers</a></li>
                <li><a href="#kubernetes-13">13. Secrets</a></li>
                <li><a href="#kubernetes-14">14. Managing service accounts</a></li>
                <li><a href="#kubernetes-15">15. Liveness and Readiness Probes</a></li> -->
            </ul>
        </div>
    </div>

    <!-- <div class="card mb-4" id="kubernetes-0">
        <div class="card-body">
            <h2 class="card-title">What is Kubernetes</h2>
            <p class="card-text">Container management technology that orchestrates microservice apps</p>
            <ul>
                <li>Master(s) - contorl the cluster, has an interface, exposes the REST API, and consumes JSON. Persistenly store cluster state and config (uses "etcd" key-value store). Kube-controler-manager makes sure if current state matches the desired state. Kube-scheduler watches API server for new pods and assigns works to nodes.</li>
                <li>Nodes - Kublet registres node with cluster, watches API server, instantiates pods, and reports back to master. It also exposes endpoint on port 10255. Container Enginer performs container management (Docker/Rocket). Kube-proxy manages networking, load-balances across all pods in a service</li>
                <li>Pods - A container run inside a pod. Pod is also a unit of scaling. Pod is "Atomic" and exists in a single node</li>
                <li>Services - Pods will have unique IPs so Services are needed to perform load-balancing and traffic management role. Services will find their pods to own by "labels"</li>
                <li>Deployments - YAML or JSON file to be deployed. Versioned/Spec-once, deploy-many/simple rollback-rollforward. Deployed via API Server</li>
                <li>Desired State and Declarative Model - Manifest file written in YAML or JSON, which described desired state (how you want to clusters to look)</li>
            </ul>
        </div>
        <div class="card-footer text-muted">
            Posted on January 27, 2019 by
            <a href="#">Seungmoon Rieh</a>
        </div>
    </div> -->

    <div class="card mb-4" id="kubernetes-1">
        <div class="card-body">
            <h2 class="card-title">1. Kubernetes API</h2>

            <h3 class="card-title">Standard API terminology</h3>
            <p class="card-text">Most K8s resource types are objects, which have unique name to allow idempotent creation (virtual types may not have unique name, for example "permission check")</p>
            <ul>
                <li>Resource type - name used in the URLs (pods, namespaces, services)</li>
                <li>Kind - JSON representation of resource types</li>
                <li>Collection - list of instances of a resource type</li>
                <li>Resource - single instance of the resource type</li>
            </ul>
            <p class="card-text">All resource types are either <strong>cluster-scoped</strong> or <strong>namespace-scoped</strong>. namespace-scoped resource types will be deleted when the namespace is deleted</p>
            <p class="card-text">cluster-scoped</p>
            <ul>
                <li>GET /apis/GROUP/VERSION/RESOURCETYPE</li>
                <li>GET /apis/GROUP/VERSION/RESOURCETYPE/NAME</li>
            </ul>
            <p class="card-text">namespace-scoped</p>
            <ul>
                <li>GET /apis/GROUP/VERSION/namespaces/NAMESPACE/RESOURCETYPE</li>
                <li>GET /apis/GROUP/VERSION/namespaces/NAMESPACE/RESOURCETYPE/NAME</li>
                <li>GET /apis/GROUP/VERSION/namespaces/NAMESPACE/RESOURCETYPE/NAME</li>
            </ul>
            <p class="card-text">A namespace is a cluster-scoped resource type. Retrive all namespaces with "GET /api/v1/namespaces" and particular namespace with "GET /api/v1/namespaces/NAME"</p>
            <p class="card-text">K8s uses "list" to return a collection of resource and "get" to return a single resource</p>
            <p class="card-text">Some resources have sub-resource(s)</p>
            <ul>
                <li>GET /apis/GROUP/VERSION/RESOURCETYPE/NAME/SUBRESOURCE</li>
                <li>GET /apis/GROUP/VERSION/namespaces/NAMESPACE/RESOURCETYPE/NAME/SUBRESOURCE</li>
            </ul>

            <h3 class="card-title">Efficient detection of changes</h3>
            <p class="card-text"><strong>watch</strong> - detects incremental changes in cluster state. Use "resourceVersion" to store the state of resources</p>
            <ul>
                <li>GET /api/v1/namespaces/test/pods - list all pods in given namespace</li>
                <li>GET /api/v1/namespaces/test/pods?watch=1&resourceVersion=10245 - starting resource version 10245, receive notifications for create/delete/update as JSON</li>
            </ul>
            <p class="card-text">K8s server can only store history for a limted time. Clusters using etcd3 preserve changes for the last 5 mins by default. Clients are expected to handle http status code "410 Gone"</p>
            <p class="card-text"><strong>bookmarks</strong> - marks that all changes up to given "resourceVersion" has already been sent. (in an attempt to mitigate the short history window problem)</p>
            <ul>
                <li>GET /api/v1/namespaces/test/pods?watch=1&resourceVersion=10245&allowWatchBookmarks=true</li>
            </ul>

            <h3 class="card-title">Retrieving large results sets in chunks</h3>
            <p class="card-text">Break single large collection requests into small chunks by parameters "limit" and "continue"</p>
            <ul>
                <li>GET /api/v1/pods?limit=500 - retrive all pods in cluster, up to 500</li>
                <li>GET /api/v1/pods?limit=500&continue=ENCODED_CONTINUE_TOKEN - continue from the previous call to get 501-1000 pods</li>
                <li>GET /api/v1/pods?limit=500&continue=ENCODED_CONTINUE_TOKEN_2 - continue from the previous call to get last set of pods</li>
            </ul>

            <h3 class="card-title">Receiving resources as Tables</h3>
            <ul>
                <li>GET /api/v1/pods<br>
                    Accept: application/json;as=Table;g=meta.k8s.io;v=v1beta1<br>
                    - retrive all pods in cluster in table format</li>
            </ul>
            <p class="card-text">Because there are resource types that don't support Table response, client should handle both Table/non-Table case by using content-type</p>
            <ul>
                <li>Accept: application/json;as=Table;g=meta.k8s.io;v=v1beta1, application/json</li>
            </ul>

            <h3 class="card-title">Receiving resources as Protobuf</h3>
            <p class="card-text">This is for better performance at scale</p>
            <ul>
                <li>GET /api/v1/pods
                    Accept: application/vnd.kubernetes.protobuf<br>
                    - retrive all pods in cluster in Protobuf format</li>
                <li>POST /api/v1/namespaces/test/pods
                    Content-Type: application/vnd.kubernetes.protobuf
                    Accept: application/json
                    - create a pod with Protobuf encoded data, but receive response in JSON</li>
            </ul>
            <p class="card-text">Similar to Table response, multiple content-types are needed in the "Accept" header to support resource types that don't have Protobuf support</p>
            <ul>
                <li>Accept: application/vnd.kubernetes.protobuf, application/json</li>
            </ul>

            <h3 class="card-title">Resource deletion</h3>
            <p class="card-text">Takes place in two phases 1. finalization 2. removal. Finalizers are removed in any order. Once the last finalizer is removed, the resource is removed from etcd.</p>

            <h3 class="card-title">Dry-run</h3>
            <p class="card-text">dry-run executes the request up until persisting objects in storage. The reponse body should be as close as possible to the actual run. Authorization of dry and non-dry runs are identical</p>
            <ul>
                <li>POST /api/v1/namespaces/test/pods?dryRun=All<br>
                    Content-Type: application/json<br>
                    Accept: application/json<br>
                    - ALL: every stage runs normal except the final stage of persisting objects in storage</li>
            </ul>
        </div>
        <div class="card-footer text-muted">
            Reference: <a href="https://kubernetes.io/docs/reference/using-api/api-concepts/">Kubernetes API Concepts</a>
        </div>
    </div>

    <div class="card mb-4" id="kubernetes-2">
        <div class="card-body">
            <h2 class="card-title">2. Pods</h2>
            <p class="card-text">Its the smallest deployable unit</p>
            <ul>
                <li>Can contain an init container tha runs during Pod startup</li>
                <li>Similar to Docker containers with shared namespace and volumn</li>
                <li>Pod gets created by resources such as Deployment, Job, or StatefulSet</li>
                <li>Controller for those resources handles Pod replication, rollout, and failure</li>
                <li>Controllers create Pod from Pod Template</li>
<pre><code class="yaml">apiVersion: batch/v1
kind: Job
metadata:
  name: hello
spec:
  template:
    # This is the pod template
    spec:
      containers:
      - name: hello
        image: busybox
        command: ['sh', '-c', 'echo "Hello, Kubernetes!" && sleep 3600']
      restartPolicy: OnFailure
    # The pod template ends here</code></pre>
                <li>Modifying the Pod Template will make StatefulSet to create new Pods, which then will replace old Pods</li>
                <li>Every container in a Pod share the same IP address and port. These containers can communicate to each other using localhost</li>
                <li>Any container in a Pod can enable privileged mode to use OS admin level capabilities</li>
                <li>Static Pods are managed directly by kubelet without API server. Kubelet though will create mirror Pods on API server for each static Pod</li>
            </ul>
        </div>

        <div class="card-footer text-muted">
            Reference: <a href="https://kubernetes.io/docs/concepts/workloads/pods/">Pods</a>
        </div>
    </div>

    <div class="card mb-4" id="kubernetes-3">
        <div class="card-body">
            <h2 class="card-title">3. Pod Lifecycle</h2>
            <ul>
                <li>Pods are created, assinged a unique ID (UUID), and scheduled to nodes. They can never be rescheduled to different nodes</li>
            </ul>
            <p class="card-text">Pod lifecycle</p>
            <ul>
                <li>Pending - containers have not been setup yet</li>
                <li>Running - Pod is bounded to a node. Containers are created but still running</li>
                <li>Succeeded - Containers are terminated with success</li>
                <li>Failed - At least one container is terminated with failure</li>
                <li>Unknown - Pod status cannot be obtained. Most often error communicating with the node</li>
            </ul>
            <p class="card-text">Container lifecycle</p>
            <ul>
                <li>Waiting - running operations to complete startup</li>
                <li>Running - executing without issues</li>
                <li>Terminated - either ran to completion or failed</li>
            </ul>
            <p class="card-text">Container restart policy</p>
            <ul>
                <li><code>spec</code> of Pod has <code>restartPolicy</code>, which has Always, OnFailure, Never. Default is Always</li>
            </ul>
            <p class="card-text">Pod condition</p>
            <ul>
                <li>PodScheduled - Pod is scheduled to a node</li>
                <li>ContainersReady - all containers in Pod are ready</li>
                <li>Initialized - all init containers are started</li>
                <li>Ready - Pod can serve requests</li>
            </ul>
            <p class="card-text">Pod readiness</p>
            <ul>
                <li><code>spec</code> of Pod has <code>readinessGates</code>, that allows additional conditions to be specified</li>
            </ul>
<pre><code class="yaml">kind: Pod
...
spec:
  readinessGates:
    - conditionType: "www.example.com/feature-1"
status:
  conditions:
    - type: Ready                              # a built in PodCondition
      status: "False"
      lastProbeTime: null
      lastTransitionTime: 2018-01-01T00:00:00Z
    - type: "www.example.com/feature-1"        # an extra PodCondition
      status: "False"
      lastProbeTime: null
      lastTransitionTime: 2018-01-01T00:00:00Z
  containerStatuses:
    - containerID: docker://abcd...
      ready: true
...</code></pre>
            <p class="card-text">Container probe</p>
            <ul>
                <li>Kubelet performs diagnostic on a container periodically (this is call Probe)</li>
                <li>Kubelet calls Handler, which is implemented by the container</li>
                <li>ExecAction Handler - executes a command inside container. Diagnostic successful if command exits with 0</li>
                <li>TCPSocketAction Handler - TCP check on IP address on specified port. Diagnostic successful if port is open</li>
                <li>HTTPGetAction Handler - HTTP GET check on IP address on specified port and path. Diagnostic successful if  200 &le; response &lt; 400</li>
            </ul>
            <p class="card-text">livenessProbe</p>
            <ul>
                <li>Indicates whether the container is running. If liveenss probe fails, the kubelet kills the container, and container is subject to its restart policy</li>
            </ul>
            <p class="card-text">readinessProbe</p>
            <ul>
                <li>Indicates whether the container is ready to respond to requests. If readiness probe fails, then endpoint controller removes Pod IP address from Service endpoints that match the Pod</li>
                <li>Used when container needs to load large data, configuration files</li>
            </ul>
            <p class="card-text">startupProbe</p>
            <ul>
                <li>Indicates whether the application within the container has started. If starup probe fails, the kubelet kills the container, and container is subject to its restart policy</li>
                <li>Used when containers take long time to come into service</li>
            </ul>
            <p class="card-text">Pod Termination</p>
            <ul>
                <li>Kubelet tool to delete Pod, with default graceful period of 30 seconds</li>
                <li>Control plane removes shutting-down Pods from Endpoints</li>
                <li>Resources no longer trest shutting-down Pods valid</li>
                <li>When the grace period expires, kubelet triggeres forcible shutdown (contrainer runtime sends SIGKILL to any running processes in containers)</li>
                <li>API server deletes Pod's object</li>
            </ul>

        </div>
        <div class="card-footer text-muted">
            Reference: <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/">Pod Lifecycle</a>
        </div>
    </div>

    <div class="card mb-4" id="kubernetes-4">
        <div class="card-body">
            <h2 class="card-title">4. Init Containers</h2>
            <p class="card-text">Specialized containers that run before app containers in Pod</p>
            <ul>
                <li>Init containers always run to completion</li>
                <li>Each init container must succeed before next one can run</li>
                <li>If init container fails, kubelet repeatly restarts the container</li>
                <li>Init containers do not support lifecycle, livenessProbe, readinessProbe, startupProbe because they must run to completion before Pod can be ready</li>
                <li>Init containers can have custom code and no need to use FROM</li>
                <li>Init containers can be given access to Secret (unlike app containers)</li>
                <li>If Pod restarts, all init containers must run again</li>
                <li>Init container code must be idempotent (because they can be re-run)</li>
            </ul>
<pre><code class="yaml">apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
spec:
  containers:
  - name: myapp-container
    image: busybox:1.28
    command: ['sh', '-c', 'echo The app is running! && sleep 3600']
  initContainers:
  - name: init-myservice
    image: busybox:1.28
    command: ['sh', '-c', "until nslookup myservice.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for myservice; sleep 2; done"]
  - name: init-mydb
    image: busybox:1.28
    command: ['sh', '-c', "until nslookup mydb.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for mydb; sleep 2; done"]</code></pre>
            <p class="card-text">Init containters would be waiting to discover Services named myservice and mydb. </p>
<pre><code class="yaml">---
apiVersion: v1
kind: Service
metadata:
  name: myservice
spec:
  ports:
  - protocol: TCP
    port: 80
    targetPort: 9376
---
apiVersion: v1
kind: Service
metadata:
  name: mydb
spec:
  ports:
  - protocol: TCP
    port: 80
    targetPort: 9377</code></pre>
        </div>

        <div class="card-footer text-muted">
            Reference: <a href="https://kubernetes.io/docs/concepts/workloads/pods/init-containers/">Init Containers</a>
        </div>
    </div>

    <div class="card mb-4" id="kubernetes-5">
        <div class="card-body">
            <h2 class="card-title">5. Pod Topology Spread Constraints</h2>
            <img class="img-fluid" class="card-img-top" src="../img/kubernetes/kubernetes-2-a.png" alt="Card image cap">
<pre><code class="yaml">kind: Pod
apiVersion: v1
metadata:
  name: mypod
  labels:
    foo: bar
spec:
  topologySpreadConstraints:
  - maxSkew: 1
    topologyKey: zone
    whenUnsatisfiable: DoNotSchedule
    labelSelector:
      matchLabels:
        foo: bar
  containers:
  - name: pause
    image: k8s.gcr.io/pause:3.1</code></pre>
            <p class="card-text">If a new Pod goes to Zone A, then the skew will be 3-1=2, which will exceed the maxSkew of 1. Thus, it can only go to Zone B such that</p>
            <img class="img-fluid" class="card-img-top" src="../img/kubernetes/kubernetes-2-b.png" alt="Card image cap">

            <img class="img-fluid" class="card-img-top" src="../img/kubernetes/kubernetes-2-c.png" alt="Card image cap">
<pre><code class="yaml">kind: Pod
apiVersion: v1
metadata:
  name: mypod
  labels:
    foo: bar
spec:
  topologySpreadConstraints:
  - maxSkew: 1
    topologyKey: zone
    whenUnsatisfiable: DoNotSchedule
    labelSelector:
      matchLabels:
        foo: bar
  - maxSkew: 1
    topologyKey: node
    whenUnsatisfiable: DoNotSchedule
    labelSelector:
      matchLabels:
        foo: bar
  containers:
  - name: pause
    image: k8s.gcr.io/pause:3.1</code></pre>
            <p class="card-text">A new Pod can only go to Zone B to meet the maxSkew of 1 in the first constraint. However at the same time, it can only go to Node 2 to meet the maxSkew of 1 in the second constraint. Because whenUnsatisfiable is DoNotSchedule in both constraints, new Pod cannot be scheduled. (it would be scheduled if whenUnsatisfiable is ScheduleAnyway)</p>

            <img class="img-fluid" class="card-img-top" src="../img/kubernetes/kubernetes-2-d.png" alt="Card image cap">
<pre><code class="yaml">kind: Pod
apiVersion: v1
metadata:
  name: mypod
  labels:
    foo: bar
spec:
  topologySpreadConstraints:
  - maxSkew: 1
    topologyKey: zone
    whenUnsatisfiable: DoNotSchedule
    labelSelector:
      matchLabels:
        foo: bar
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: zone
            operator: NotIn
            values:
            - zoneC
  containers:
  - name: pause
    image: k8s.gcr.io/pause:3.1</code></pre>
            <p class="card-text">This will exclude Zone C from the constraint such that a new Pod goes to Zone B rather than Zone C</p>

            <h3 class="card-title">Cluster-Level Default Constraints</h2>
<pre><code class="yaml">apiVersion: kubescheduler.config.k8s.io/v1beta1
kind: KubeSchedulerConfiguration

profiles:
  - pluginConfig:
      - name: PodTopologySpread
        args:
          defaultConstraints:
            - maxSkew: 1
              topologyKey: topology.kubernetes.io/zone
              whenUnsatisfiable: ScheduleAnyway
          defaultingType: List</code></pre>
            <ul>
                <li>Pod Affinity - can place any number of Pods into qualifying topology domains</li>
                <li>Pod Anti-Affinity - can only place one Pod into a single topology domain</li>
            </ul>

        </div>

        <div class="card-footer text-muted">
            Reference: <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/">Pod Topology Spread Constraints</a>
        </div>
    </div>

    <div class="card mb-4" id="kubernetes-6">
        <div class="card-body">
            <h2 class="card-title">6. Disruptions</h2>
            <p class="card-text">There are involuntary disruptions</p>
            <ul>
                <li>Hardware failure</li>
                <li>Kernal panic</li>
                <li>Cloud provider issue</li>
                <li>Network issue</li>
                <li>Pod eviction due to Node having out of resource</li>
            </ul>
            <p class="card-text">There are voluntary disruptions. Application owners can</p>
            <ul>
                <li>Delete the Deployment</li>
                <li>Update the Deployment, causing a restart</li>
                <li>Directly delete Pods by accident</li>
            </ul>
            <p class="card-text">Cluster admins can</p>
            <ul>
                <li>Drain a Node for repair or scale down</li>
                <li>Remove a Pod from a Node to fit in something else</li>
            </ul>
            <p class="card-text">Pod description budgets (PDB)</p>
            <ul>
                <li>Limits the number of Pods down simultaneously from voluntary disruptions</li>
            </ul>
            <p class="card-text">Consider the following scenario where Pod-a, Pod-b, Pod-c are subject to PDB (whose requirement is that at least 2 out of 3 Pods must be available) while Pod-x is not</p>
            <img class="img-fluid" class="card-img-top" src="../img/kubernetes/kubernetes-2-e.png" alt="Card image cap">
            <p class="card-text">Now the cluster admin drains Node 1, which will cause Pod-a and Pod-x to start terminating</p>
            <img class="img-fluid" class="card-img-top" src="../img/kubernetes/kubernetes-2-f.png" alt="Card image cap">
            <p class="card-text">Deployment notices that Pods are terminating, and to reinstate the desired state, it creates replacement Pods (Pod-d and Pod-y)</p>
            <img class="img-fluid" class="card-img-top" src="../img/kubernetes/kubernetes-2-g.png" alt="Card image cap">
            <p class="card-text">The cluster admin now attempts to drain Node 2 and Node 3. However, the drain command will block because of PDB</p>
            <img class="img-fluid" class="card-img-top" src="../img/kubernetes/kubernetes-2-h.png" alt="Card image cap">
            <p class="card-text">At this point, there are three availabe Pods that are subject to PDB</p>
            <img class="img-fluid" class="card-img-top" src="../img/kubernetes/kubernetes-2-i.png" alt="Card image cap">
            <p class="card-text">The cluster admin now attempts to drain Node 2. Either one of Pod-b or Pod-d will be evicted but both cannot be eviced due to PDB. Assuming Pod-b got evicted, the Deployment will create a replacement Pod-e. But since there are not enough resources in Node 2 and 3, the drain will block</p>
            <img class="img-fluid" class="card-img-top" src="../img/kubernetes/kubernetes-2-j.png" alt="Card image cap">
        </div>

        <div class="card-footer text-muted">
            Reference: <a href="https://kubernetes.io/docs/concepts/workloads/pods/disruptions/">Disruptions</a>
        </div>
    </div>

    <!-- <div class="card mb-4" id="kubernetes-2">
        <div class="card-body">
            <h2 class="card-title">2. Services</h2>
            <p class="card-text">Expose an application running on Pods as network service</p>
            <ul>
                <li>Let you access the application both from inside and outside</li>
                <li>IP, DNS, Port of Service never change</li>
                <li>Services are discovered by DNS</li>
            </ul>
        </div>
        <div class="card-footer text-muted">
            Reference: <a href="https://kubernetes.io/docs/concepts/services-networking/service/">Service</a>
        </div>
    </div> -->

    <div class="card mb-4" id="kubernetes-7">
        <div class="card-body">
            <h2 class="card-title">7. Multi-container pod design</h2>
            <p class="card-text">Each pod can have multiple containers (which would run on the same node). This make communication between containers faster and securer, and allow them to share volumns and file systems</p>

            <p class="card-text"><strong>Sidecar</strong></p>
            <p class="card-text">Enhance/extend existing functionality of container</p>
            <p class="card-text">For example, an app container can stream logs to a particular location while the sidecar container mounts the logs to some other directory</p>
<pre><code class="yaml">apiVersion: v1
kind: Pod
metadata:
  name: sidecar-pod
spec:
  volumes:
  - name: logs
    emptyDir: {}
  containers:
  - name: app-container
    image: alpine
    command: ["/bin/sh"]
    args: ["-c", "while true; do date >> /var/log/app.log; sleep 2;done"]
    volumeMounts:
    - name: logs
      mountPath: /var/log
  - name: log-exporter-sidecar
    image: nginx
    ports:
      - containerPort: 80
    volumeMounts:
    - name: logs
      mountPath: /usr/share/nginx/html</code></pre>
            <p class="card-text">"app-container" streams logs to /var/log/app.log while "log-exporter-sidecar" mounts those logs into /usr/share/nginx/html</p>

            <p class="card-text"><strong>Ambassador</strong>
            <p class="card-text">Serves as a proxy to external worlds (this for for legacy apps, ConfigMap should be used for new apps)</p>
            <p class="card-text">For example, when connecting to a DB server and that server config changes across different environments, the ambassador container can act as a TCP proxy to the database, which can be connected via localhost. The sysadmin can use config maps and secrets with the proxy container to inject the correct connection and auth information</p>
<pre><code class="yaml">apiVersion: v1
kind: Pod
metadata:
  name: ambassador-pod
  labels:
    app: ambassador-app
spec:
  volumes:
  - name: shared
    emptyDir: {}
  containers:
  - name: app-container-poller
    image: yauritux/busybox-curl
    command: ["/bin/sh"]
    args: ["-c", "while true; do curl 127.0.0.1:81 > /usr/share/nginx/html/index.html; sleep 10; done"]
    volumeMounts:
    - name: shared
      mountPath: /usr/share/nginx/html
  - name: app-container-server
    image: nginx
    ports:
      - containerPort: 80
    volumeMounts:
    - name: shared
      mountPath: /usr/share/nginx/html
  - name: ambassador-container
    image: bharamicrosystems/nginx-forward-proxy
    ports:
      - containerPort: 81</code></pre>
            <p class="card-text">"app-container-poller" call on port 81 and send stuff to /usr/share/nginx/html/index.html. "app-container-server" listens on  port 80. These two containers share the same mount point. Lastly, "ambassador-container" listens on port 81, so that when users curl on 80 they get response from html page</p>

            <p class="card-text"><strong>Adaptor</strong>
            <p class="card-text">Help standarized heterogeneous system</p>
            <p class="card-text">For example, when there are multiple applications running on separate containers that are outputing logs in different formats, the adaptor container can standardize logs</p>
<pre><code class="yaml">apiVersion: v1
kind: Pod
metadata:
  name: adapter-pod
  labels:
    app: adapter-app
spec:
  volumes:
  - name: logs
    emptyDir: {}
  containers:
  - name: app-container
    image: alpine
    command: ["/bin/sh"]
    args: ["-c", "while true; do date >> /var/log/app.log; sleep 2;done"]
    volumeMounts:
    - name: logs
      mountPath: /var/log
  - name: log-adapter
    image: alpine
    command: ["/bin/sh"]
    args: ["-c", "tail -f /var/log/app.log|sed -e 's/^/Date /' > /var/log/out.log"]
    volumeMounts:
    - name: logs
      mountPath: /var/log</code></pre>
            <p class="card-text">"app-container" outputs stream of dates in log file while "log-adapter" appends a word to those stream of dates</p>

        </div>
        <div class="card-footer text-muted">
            Reference: <a href="https://betterprogramming.pub/understanding-kubernetes-multi-container-pod-patterns-577f74690aee">Understanding Kubernetes Multi-Container Pod Patterns</a>
        </div>
    </div>

    <div class="card mb-4" id="kubernetes-8">
        <div class="card-body">
            <h2 class="card-title">8. Deployments</h2>
            <p class="card-text">Provides declarative updates for Pods and ReplicaSets. Deployment Controller change actual state to desired state at a controlled rate</p>
            <p class="card-text">Usecase</p>
            <ul>
                <li>Create a Deployment to rollout a ReplicaSet, which creates Pods in the background</li>
                <li>Declare the new state of Pods by updating the PodTemplateSpec of the Deployment</li>
                <li>Rollback to an earlier Deployment version</li>
                <li>Scale up the Deployments</li>
                <li>Clean up old ReplicaSets</li>
            </ul>
            <p class="card-text">Create a Deployment</p>
<pre><code class="yaml">apiVersion: apps/v1 # Mandatory field
kind: Deployment # Mandatory field
metadata: # Mandatory field
  name: nginx-deployment # Deployment named ".metadata.name" is created
  labels:
    app: nginx
spec:
  replicas: 3 # Three replicated Pods. If this field does not exist, it will default to 1
  selector: # Required field for "spec". This specifies the label selector of Pod targeted by this Deployment
    matchLabels:
      app: nginx # How Deployment finds which Pods to manage
  template: # Required field for "spec". This is a Pod template, which has the same schema as Pod
    metadata:
      labels:
        app: nginx # Pod label. This must match ".spec.selector"
    spec:
      containers: # nginx container runs nginx image version 1.14.2
      - name: nginx
        image: nginx:1.14.2
        ports:
        - containerPort: 80</code></pre>
            <ul>
                <li>To create Deployment
<pre><code class="bash">kubectl apply -f nginx-deployment.yaml</pre></code></li>
                <li>To check Deployment
<pre><code class="bash">kubectl get deployments</code></pre></li>
<img class="img-fluid" class="card-img-top" src="../img/kubernetes/kubernetes-4-a.png" alt="Card image cap">
                <ul>
                    <li>NAME - names of Deployments in the namespace</li>
                    <li>READY - how many replicas of the application are available to users</li>
                    <li>UP TO DATE - number of replicas updated to achieve the desired state</li>
                    <li>AVAILABLE - how many replicas of the application are available to users</li>
                    <li>AGE - amount of time the appliation has been running</li>
                </ul>
                <li>To check Deployment rollout status
<pre><code class="bash">kubectl rollout status deployment/nginx-deployment</code></pre></li>
                <li>To see the ReplicaSet created by Deployment
<pre><code class="bash">kubectl get rs</code></pre></li>
<img class="img-fluid" class="card-img-top" src="../img/kubernetes/kubernetes-4-a.png" alt="Card image cap">
                <ul>
                    <li>NAME - names of ReplicaSets in the namespace</li>
                    <li>DESIRED - desired number of replicas in the application</li>
                    <li>CURRENT - how many applications are currently running</li>
                    <li>READY - how many replicas of the application are available to users</li>
                    <li>AGE - amount of time the appliation has been running</li>
                </ul>
                <li>To see the labels generated for each Pod
<pre><code class="bash">kubectl get pods --show-labels</code></pre></li>
                <li>To update iamge from nginx:1.14.2 to nginx:1.16.1, run one of the following
<pre><code class="bash">kubectl --record deployment.apps/nginx-deployment set image deployment.v1.apps/nginx-deployment nginx=nginx:1.16.1</code></pre>
<pre><code class="bash">kubectl set image deployment/nginx-deployment nginx=nginx:1.16.1 --record</code></pre></li>
                <li>To see the rollout status
<pre><code class="bash">kubectl rollout status deployment/nginx-deployment</code></pre></li>
            </ul>
            <p class="card-text">Deployment ensures that at least 75% of Pods are up while they are being updated. It also ensures that at most 125% of the desired number of Pods are up</p>

            <p class="card-text">Rollover</p>
            <ul>
                <li>Everytime a new Deployment is observed by Deployment Controller, a ReplicaSet is created to bring up the desired Pods</li>
                <li>If Deployment is updated, the existing ReplicaSet that control Pods whose labels match <code>.spec.selector</code> but whose template does not match <code>.spec.template</code> are scaled down</li>
                <li>Eventually, new ReplicaSet is scaled to <code>.spec.replicas</code> and all old ReplicaSets is scaled to 0</li>
            </ul>

            <p class="card-text">Rollback</p>
            <ul>
                <li>Check the revisions of Deployment</li>
<pre><code class="bash">kubectl rollout history deployment.v1.apps/nginx-deployment</code></pre>
                <li>To see the details of each revision</li>
<pre><code class="bash">kubectl rollout history deployment.v1.apps/nginx-deployment --revision=2</code></pre>
                <li>Rollback to a specific version</li>
<pre><code class="bash">kubectl rollout undo deployment.v1.apps/nginx-deployment --to-revision=2</code></pre>
            </ul>

            <p class="card-text">Scaling</p>
            <ul>
                <li>Scale a Deployment</li>
<pre><code class="bash">kubectl scale deployment.v1.apps/nginx-deployment --replicas=10</code></pre>
                <li>Setup autoscaler for Deployment and choose the minimum and maximum number of Pods</li>
<pre><code class="bash">kubectl autoscale deployment.v1.apps/nginx-deployment --min=10 --max=15 --cpu-percent=80</code></pre>
                <li>Rollback to a specific version</li>
<pre><code class="bash">kubectl rollout undo deployment.v1.apps/nginx-deployment --to-revision=2</code></pre>
            </ul>

            <p class="card-text">Proportional scaling</p>
            <ul>
                <li>Deployment Controller balances additional replicas in the existing ReplicaSets</li>
            </ul>

            <p class="card-text">Pause and resume Deployment</p>
            <ul>
                <li>Can apply multiple fixes in between pausing and resuming without triggering unnecessary rollouts</li>
                <li>To pause the Deployment</li>
<pre><code class="bash">kubectl rollout pause deployment.v1.apps/nginx-deployment</code></pre>
                <li>Can update the image</li>
<pre><code class="bash">kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.16.1</code></pre>
                <li>Can update the resources</li>
<pre><code class="bash">kubectl set resources deployment.v1.apps/nginx-deployment -c=nginx --limits=cpu=200m,memory=512Mi</code></pre>
                <li>To resume the Deployment</li>
<pre><code class="bash">kubectl rollout resume deployment.v1.apps/nginx-deployment</code></pre>
            </ul>

        </div>
        <div class="card-footer text-muted">
            Reference: <a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/">Deployment</a>
        </div>
    </div>

    <div class="card mb-4" id="kubernetes-9">
        <div class="card-body">
            <h2 class="card-title">9. ReplicaSet</h2>
            <p class="card-text">Generally, ReplicaSet should not be manipulated. Rather, Deployment should be used.</p>
            <p class="card-text">ReplicaSet is mapped to Pod by Pod's metadata.ownerReferences field.</p>
<pre><code class="yaml">apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # modify replicas according to your case
  replicas: 3
  selector:
    matchLabels:
      tier: frontend
  template:
    metadata:
      labels:
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google_samples/gb-frontend:v3</code></pre>
<pre><code class="yaml">apiVersion: v1
kind: Pod
metadata:
  name: pod1
  labels:
    tier: frontend
spec:
  containers:
  - name: hello1
    image: gcr.io/google-samples/hello-app:2.0

---

apiVersion: v1
kind: Pod
metadata:
  name: pod2
  labels:
    tier: frontend
spec:
  containers:
  - name: hello2
    image: gcr.io/google-samples/hello-app:1.0</code></pre>
            <p class="card-text">These Pods do not have Controller as their owner reference. And they match the selector of frontend ReplicaSet (right above). This means these Pods will be acquired by that ReplicaSet. Moreover, if the frontend ReplicaSet is already deployed, creating these two additional Pods cause them to immediately terminate because the ReplicaSet exceeds the desired count.</p>
            <p class="card-text">Delete ReplicaSet and its Pods.</p>
<pre><code class="bash">kubectl proxy --port=8080
curl -X DELETE  'localhost:8080/apis/apps/v1/namespaces/default/replicasets/frontend' \
> -d '{"kind":"DeleteOptions","apiVersion":"v1","propagationPolicy":"Foreground"}' \
> -H "Content-Type: application/json"</code></pre>
            <p class="card-text">Delete just a ReplicaSet</p>
<pre><code class="bash">kubectl proxy --port=8080
curl -X DELETE  'localhost:8080/apis/apps/v1/namespaces/default/replicasets/frontend' \
> -d '{"kind":"DeleteOptions","apiVersion":"v1","propagationPolicy":"Orphan"}' \
> -H "Content-Type: application/json"</code></pre>
            <p class="card-text">Scale-down a ReplicaSet</p>
            <ul>
                <li>Pending Pods are scaled-down first</li>
                <li>If controller.kubernetes.io/pod-deletion-cost annotation is set, Pods with lower value are scaled-down second</li>
                <li>Pods on Nodes with more replicas are scaled-down thrid</li>
                <li>Pods created recently are scaled-down fourth</li>
            </ul>
        </div>

        <div class="card-footer text-muted">
            Reference: <a href="https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/">ReplicaSet</a>
        </div>
    </div>

    <div class="card mb-4" id="kubernetes-10">
        <div class="card-body">
            <h2 class="card-title">10. StatefulSet</h2>
            <p class="card-text">Similar to Deployment, but guarantees ordering and uniqueness of Pods.</p>
            <p class="card-text">Usecase</p>
            <ul>
                <li>Stable, unique network identifier</li>
                <li>Stable, persistent storage</li>
                <li>Ordered, graceful deployment and scaling</li>
                <li>Ordered, automated rolling updates</li>
            </ul>
            <p class="card-text">Limitations</p>
            <ul>
                <li>Storage for a Pod must be provisioned by PersistentVolumns Provsioner</li>
                <li>Deleting StatefulSet does not delete volumns associated with it</li>
                <li>StatefulSet requires Headless Service for the network identity of the Pods</li>
                <li>StatefulSet does not guarantee on the termination of Pods when Statefulset gets deleted</li>
            </ul>
            <p class="card-text">Example</p>
            <ul>
                <li>Headless service named "nginx" is used to control the network domain</li>
                <li>3 replicas of nginx container will be launched in unique Pods</li>
                <li>volumeClaimTemplates will provide stable storage using PersistentVolumns</li>
            </ul>
<pre><code class="yaml">apiVersion: v1
kind: Service
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  ports:
  - port: 80
    name: web
  clusterIP: None
  selector:
    app: nginx
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: web
spec:
  selector:
    matchLabels:
      app: nginx # has to match .spec.template.metadata.labels
  serviceName: "nginx"
  replicas: 3 # by default is 1
  template:
    metadata:
      labels:
        app: nginx # has to match .spec.selector.matchLabels
    spec:
      terminationGracePeriodSeconds: 10
      containers:
      - name: nginx
        image: k8s.gcr.io/nginx-slim:0.8
        ports:
        - containerPort: 80
          name: web
        volumeMounts:
        - name: www
          mountPath: /usr/share/nginx/html
  volumeClaimTemplates:
  - metadata:
      name: www
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: "my-storage-class"
      resources:
        requests:
          storage: 1Gi</code></pre>
            <ul>
                <li>Three Pods will be deployed in the order web-0, web-1, web-2</li>
                <li>web-1 will not be deployed until web-0 is Running and Ready</li>
                <li>When scaling down, web-1 will not be terminated until web-2 is fully shutdown</li>
            </ul>
        </div>

        <div class="card-footer text-muted">
            Reference: <a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/">StatefulSet</a>
        </div>
    </div>

    <div class="card mb-4" id="kubernetes-11">
        <div class="card-body">
            <h2 class="card-title">11. Jobs</h2>
            <p class="card-text">A Job creates one more more Pods. It reliably runs one Pod to completion. Deleting a Job will clean up Pods it created. Suspending a Job will delete its active Pods</p>
            <p class="card-text">A Job is better than bare Pod because it can automatically replace failed Pod with new one. While Replication Controller manages Pods that are not expected to terminate, Job manages Pods that are expected to terminate</p>
            <p class="card-text">Example</p>
<pre><code class="yaml">apiVersion: batch/v1
kind: Job
metadata:
  name: pi
spec:
  template:
    spec:
      containers:
      - name: pi
        image: perl
        command: ["perl",  "-Mbignum=bpi", "-wle", "print bpi(2000)"]
      restartPolicy: Never
  backoffLimit: 4</code></pre>
            <p class="card-text">To list all the Pods that belong to a Job</p>
<pre><code class="bash">run pods=$(kubectl get pods --selector=job-name=pi --output=jsonpath='{.items[*].metadata.name}') && echo $pods"</code></pre>
            <p class="card-text">Three types of tasks suitable to run as a Job</p>
            <ul>
                <li>Non-parallel Jobs - normally only one Pod is started. Job is complete as soon as its Pod terminates successfully. Can leave both <code>.spec.completions</code> and <code>.spec.parallelism</code> unset (they will default to 1)</li>
                <li>Parallel Jobs with a fixed completion count - Job is complete when there is one successful Pod for each value in the range 1 to <code>.spec.completions</code></li>
                <li>Parallel Jobs with a work queue - when any Pod from the Job terminates with success, no new Pods are created. Once at least one Pod is terminated with success and all Pods are terminated, Job succeeds. Must leave <code>.spec.completions</code> unset and set <code>.spec.parallelism</code> to a non-negative integer</li>
            </ul>
            <p class="card-text">Requested parallelism <code>.spec.parallelism</code> is set to 1 if not specified. Setting it to 0 makes Job effective paused</p>
            <ul>
                <li>Fixed completion count Jobs - actual number of Pods running in parallel will not exceed the number of remaining completions. Higher value of <code>.spec.parallelism</code> is ignored</li>
                <li>Work queue Jobs - no new Pods are started after any Pod has succeeded</li>
            </ul>
            <p class="card-text">Pod and container failure</p>
            <ul>
                <li>If container fails and <code>.spec.template.spec.restartPolicy = "OnFailure"</code>, Pod stays on the node but container re-runs. You can avoid this by <code>.spec.template.spec.restartPolicy = "Never"</code></li>
                <li>If Pod fails, then Job Controller starts a new Pod</li>
                <li><code>.spec.backoffLimit</code> is specifiy number fo retries before marking Job as failure (default is 6)</li>
            </ul>
            <p class="card-text">Job termination and cleaup</p>
            <ul>
                <li>Delete the Job, all the Pods created by that Job are deleted too</li>
                <li>Setting <code>.spec.activeDeadlineSeconds</code> will make Job fail and terminate all running Pods once <code>activeDeadlineSeconds</code> is reached</li>
            </ul>
<pre><code class="yaml">apiVersion: batch/v1
kind: JobapiVersion: batch/v1
kind: Job
metadata:
  name: pi-with-timeout
spec:
  backoffLimit: 5
  activeDeadlineSeconds: 100
  template:
    spec:
      containers:
      - name: pi
        image: perl
        command: ["perl",  "-Mbignum=bpi", "-wle", "print bpi(2000)"]
      restartPolicy: Never</code></pre>
        </div>

        <div class="card-footer text-muted">
            Reference: <a href="https://kubernetes.io/docs/concepts/workloads/controllers/job/">Jobs</a>
        </div>
    </div>

    <div class="card mb-4" id="kubernetes-12">
        <div class="card-body">
            <h2 class="card-title">12. Cron Job</h2>
            <p class="card-text">It can create Jobs on a repeating schedule or any individual tasks</p>
            <p class="card-text">Example</p>
<pre><code class="yaml">apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: hello
spec:
  schedule: "*/1 * * * *"
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: hello
            image: busybox
            imagePullPolicy: IfNotPresent
            command:
            - /bin/sh
            - -c
            - date; echo Hello from the Kubernetes cluster
          restartPolicy: OnFailure</code></pre>
        </div>
        <div class="card-footer text-muted">
            Reference: <a href="https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/">CronJob</a>
        </div>
    </div>

    <div class="card mb-4" id="kubernetes-13">
        <div class="card-body">
            <h2 class="card-title">13. Labels, Selectors, and Annotations</h2>
            <p class="card-text">Labels - key/value pairs enabling users to map their own structures to system objects (for example, Pods) in loosely coupled fashion. Labels do not need to be unique.</p>
<pre><code class="yaml">"metadata": {
  "labels": {
    "key1" : "value1",
    "key2" : "value2"
  }
}</code></pre>
            <p class="card-text">Example, Pods with two labels <code>environment: production</code> and <code>app: nginx</code></p>
<pre><code class="yaml">apiVersion: v1
kind: Pod
metadata:
  name: label-demo
  labels:
    environment: production
    app: nginx
spec:
  containers:
  - name: nginx
    image: nginx:1.14.2
    ports:
    - containerPort: 80</code></pre>
            <p class="card-text">Selectors - equality-based allows filtering by label keys and values while set-based allows filtering keys according to a set of values. For example,</p>
<pre><code class="bash">kubectl get pods -l environment=production,tier=frontend # equality based
kubectl get pods -l 'environment in (production),tier in (frontend)' # set based</code></pre>
            <p class="card-text">Service and Replication Controller only support equality-based selector</p>
<pre><code class="yaml">selector:
    component: redis</code></pre>
            <p class="card-text">Job, Deployment, ReplicaSet, DaemonSet also support set-based selector</p>
<pre><code class="yaml">selector:
  matchLabels:
    component: redis
  matchExpressions:
    - {key: tier, operator: In, values: [cache]}
    - {key: environment, operator: NotIn, values: [dev]}</code></pre>
            <p class="card-text">Annotations - allows attaching arbitrary non-identifying metadata to objects (while Labels are used to select objects, annotations are for recording metadata)</p>
<pre><code class="yaml">apiVersion: v1
kind: Pod
metadata:
  name: annotations-demo
  annotations:
    imageregistry: "https://hub.docker.com/"
spec:
  containers:
  - name: nginx
    image: nginx:1.14.2
    ports:
    - containerPort: 80</code></pre>
        </div>
        <div class="card-footer text-muted">
            Reference: <a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/">Labels and Selectors</a> | <a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/">Annotations</a>
        </div>
    </div>

    <div class="card mb-4" id="kubernetes-14">
        <div class="card-body">
            <h2 class="card-title">14. Volumes</h2>
            <p class="card-text">Docker images are the root of filesystem hierarchy. Volumes mount at specific path within the image.</p>
            <p class="card-text">ConfigMap allows injecting configration data into Pods. <code>log-config</code> ConfigMap is mounted as a volume at path <code>/etc/config/log_level</code> with Pod called <code>configmap-pod</code></p>
<pre><code class="yaml">apiVersion: v1
kind: Pod
metadata:
  name: configmap-pod
spec:
  containers:
    - name: test
      image: busybox
      volumeMounts:
        - name: config-vol
          mountPath: /etc/config
  volumes:
    - name: config-vol
      configMap:
        name: log-config
        items:
          - key: log_level
            path: log_level</code></pre>
        <p class="card-text"><code>emptyDir</code> is created when Pod is assigned to Node. When Pod is removed from Node, data is deleted permanently</p>
<pre><code class="yaml">apiVersion: v1
kind: Pod
metadata:
  name: test-pd
spec:
  containers:
  - image: k8s.gcr.io/test-webserver
    name: test-container
    volumeMounts:
    - mountPath: /cache
      name: cache-volume
  volumes:
  - name: cache-volume
    emptyDir: {}</code></pre>
        </div>

        <div class="card-footer text-muted">
            Reference: <a href="https://kubernetes.io/docs/concepts/storage/volumes/">Volumes</a>
        </div>
    </div>

    <div class="card mb-4" id="kubernetes-15">
        <div class="card-body">
            <h2 class="card-title">15. Persistent Volumes</h2>
            <p class="card-text">PersistentVolume (PV) is a piece of storage in a cluster. It is similar to Node. PersistentVolumeClaim (PVC) a request for storage by a user. It is similar to Pod. PVC comsume PV resources. While Pod can request CPU and memory, PVC can request specific size and access mode.</p>
            <h3 class="card-title">Binding</h3>
            <p class="card-text">If PV was dynamically provisioned for a PVC, those PV and PVC will bind together. Otherwise, users will get at least what they asked for but volumes maybe at the excess.</p>
            <h3 class="card-title">Storage Object in Use Protection</h3>
            <p class="card-text">If user deletes PVC, deletion is postponed until PVC is not in use by any Pods. If admin deletes PV, deletion is postponed until PV is not bound to PVC.</p>
            <h3 class="card-title">Reserving PV</h3>
<pre><code class="yaml">apiVersion: v1
kind: PersistentVolume
metadata:
  name: foo-pv
spec:
  storageClassName: ""
  claimRef:
    name: foo-pvc
    namespace: foo
  ...</code></pre>
            <h3 class="card-title">PV</h3>
<pre><code class="yaml">apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv0003
spec:
  capacity:
    storage: 5Gi
  volumeMode: Filesystem # Filesystem - default, Block - raw block device
  accessModes:
    - ReadWriteOnce # ReadWriteOnce, ReadWriteMany, ReadOnlyMany. Once - mounted by single Node, Many - mounted by many Nodes
  persistentVolumeReclaimPolicy: Recycle # Retain, Recycle, Delete
  storageClassName: slow
  mountOptions:
    - hard
    - nfsvers=4.1
  nfs:
    path: /tmp
    server: 172.17.0.2</code></pre>
            <h3 class="card-title">PVC</h3>
<pre><code class="yaml">apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: myclaim
spec:
  accessModes:
    - ReadWriteOnce
  volumeMode: Filesystem
  resources:
    requests:
      storage: 8Gi
  storageClassName: slow
  selector:
    matchLabels: # volumn must have a label with this value
      release: "stable"
    matchExpressions: # a list of requirements
      - {key: environment, operator: In, values: [dev]}</code></pre>
            <h3 class="card-title">Claims as Volumns</h3>
            <p class="card-text">Pods access storage by using Claim as volume. Claim must exist in the same namespace as Pod. The cluster finds Claim in Pods's namespace and uses it to get PV.</p>
<pre><code class="yaml">apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
    - name: myfrontend
      image: nginx
      volumeMounts:
      - mountPath: "/var/www/html"
        name: mypd
  volumes:
    - name: mypd
      persistentVolumeClaim:
        claimName: myclaim</code></pre>

        </div>
        <div class="card-footer text-muted">
            Reference: <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/">Persistent Volumes</a>
        </div>
    </div>

    <div class="card mb-4" id="kubernetes-16">
        <div class="card-body">
            <h2 class="card-title">16. Volume Snapshots</h2>
            <p class="card-text"><code>VolumeSnapshotContent</code> - snapshot taken from a volumn. <code>VolumeSnapshot</code> - request for a snapshot by a user. <code>VolumeSnapshot</code> is only available for CSI (Container Storage Interface) drivers. </p>
            <h3 class="card-title">VS</h3>
<pre><code class="yaml">apiVersion: snapshot.storage.k8s.io/v1
kind: VolumeSnapshot
metadata:
  name: new-snapshot-test
spec:
  volumeSnapshotClassName: csi-hostpath-snapclass
  source:
    persistentVolumeClaimName: pvc-test # name of PVC data source for the snapshot</code></pre>
            <h3 class="card-title">VSC</h3>
<pre><code class="yaml">apiVersion: snapshot.storage.k8s.io/v1
kind: VolumeSnapshotContent
metadata:
  name: snapcontent-72d9a349-aacd-42d2-a240-d775650d2455
spec:
  deletionPolicy: Delete
  driver: hostpath.csi.k8s.io
  source:
    volumeHandle: ee0cfb94-f8d4-11e9-b2d8-0242ac110002 # unique identifier creatd on the storage (returned by CSI driver druing volume creation)
  volumeSnapshotClassName: csi-hostpath-snapclass
  volumeSnapshotRef:
    name: new-snapshot-test
    namespace: default
    uid: 72d9a349-aacd-42d2-a240-d775650d2455</code></pre>
        </div>
        <div class="card-footer text-muted">
            Reference: <a href="https://kubernetes.io/docs/concepts/storage/volume-snapshots/">Volume Snapshots</a>
        </div>
    </div>

    <div class="card mb-4" id="kubernetes-17">
        <div class="card-body">
            <h2 class="card-title">17. Dynamic Volume Provisioning</h2>
            <p class="card-text">To enable dynamic provisioning, cluster admin must pre-create StorageClass object for users.</p>
<pre><code class="yaml"> # Create storage class "slow" that provisions persistent disks like standard disk.
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: slow
provisioner: kubernetes.io/gce-pd
parameters:
  type: pd-standard</code></pre>
<pre><code class="yaml"> # Create storage class "fast" that provisions persistent disks like SSD.
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: fast
provisioner: kubernetes.io/gce-pd
parameters:
  type: pd-ssd</code></pre>
            <p class="card-text">Users request dynamically provisioned storage by including a storage class in their <code>PersistentVolumeClaim</code>. When this claim is deleted, the volume gets destroyed.</p>
<pre><code class="yaml">apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: claim1
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: fast
  resources:
    requests:
      storage: 30Gi</code></pre>
            <p class="card-text">Cluster admin can make Claims to use dynamic provisioning by default. This is done by marking a specific StorageClass as default by adding <code>storageclass.kubernetes.io/is-default-class</code> annotation to it.</p>
        </div>
        <div class="card-footer text-muted">
            Reference: <a href="https://kubernetes.io/docs/concepts/storage/dynamic-provisioning/">Dynamic Volume Provisioning</a>
        </div>
    </div>

    <!--
    <div class="card mb-4" id="kubernetes-10">
        <div class="card-body">
            <h2 class="card-title">10. ConfigMaps</h2>
            <p class="card-text">API object used to store non-confidential data in key-value pairs. It decouples environment specific configuration from container images</p>
        </div>
        <div class="card-footer text-muted">
            Reference: <a href="https://kubernetes.io/docs/concepts/configuration/configmap/">ConfigMaps</a>
        </div>
    </div>

    <div class="card mb-4" id="kubernetes-11">
        <div class="card-body">
            <h2 class="card-title">11. SecurityContext</h2>
            <p class="card-text">Defines privilege and access control settings for a Pod or Container</p>
        </div>
        <div class="card-footer text-muted">
            Reference: <a href="https://kubernetes.io/docs/tasks/configure-pod-container/security-context/">Configure a Security Context for a Pod or Container</a>
        </div>
    </div>

    <div class="card mb-4" id="kubernetes-12">
        <div class="card-body">
            <h2 class="card-title">12. Managing Resources for Containers</h2>
            <p class="card-text">Example</p>
<pre><code class="yaml">apiVersion: v1
kind: Pod
metadata:
  name: frontend
spec:
  containers:
  - name: app
    image: images.my-company.example/app:v4
    resources:
      requests:
        memory: "64Mi"
        cpu: "250m"
      limits:
        memory: "128Mi"
        cpu: "500m"
  - name: log-aggregator
    image: images.my-company.example/log-aggregator:v6
    resources:
      requests:
        memory: "64Mi"
        cpu: "250m"
      limits:
        memory: "128Mi"
        cpu: "500m"</code></pre>
        </div>
        <div class="card-footer text-muted">
            Reference: <a href="https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/">Managing Resources for Containers</a>
        </div>
    </div>

    <div class="card mb-4" id="kubernetes-13">
        <div class="card-body">
            <h2 class="card-title">13. Secrets</h2>
            <p class="card-text">A secret can be used in a Pod in three ways</p>
            <ul>
                <li>As files in a volume mounted on one or more of its containers</li>
                <li>As container environment variable</li>
                <li>By the kubelet when pulling images for the Pod</li>
            </ul>
            <p class="card-text">Opaque Secrets - default Secret type</p>
            <p class="card-text">Service account token Secrets - used to store token that identifies a service account</p>
            <p class="card-text">Docker config Secrets - used to store a serialized JSON that follows the same format rule as "~/docker/config.json"</p>
            <p class="card-text">Basic authentication Secrets - used to store credentials needed to basic auth (username/password)</p>
            <p class="card-text">SSH authentication Secrets - used to store data for SSH auth</p>
            <p class="card-text">TLS Secrets - used to store a certificate and its associated key that are typically used for TLS</p>
            <p class="card-text">Bootstrap token Secrets - used during the node bootstrap process</p>
        </div>
        <div class="card-footer text-muted">
            Reference: <a href="https://kubernetes.io/docs/concepts/configuration/secret/">Secrets</a>
        </div>
    </div>

    <div class="card mb-4" id="kubernetes-14">
        <div class="card-body">
            <h2 class="card-title">14. Managing service accounts</h2>
            <p class="card-text"></p>
        </div>
        <div class="card-footer text-muted">
            Reference: <a href="https://kubernetes.io/docs/reference/access-authn-authz/service-accounts-admin/">Managing Service Accounts</a>
        </div>
    </div>

    <div class="card mb-4" id="kubernetes-15">
        <div class="card-body">
            <h2 class="card-title">15. Liveness and Readiness Probes</h2>
            <p class="card-text"></p>
        </div>
        <div class="card-footer text-muted">
            Reference: <a href="https://blog.colinbreck.com/kubernetes-liveness-and-readiness-probes-how-to-avoid-shooting-yourself-in-the-foot/">Kubernetes Liveness and Readiness Probes: How to Avoid Shooting Yourself in the Foot</a>
        </div>
    </div> -->


    <!-- <div class="card mb-4" id="kubernetes-4">
        <div class="card-body">
            <h2 class="card-title">4. Setup Kubernetes Master</h2>

<pre><code class="groovy">sudo -i

hostnamectl set-hostname 'k8s-master'
exec bash
setenforce 0
sed -i --follow-symlinks 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/sysconfig/selinux

firewall-cmd --permanent --add-port=6443/tcp
firewall-cmd --permanent --add-port=2379-2380/tcp
firewall-cmd --permanent --add-port=10250/tcp
firewall-cmd --permanent --add-port=10251/tcp
firewall-cmd --permanent --add-port=10252/tcp
firewall-cmd --permanent --add-port=10255/tcp
firewall-cmd --reload
modprobe br_netfilter
echo '1' > /proc/sys/net/bridge/bridge-nf-call-iptables

cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
exclude=kube*
EOF

yum install kubeadm docker -y
systemctl restart docker && systemctl enable docker
yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes
systemctl restart kubelet && systemctl enable kubelet
kubeadm init

mkdir -p $HOME/.kube
cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
chown $(id -u):$(id -g) $HOME/.kube/config

export kubever=$(kubectl version | base64 | tr -d '\n')
kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=$kubever"

kubectl get nodes
kubectl get pods --all-namespaces</code></pre>

<pre><code class="groovy"># From /etc/hosts
[ip] k8s-master
[ip] k8s-node1
[ip] k8s-node2
[ip] k8s-node3</code></pre>

        </div>
        <div class="card-footer text-muted">
            Reference: <a href="https://www.linuxtechi.com/install-kubernetes-1-7-centos7-rhel7">How to Install Kubernetes (k8s) 1.7 on CentOS 7 / RHEL 7</a>
        </div>
    </div> -->

    <!-- <div class="card mb-4" id="kubernetes-5">
        <div class="card-body">
            <h2 class="card-title">5. Setup Kubernetes Node</h2>
            <p class="card-text">Do this on each node. Replace [k8s-node-name] with appropriate node name</p>

<pre><code class="groovy">sudo -i

hostnamectl set-hostname '[k8s-node-name]'
setenforce 0
sed -i --follow-symlinks 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/sysconfig/selinux
firewall-cmd --permanent --add-port=10250/tcp
firewall-cmd --permanent --add-port=10255/tcp
firewall-cmd --permanent --add-port=30000-32767/tcp
firewall-cmd --permanent --add-port=6783/tcp
firewall-cmd  --reload
echo '1' > /proc/sys/net/bridge/bridge-nf-call-iptables

cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
exclude=kube*
EOF

yum  install kubeadm docker -y
systemctl restart docker && systemctl enable docker
yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes
systemctl restart kubelet && systemctl enable kubelet
systemctl enable docker.service && systemctl start docker.service</code></pre>

<pre><code class="groovy">kubeadm join --token [k8s-master-token] --discovery-token-unsafe-skip-ca-verification [k8s-master-ip]:6443</code></pre>

        </div>
        <div class="card-footer text-muted">
            Reference: <a href="https://www.linuxtechi.com/install-kubernetes-1-7-centos7-rhel7">How to Install Kubernetes (k8s) 1.7 on CentOS 7 / RHEL 7</a>
        </div>
    </div> -->

    <!-- <div class="card mb-4" id="kubernetes-6">
        <div class="card-body">
            <h2 class="card-title">6. Write Deployment for Jenkins</h2>
            <p class="card-text">"deployment.yaml". Replace [dockerhub_user] with appropriate username</p>

<pre><code class="groovy">apiVersion: extensions/v1beta1
kind: Deployment
metadata:
name: jenkins
spec:
replicas: 1
template:
metadata:
labels:
app: jenkins
spec:
containers:
- name: jenkins
image: [dockerhub_user]/jenkins-master
env:
- name: JAVA_OPTS
value: -Djenkins.install.runSetupWizard=false
ports:
- name: http-port
containerPort: 8080
- name: jnlp-port
containerPort: 50000
volumeMounts:
- name: jenkins-home
mountPath: /var/jenkins_home
volumes:
- name: jenkins-home
emptyDir: {}</code></pre>

        </div>
        <div class="card-footer text-muted">
            Reference: <a href="https://rancher.com/blog/2018/2018-11-27-scaling-jenkins/">Deploying and Scaling Jenkins on Kubernetes</a>
        </div>
    </div>

    <div class="card mb-4" id="kubernetes-7">
        <div class="card-body">
            <h2 class="card-title">7. Write Service for Jenkins</h2>
            <p class="card-text">"service.yaml"</p>

<pre><code class="groovy">apiVersion: v1
kind: Service
metadata:
=name: jenkins
spec:
type: LoadBalancer
ports:
- port: 80
targetPort: 8080
selector:
app: jenkins

---

apiVersion: v1
kind: Service
metadata:
name: jenkins-jnlp
spec:
type: ClusterIP
ports:
- port: 50000
targetPort: 50000
selector:
app: jenkins</code></pre>

        </div>
        <div class="card-footer text-muted">
            Reference: <a href="https://rancher.com/blog/2018/2018-11-27-scaling-jenkins/">Deploying and Scaling Jenkins on Kubernetes</a>
        </div>
    </div> -->

    <!-- <div class="card mb-4" id="kubernetes-8">
        <div class="card-body">
            <h2 class="card-title">8. Deploy Jenkins master</h2>
            <p class="card-text">Do this from Kubernetes master</p>

            <ul>
                <li>kubectl apply -f deployment.yaml</li>
                <li>kubectl create -f service.yaml</li>
                <li>kubectl get service</li>
            </ul>
        </div>
        <div class="card-footer text-muted"></div>
    </div>

    <div class="card mb-4" id="kubernetes-9">
        <div class="card-body">
            <h2 class="card-title">9. Deploy Jenkins agent</h2>
            <p class="card-text">Do this from Kubernetes master</p>

            <ul>
                <li>kubectl cluster-info | grep master</li>
                <li>kubectl get pods | grep jenkins</li>
                <li>kubectl describe pod</li>
            </ul>
        </div>
        <div class="card-footer text-muted"></div>
    </div>
 -->    <!-- Kubernetes END -->

</div> <!-- /.col-md-12 -->
</div> <!-- /.row -->
</div> <!-- /.container -->


<footer class="py-5 bg-dark">
    <div class="container">
        <p class="m-0 text-center text-white">Copyright &copy; Seungmoon Rieh 2020</p>
    </div>
</footer>


</body>

</html>