<!DOCTYPE html>

<html lang="en">

<head>

<!-- Metadata -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="">
<meta name="author" content="Seungmoon Rieh">
<meta name="keywords" content="">

<!-- Title and image -->
<title>Seungmoon Rieh</title>
<link href="/img/seungmoonrieh.jpg" rel="icon">

<!-- CSS -->
<link href="/css/bootstrap.min.css" rel="stylesheet">
<link href="/css/monokai-sublime.css" rel="stylesheet">
<link href="/css/site.css" rel="stylesheet">

<!-- JavaScript -->
<script src="/js/jquery.min.js" type="text/javascript"></script>
<script src="/js/bootstrap.bundle.min.js" type="text/javascript"></script>
<script src="/js/highlight.pack.js" type="text/javascript"></script>
<script src="/js/include_html.js" type="text/javascript"></script>
<script src="/js/mathjax/tex-chtml.js" type="text/javascript"></script>
<script src="/js/site.js" type="text/javascript"></script>

</head>

<body>

<include src="/header.html"></include>

<div class="container">
<div class="row">
<div class="col-md-12">
<h1 class="my-4">Machine Learning</h1>

<!-- Recurrent neural network BEGIN -->
<div class="card mb-4" id="subject">
  <div class="card-body">
    <h2 class="card-title">Recurrent neural network</h2>
    <ul class="list-unstyled mb-0">
      <li><a href="#recurrent-neural-network-">Recurrent neural network</a></li>
    </ul>
  </div>
</div>

<div class="card mb-4" id="recurrent-neural-network-">
  <div class="card-body">
    <h2 class="card-title">Recurrent neural network</h2>
    <ul>
      <li>Process input (Ex. a word in a sentence) time step by time step</li>
      <li>First need to extract numerical features from text data</li>
      <ul>
        <li>Ex. bag of words, N-grams, word embeddings</li>
      </ul>
      <li>Remember contexts through hidden layer activations</li>
    </ul>

    <h3 class="card-title">Why not standard network</h3>
    <ul>
      <li>Inputs and outputs can be of different lengths in different examples</li>
      <li>Does not share features learned across different positions of text</li>
    </ul>

    <h3 class="card-title">RNN types</h3>
    <ul>
      <li>One to many</li>
      <ul>
        <li>Ex. music generation</li>
      </ul>
      <li>Many to one</li>
      <ul>
        <li>Ex. sentiment classification</li>
      </ul>
      <li>Many to many</li>
      <ul>
        <li>Ex. named entity recognition, machine translation</li>
      </ul>
    </ul>

    <!-- <ul>
      <li>First need to extract numerical features from text data. (Ex. bag of words, N-grams, word embeddings)</li>
      <li>Process the input (word in a sentence) timestep by timestep.</li>
      <li>Backprop needs separate layer for each time step with the same weights for all layers.</li>
      <li>Different loss per timestep.</li>
      <li>Gradients from multiple timesteps are added to calculate the final gradient.</li>
    </ul> -->

    <h3 class="card-title">Example</h3>
    <ul>
      <li>Input \( x \)</li>
      <ul>
        <li>5000 words</li>
        <li>10 time steps</li>
        <li>20 traning examples</li>
        <li>Then, \( (n_{x}, m, T_{x}) = (5000, 20, 10) \)</li>
      </ul>
      <li>Hidden state \( a \)</li>
      <ul>
        <li>\( (n_{a}, m, T_{x}) \)</li>
      </ul>
      <li>Prediction \( \hat{y} \)</li>
      <ul>
        <li>\( (n_{y}, m, T_{x}) \)</li>
      </ul>
    </ul>

    <h3 class="card-title">Forward prop</h3>
    <ul>
      <li>Repeated use of a single cell by the number of time steps \( T_{x} \)</li>
      <ul>
        <li>\( a^{\langle 0 \rangle} = 0 \)</li>
        <li>\( a^{\langle 1 \rangle} = \text{tanh}(W_{aa}a^{\langle 0 \rangle} + W_{ax}a^{\langle 1 \rangle} + b_{a}) \)</li>
        <li>\( \hat{y}^{\langle 1 \rangle} = \text{softmax}(W_{ya}a^{\langle 1 \rangle} + b_{y}) \)</li>
        <li>\( \dots \)</li>
        <li>\( a^{\langle t \rangle} = \text{tanh}(W_{aa}a^{\langle t-1 \rangle} + W_{ax}a^{\langle t \rangle} + b_{a}) = \text{tanh}(W_{a}[a^{\langle t-1 \rangle}, x^{\langle t \rangle}] + b_{a}) \)</li>
        <li>\( \hat{y}^{\langle t \rangle} = \text{softmax}(W_{ya}a^{\langle t \rangle} + b_{y}) = \text{softmax}(W_{y}a^{\langle t \rangle} + b_{y}) \)</li>
      </ul>
      <li>Each cell takes two inputs at each time step</li>
      <ul>
        <li>\( a^{\langle t-1 \rangle} \) - the hidden state from the previous cell</li>
        <li>\( x^{\langle t \rangle} \) - current time-step's input data</li>
      </ul>
      <li>Each cell produces two outputs at each time step</li>
      <ul>
        <li>\( a^{\langle t \rangle} \) - a hidden state</li>
        <li>\( y^{\langle t \rangle} \) - a prediction</li>
      </ul>
      <li>The weights and biases \( W_{aa}, b_{a}, W_{ax}, b_{x} \) are re-used each time step</li>
    </ul>

<pre><code class="python">def rnn_cell_forward(xt, a_prev, parameters):
    """
    Args:
    xt -- input data at timestep "t", of shape (n_x, m)
    a_prev -- hidden state at timestep "t-1", of shape (n_a, m)
    parameters -- python dictionary containing
                  Wax -- weight matrix multiplying the input, of shape (n_a, n_x)
                  Waa -- weight matrix multiplying the hidden state, of shape (n_a, n_a)
                  Wya -- weight matrix relating the hidden-state to the output, of shape (n_y, n_a)
                  ba -- bias, of shape (n_a, 1)
                  by -- bias relating the hidden-state to the output, of shape (n_y, 1)
    Returns:
    a_next -- next hidden state, of shape (n_a, m)
    yt_pred -- prediction at timestep "t", of shape (n_y, m)
    cache -- values needed for the backward pass (a_next, a_prev, xt, parameters)
    """

    Wax = parameters["Wax"]
    Waa = parameters["Waa"]
    Wya = parameters["Wya"]
    ba = parameters["ba"]
    by = parameters["by"]

    a_next = np.tanh(np.dot(Waa, a_prev) + np.dot(Wax, xt) + ba)
    yt_pred = softmax(np.dot(Wya, a_next) + by)
    cache = (a_next, a_prev, xt, parameters)

    return a_next, yt_pred, cache</code></pre>

<pre><code class="python">def rnn_forward(x, a0, parameters):
    """
    Arg:
    x -- input data for every time-step, of shape (n_x, m, T_x)
    a0 -- Initial hidden state, of shape (n_a, m)
    parameters -- python dictionary containing
                  Wax -- weight matrix multiplying the input, of shape (n_a, n_x)
                  Waa -- weight matrix multiplying the hidden state, of shape (n_a, n_a)
                  Wya -- weight matrix relating the hidden-state to the output, of shape (n_y, n_a)
                  ba -- bias, of shape (n_a, 1)
                  by -- bias relating the hidden-state to the output, of shape (n_y, 1)

    Returns:
    a -- hidden states for every time-step, of shape (n_a, m, T_x)
    y_pred -- predictions for every time-step, of shape (n_y, m, T_x)
    caches -- values needed for the backward pass (list of caches, x)
    """

    caches = []

    n_x, m, T_x = x.shape
    n_y, n_a = parameters["Wya"].shape

    a = np.zeros((n_a, m, T_x))
    y_pred = np.zeros((n_y, m, T_x))

    a_next = a0

    for t in range(T_x):
        a_next, yt_pred, cache = rnn_cell_forward(x[:,:,t], a[:,:,t], parameters)
        a[:,:,t] = a_next
        y_pred[:,:,t] = yt_pred
        caches.append(cache)

    caches = (caches, x)

    return a, y_pred, caches</code></pre>

    <h3 class="card-title">Loss function</h3>
    <ul>
      <li>\( L^{\langle t \rangle}(\hat{y}^{\langle t \rangle}, y^{\langle t \rangle}) = -y^{\langle t \rangle}log\hat{y}^{\langle t \rangle} - (1-y^{\langle t \rangle})log(1-\hat{y}^{\langle t \rangle}) \)</li>
      <li>\( J(\hat{y},y) = \displaystyle\sum_{t=1}^{T_{y}} L^{\langle t \rangle}(\hat{y}^{\langle t \rangle}, y^{\langle t \rangle}) \)</li>
    </ul>

    <h3 class="card-title">Backward prop</h3>
    <ul>
      <li>\( dW_{ax} = da_{\text{next}} * (1-\tanh^2(W_{ax}x^{\langle t \rangle}+W_{aa} a^{\langle t-1 \rangle} + b_{a}) ) x^{\langle t \rangle T} \)</li>
      <li>\( dW_{aa} = da_{\text{next}} * (1-\tanh^2(W_{ax}x^{\langle t \rangle}+W_{aa} a^{\langle t-1 \rangle} + b_{a}) ) a^{\langle t-1 \rangle T} \)</li>
      <li>\( db_{a} = da_{\text{next}} * \displaystyle\sum_{\text{batch}}(1-\tanh^2(W_{ax}x^{\langle t \rangle} + W_{aa} a^{\langle t-1 \rangle} + b_{a})) \)</li>
      <li>\( dx^{\langle t \rangle} = da_{\text{next}} * {W_{ax}}^T (1-\tanh^2(W_{ax}x^{\langle t \rangle}+W_{aa} a^{\langle t-1 \rangle} + b_{a})) \)</li>
      <li>\( da_{\text{prev}} = da_{\text{next}} * {W_{aa}}^T (1-\tanh^2(W_{ax}x^{\langle t \rangle}+W_{aa} a^{\langle t-1 \rangle} + b_{a})) \)</li>
    </ul>

<pre><code class="python">def rnn_cell_backward(da_next, cache):
    """
    Args:
    da_next -- gradient of loss with respect to next hidden state
    cache -- output of rnn_cell_forward()

    Returns:
    gradients -- python dictionary containing
                 dx -- gradients of input data, of shape (n_x, m)
                 da_prev -- gradients of previous hidden state, of shape (n_a, m)
                 dWax -- gradients of input-to-hidden weights, of shape (n_a, n_x)
                 dWaa -- gradients of hidden-to-hidden weights, of shape (n_a, n_a)
                 dba -- gradients of bias vector, of shape (n_a, 1)
    """

    (a_next, a_prev, xt, parameters) = cache

    Wax = parameters["Wax"]
    Waa = parameters["Waa"]
    Wya = parameters["Wya"]
    ba = parameters["ba"]
    by = parameters["by"]

    dtanh = (1- a_next**2) * da_next

    dxt = np.dot(Wax.T, dtanh)
    dWax = np.dot(dtanh, xt.T)

    da_prev = np.dot(Waa.T, dtanh)
    dWaa = np.dot(dtanh, a_prev.T)

    dba = np.sum(dtanh, keepdims=True, axis=-1)

    gradients = {"dxt": dxt, "da_prev": da_prev, "dWax": dWax, "dWaa": dWaa, "dba": dba}

    return gradients</code></pre>

<pre><code class="python">def rnn_backward(da, caches):
    """
    Args:
    da -- upstream gradients of all hidden states, of shape (n_a, m, T_x)
    caches -- output of rnn_forward()

    Returns:
    gradients -- python dictionary containing
                 dx -- gradient w.r.t. the input data, of shape (n_x, m, T_x)
                 da0 -- gradient w.r.t the initial hidden state, of shape (n_a, m)
                 dWax -- gradient w.r.t the input's weight matrix, of shape (n_a, n_x)
                 dWaa -- gradient w.r.t the hidden state's weight matrix, of shape (n_a, n_a)
                 dba -- gradient w.r.t the bias, of shape (n_a, 1)
    """

    (caches, x) = caches
    (a1, a0, x1, parameters) = caches[0]

    n_a, m, T_x = da.shape
    n_x, m = x1.shape

    dx = np.zeros((n_x, m, T_x))
    dWax = np.zeros((n_a, n_x))
    dWaa = np.zeros((n_a, n_a))
    dba = np.zeros((n_a, 1))
    da0 = np.zeros((n_a, m))
    da_prevt = np.zeros((n_a, m))

    for t in reversed(range(T_x)):
        gradients = rnn_cell_backward(da[:, :, t] + da_prevt, caches[t])
        dxt, da_prevt, dWaxt, dWaat, dbat = gradients["dxt"], gradients["da_prev"], gradients["dWax"], gradients["dWaa"], gradients["dba"]
        dx[:, :, t] = dxt
        dWax += dWaxt
        dWaa += dWaat
        dba += dbat

    da0 = da_prevt

    gradients = {"dx": dx, "da0": da0, "dWax": dWax, "dWaa": dWaa,"dba": dba}

    return gradients</code></pre>

    <h3 class="card-title">Language model</h3>
    <ul>
      <li>Compute \( P(sentence) = P(y^{&lt;1&gt;} \dots y^{&lt;t&gt;}) \)</li>
      <ul>
        <li>Ex. P(The apple and pair salad) = \( 3.2 \times 10^{-13} \), P(The apple and peer salad) = \( 5.7 \times 10^{-13} \)</li>
        <li>\( P(y^{<1>}, y^{<2>}, y^{<3>}) = P(y^{<1>})P(y^{<2>}|y^{<1>})P(y^{<3>}|y^{<1>},y^{<2>}) \)</li>
      </ul>
      <li>Training set - large english corpus</li>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://www.deeplearning.ai/deep-learning-specialization">Deep Learning Specialization</a>
  </div>
</div>
<!-- Recurrent neural network END -->

</div> <!-- /.col-md-12 -->
</div> <!-- /.row -->
</div> <!-- /.container -->

<include src="/footer.html"></include>

</body>

</html>