<!DOCTYPE html>

<html lang="en">

<head>

<!-- Metadata -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="">
<meta name="author" content="Seungmoon Rieh">
<meta name="keywords" content="">

<!-- Title and image -->
<title>Seungmoon Rieh</title>
<link href="/img/seungmoonrieh.jpg" rel="icon">

<!-- CSS -->
<link href="/css/bootstrap.min.css" rel="stylesheet">
<link href="/css/monokai-sublime.css" rel="stylesheet">
<link href="/css/site.css" rel="stylesheet">

<!-- JavaScript -->
<script src="/js/jquery.min.js" type="text/javascript"></script>
<script src="/js/bootstrap.bundle.min.js" type="text/javascript"></script>
<script src="/js/highlight.pack.js" type="text/javascript"></script>
<script src="/js/include_html.js" type="text/javascript"></script>
<script src="/js/mathjax/tex-chtml.js" type="text/javascript"></script>
<script src="/js/site.js" type="text/javascript"></script>

</head>

<body>

<include src="/header.html"></include>

<div class="container">
<div class="row">
<div class="col-md-12">
<h1 class="my-4">Machine Learning</h1>

<!-- Recurrent neural network BEGIN -->
<div class="card mb-4" id="subject">
  <div class="card-body">
    <h2 class="card-title">Recurrent neural network</h2>
    <ul class="list-unstyled mb-0">
      <li><a href="#recurrent-neural-network-">Recurrent neural network</a></li>
      <li><a href="#recurrent-neural-network-">Long short term memory</a></li>
    </ul>
  </div>
</div>

<div class="card mb-4" id="recurrent-neural-network-">
  <div class="card-body">
    <h2 class="card-title">Recurrent neural network</h2>
    <ul>
      <li>Process the input (word in a sentence) timestep by timestep</li>
      <li>First need to extract numerical features from text data</li>
      <ul>
        <li>Ex. bag of words, N-grams, word embeddings</li>
      </ul>
    </ul>

    <h3 class="card-title">Why not standard network</h3>
    <ul>
      <li>Inputs and outputs can be of different lengths in different examples</li>
      <li>Does not share features learned across different positions of text</li>
    </ul>

    <h3 class="card-title">RNN types</h3>
    <ul>
      <li>One to many</li>
      <ul>
        <li>Ex. music generation</li>
      </ul>
      <li>Many to one</li>
      <ul>
        <li>Ex. sentiment classification</li>
      </ul>
      <li>Many to many</li>
      <ul>
        <li>Ex. named entity recognition, machine translation</li>
      </ul>
    </ul>

    <!-- <ul>
      <li>First need to extract numerical features from text data. (Ex. bag of words, N-grams, word embeddings)</li>
      <li>Process the input (word in a sentence) timestep by timestep.</li>
      <li>Backprop needs separate layer for each time step with the same weights for all layers.</li>
      <li>Different loss per timestep.</li>
      <li>Gradients from multiple timesteps are added to calculate the final gradient.</li>
    </ul> -->

    <h3 class="card-title">Forward prop</h3>
    <ul>
      <li>Repeated use of a single cell by the number of time steps \( T_{x} \)</li>
      <ul>
        <li>\( a^{\langle 0 \rangle} = 0 \)</li>
        <li>\( a^{\langle 1 \rangle} = \text{tanh}(W_{aa}a^{\langle 0 \rangle} + W_{ax}a^{\langle 1 \rangle} + b_{a}) \)</li>
        <li>\( \hat{y}^{\langle 1 \rangle} = \text{softmax}(W_{ya}a^{\langle 1 \rangle} + b_{y}) \)</li>
        <li>\( \dots \)</li>
        <li>\( a^{\langle t \rangle} = \text{tanh}(W_{aa}a^{\langle t-1 \rangle} + W_{ax}a^{\langle t \rangle} + b_{a}) = \text{tanh}(W_{a}[a^{\langle t-1 \rangle}, x^{\langle t \rangle}] + b_{a}) \)</li>
        <li>\( \hat{y}^{\langle t \rangle} = \text{softmax}(W_{ya}a^{\langle t \rangle} + b_{y}) = \text{softmax}(W_{y}a^{\langle t \rangle} + b_{y}) \)</li>
      </ul>
      <li>Each cell takes two inputs at each time step</li>
      <ul>
        <li>\( a^{\langle t-1 \rangle} \) - the hidden state from the previous cell</li>
        <li>\( x^{\langle t \rangle} \) - current time-step's input data</li>
      </ul>
      <li>Each cell produces two outputs at each time step</li>
      <ul>
        <li>\( a^{\langle t \rangle} \) - a hidden state</li>
        <li>\( y^{\langle t \rangle} \) - a prediction</li>
      </ul>
      <li>The weights and biases \( W_{aa}, b_{a}, W_{ax}, b_{x} \) are re-used each time step</li>
    </ul>

<pre><code class="python">def rnn_cell_forward(xt, a_prev, parameters):
    """
    Args:
    xt -- input data at timestep "t", of shape (n_x, m)
    a_prev -- hidden state at timestep "t-1", of shape (n_a, m)
    parameters -- python dictionary containing
                  Wax -- weight matrix multiplying the input, of shape (n_a, n_x)
                  Waa -- weight matrix multiplying the hidden state, of shape (n_a, n_a)
                  Wya -- weight matrix relating the hidden-state to the output, of shape (n_y, n_a)
                  ba -- bias, of shape (n_a, 1)
                  by -- bias relating the hidden-state to the output, of shape (n_y, 1)
    Returns:
    a_next -- next hidden state, of shape (n_a, m)
    yt_pred -- prediction at timestep "t", of shape (n_y, m)
    cache -- values needed for the backward pass (a_next, a_prev, xt, parameters)
    """

    Wax = parameters["Wax"]
    Waa = parameters["Waa"]
    Wya = parameters["Wya"]
    ba = parameters["ba"]
    by = parameters["by"]

    a_next = np.tanh(np.dot(Waa, a_prev) + np.dot(Wax, xt) + ba)
    yt_pred = softmax(np.dot(Wya, a_next) + by)
    cache = (a_next, a_prev, xt, parameters)

    return a_next, yt_pred, cache</code></pre>

<pre><code class="python">def rnn_forward(x, a0, parameters):
    """
    Arg:
    x -- input data for every time-step, of shape (n_x, m, T_x)
    a0 -- Initial hidden state, of shape (n_a, m)
    parameters -- python dictionary containing
                  Wax -- weight matrix multiplying the input, of shape (n_a, n_x)
                  Waa -- weight matrix multiplying the hidden state, of shape (n_a, n_a)
                  Wya -- weight matrix relating the hidden-state to the output, of shape (n_y, n_a)
                  ba -- bias, of shape (n_a, 1)
                  by -- bias relating the hidden-state to the output, of shape (n_y, 1)

    Returns:
    a -- hidden states for every time-step, of shape (n_a, m, T_x)
    y_pred -- predictions for every time-step, of shape (n_y, m, T_x)
    caches -- values needed for the backward pass (list of caches, x)
    """

    caches = []

    n_x, m, T_x = x.shape
    n_y, n_a = parameters["Wya"].shape

    a = np.zeros((n_a, m, T_x))
    y_pred = np.zeros((n_y, m, T_x))

    a_next = a0

    for t in range(T_x):
        a_next, yt_pred, cache = rnn_cell_forward(x[:,:,t], a[:,:,t], parameters)
        a[:,:,t] = a_next
        y_pred[:,:,t] = yt_pred
        caches.append(cache)

    caches = (caches, x)

    return a, y_pred, caches</code></pre>

    <h3 class="card-title">Loss function</h3>
    <ul>
      <li>\( L^{\langle t \rangle}(\hat{y}^{\langle t \rangle}, y^{\langle t \rangle}) = -y^{\langle t \rangle}log\hat{y}^{\langle t \rangle} - (1-y^{\langle t \rangle})log(1-\hat{y}^{\langle t \rangle}) \)</li>
      <li>\( L(\hat{y},y) = \displaystyle\sum_{t=1}^{T_{y}} L^{\langle t \rangle}(\hat{y}^{\langle t \rangle}, y^{\langle t \rangle}) \)</li>
    </ul>

    <h3 class="card-title">Backward prop</h3>

<pre><code class="python">def rnn_cell_backward(da_next, cache):
    """
    Args:
    da_next -- gradient of loss with respect to next hidden state
    cache -- output of rnn_cell_forward()

    Returns:
    gradients -- python dictionary containing
                 dx -- gradients of input data, of shape (n_x, m)
                 da_prev -- gradients of previous hidden state, of shape (n_a, m)
                 dWax -- gradients of input-to-hidden weights, of shape (n_a, n_x)
                 dWaa -- gradients of hidden-to-hidden weights, of shape (n_a, n_a)
                 dba -- gradients of bias vector, of shape (n_a, 1)
    """

    (a_next, a_prev, xt, parameters) = cache

    Wax = parameters["Wax"]
    Waa = parameters["Waa"]
    Wya = parameters["Wya"]
    ba = parameters["ba"]
    by = parameters["by"]

    dtanh = (1- a_next**2) * da_next

    dxt = np.dot(Wax.T, dtanh)
    dWax = np.dot(dtanh, xt.T)

    da_prev = np.dot(Waa.T, dtanh)
    dWaa = np.dot(dtanh, a_prev.T)

    dba = np.sum(dtanh, keepdims=True, axis=-1)

    gradients = {"dxt": dxt, "da_prev": da_prev, "dWax": dWax, "dWaa": dWaa, "dba": dba}

    return gradients</code></pre>

<pre><code class="python">def rnn_backward(da, caches):
    """
    Args:
    da -- upstream gradients of all hidden states, of shape (n_a, m, T_x)
    caches -- output of rnn_forward()

    Returns:
    gradients -- python dictionary containing
                 dx -- gradient w.r.t. the input data, of shape (n_x, m, T_x)
                 da0 -- gradient w.r.t the initial hidden state, of shape (n_a, m)
                 dWax -- gradient w.r.t the input's weight matrix, of shape (n_a, n_x)
                 dWaa -- gradient w.r.t the hidden state's weight matrix, of shape (n_a, n_a)
                 dba -- gradient w.r.t the bias, of shape (n_a, 1)
    """

    (caches, x) = caches
    (a1, a0, x1, parameters) = caches[0]

    n_a, m, T_x = da.shape
    n_x, m = x1.shape

    dx = np.zeros((n_x, m, T_x))
    dWax = np.zeros((n_a, n_x))
    dWaa = np.zeros((n_a, n_a))
    dba = np.zeros((n_a, 1))
    da0 = np.zeros((n_a, m))
    da_prevt = np.zeros((n_a, m))

    for t in reversed(range(T_x)):
        gradients = rnn_cell_backward(da[:, :, t] + da_prevt, caches[t])
        dxt, da_prevt, dWaxt, dWaat, dbat = gradients["dxt"], gradients["da_prev"], gradients["dWax"], gradients["dWaa"], gradients["dba"]
        dx[:, :, t] = dxt
        dWax += dWaxt
        dWaa += dWaat
        dba += dbat

    da0 = da_prevt

    gradients = {"dx": dx, "da0": da0, "dWax": dWax, "dWaa": dWaa,"dba": dba}

    return gradients</code></pre>

    <h3 class="card-title">Language model</h3>
    <ul>
      <li>Compute \( P(sentence) = P(y^{&lt;1&gt;} \dots y^{&lt;t&gt;}) \)</li>
      <ul>
        <li>Ex. P(The apple and pair salad) = \( 3.2 \times 10^{-13} \), P(The apple and peer salad) = \( 5.7 \times 10^{-13} \)</li>
        <li>\( P(y^{<1>}, y^{<2>}, y^{<3>}) = P(y^{<1>})P(y^{<2>}|y^{<1>})P(y^{<3>}|y^{<1>},y^{<2>}) \)</li>
      </ul>
      <li>Training set - large english corpus</li>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://www.deeplearning.ai/deep-learning-specialization">Deep Learning Specialization</a>
  </div>
</div>

<div class="card mb-4" id="recurrent-neural-network-">
  <div class="card-body">
    <h2 class="card-title">Long short term memory</h2>
    <ul>
      <li>Addresses vanishing gradient</li>
      <li>If exploding gradient, apply gradient clipping</li>
    </ul>

    <h3 class="card-title">Vanishing gradients with RNNs</h3>
    <ul>
      <li>Ex. "the cats, which, ..., were full" vs "the cat, which, ..., was full"</li>
      <li>Capturing long-term dependencies is hard</li>
    </ul>

    <h3 class="card-title">Candidate value</h3>
    <ul>
      <li>Tensor containing values between -1 to 1</li>
      <li>Information from current time step that may be stored in current cell state</li>
      <li>\( \tilde{c}^{\langle t \rangle} = tanh(W_{c}[a^{\langle t-1 \rangle}, x^{\langle t \rangle}] + b_{c}) \)</li>
    </ul>

    <h3 class="card-title">Update gate</h3>
    <ul>
      <li>Tensor containing values between 0 and 1</li>
      <ul>
        <li>If unit in update gate is close to 1, it allows the value of the candidate to be passed onto the hidden state</li>
        <li>If unit in update gate is close to 0, it prevents the corresponding value in the candidate from being passed onto the hidden state</li>
      </ul>
      <li>Decide what aspects of the candidate to add to the cell state</li>
      <li>\( \Gamma_{u}^{\langle t \rangle} = \sigma(W_{u}[a^{\langle t-1 \rangle}, x^{\langle t \rangle}] + b_{u}) \)</li>
    </ul>

    <h3 class="card-title">Forget gate</h3>
    <ul>
      <li>Tensor containing values between 0 and 1</li>
      <ul>
        <li>If unit in forget gate has value close to 0, LSTM will "forget" the stored state in the previous cell state</li>
        <li>If unit in forget gate has value close to 1, LSTM will mostly remember the corresponding value in the stored state</li>
      </ul>
      <li>\( \Gamma_{f}^{\langle t \rangle} = \sigma(W_{f}[a^{\langle t-1 \rangle}, x^{\langle t \rangle}] + b_{f}) \)</li>
    </ul>

    <h3 class="card-title">Output gate</h3>
    <ul>
      <li>Decides what gets sent as the prediction (output) of the time step</li>
      <li>\( \Gamma_{o}^{\langle t \rangle} = \sigma(W_{o}[a^{\langle t-1 \rangle}, x^{\langle t \rangle}] + b_{o}) \)</li>
    </ul>

    <h3 class="card-title">Cell state</h3>
    <ul>
      <li>The "memory" that gets passed onto future time steps</li>
      <li>The new cell state is a combination of the previous cell state and the candidate value</li>
      <li>\( c^{\langle t \rangle} = \Gamma_{u} * \tilde{c}^{\langle t \rangle} + \Gamma_{f} c^{\langle t-1 \rangle} \)</li>
    </ul>

    <h3 class="card-title">Hidden state</h3>
    <ul>
      <li>Hidden state gets passed to the LSTM cell's next time step</li>
      <li>Determines the three gates of the next time step</li>
      <li>Hidden state is also used for the prediction</li>
      <li>\( a^{\langle t \rangle} = \Gamma_{o} * tanh(c^{\langle t \rangle}) \)</li>
    </ul>

    <h3 class="card-title">Prediction</h3>
    <ul>
      <li>\( y_{pred}^{\langle t \rangle} = \textrm{softmax}(W_{y} a^{\langle t \rangle} + b_{y}) \)</li>
    </ul>

<pre><code class="python">def lstm_cell_forward(xt, a_prev, c_prev, parameters):
    """
    Args:
    xt -- input data at timestep "t", of shape (n_x, m).
    a_prev -- hidden state at timestep "t-1", of shape (n_a, m)
    c_prev -- memory state at timestep "t-1", of shape (n_a, m)
    parameters -- python dictionary containing
                  Wf -- weight matrix of the forget gate, of shape (n_a, n_a + n_x)
                  bf -- bias of the forget gate, of shape (n_a, 1)
                  Wi -- weight matrix of the update gate, of shape (n_a, n_a + n_x)
                  bi -- bias of the update gate, of shape (n_a, 1)
                  Wc -- weight matrix of the first "tanh", of shape (n_a, n_a + n_x)
                  bc -- bias of the first "tanh", of shape (n_a, 1)
                  Wo -- weight matrix of the output gate, of shape (n_a, n_a + n_x)
                  bo -- bias of the output gate, of shape (n_a, 1)
                  Wy -- weight matrix relating the hidden-state to the output, of shape (n_y, n_a)
                  by -- bias relating the hidden-state to the output, of shape (n_y, 1)

    Returns:
    a_next -- next hidden state, of shape (n_a, m)
    c_next -- next memory state, of shape (n_a, m)
    yt_pred -- prediction at timestep "t",of shape (n_y, m)
    cache -- values needed for the backward pass (a_next, c_next, a_prev, c_prev, xt, parameters)
    """

    Wf = parameters["Wf"]  # Forget gate weight
    bf = parameters["bf"]
    Wi = parameters["Wi"]  # Update gate weight
    bi = parameters["bi"]
    Wc = parameters["Wc"]  # Candidate value weight
    bc = parameters["bc"]
    Wo = parameters["Wo"]  # Output gate weight
    bo = parameters["bo"]
    Wy = parameters["Wy"]  # Prediction weight
    by = parameters["by"]

    n_x, m = xt.shape
    n_y, n_a = Wy.shape

    concat = np.concatenate((a_prev, xt))
    concat[: n_a, :] = a_prev
    concat[n_a :, :] = xt

    ft = sigmoid(np.dot(Wf, concat) + bf)  # Forget gate
    it = sigmoid(np.dot(Wi, concat) + bi)  # Update gate
    cct = np.tanh(np.dot(Wc, concat) + bc)  # Candidate value
    c_next = np.multiply(ft, c_prev) + np.multiply(it, cct)  # Cell state
    ot = sigmoid(np.dot(Wo, concat) + bo)  # Output gate
    a_next = np.multiply(ot, np.tanh(c_next))  # Hidden state

    yt_pred = softmax(np.dot(Wy, a_next) + by)

    cache = (a_next, c_next, a_prev, c_prev, ft, it, cct, ot, xt, parameters)

    return a_next, c_next, yt_pred, cache</code></pre>

<pre><code class="python">def lstm_forward(x, a0, parameters):
    """
    Args:
    x -- input data for every time-step, of shape (n_x, m, T_x)
    a0 -- initial hidden state, of shape (n_a, m)
    parameters -- python dictionary containing
                  Wf -- weight matrix of the forget gate, of shape (n_a, n_a + n_x)
                  bf -- bias of the forget gate, of shape (n_a, 1)
                  Wi -- weight matrix of the update gate, of shape (n_a, n_a + n_x)
                  bi -- bias of the update gate, of shape (n_a, 1)
                  Wc -- weight matrix of the first "tanh", of shape (n_a, n_a + n_x)
                  bc -- bias of the first "tanh", of shape (n_a, 1)
                  Wo -- weight matrix of the output gate, of shape (n_a, n_a + n_x)
                  bo -- bias of the output gate, of shape (n_a, 1)
                  Wy -- weight matrix relating the hidden-state to the output, of shape (n_y, n_a)
                  by -- bias relating the hidden-state to the output, of shape (n_y, 1)

    Returns:
    a -- hidden states for every time-step, of shape (n_a, m, T_x)
    y -- predictions for every time-step, nof shape (n_y, m, T_x)
    c -- value of the cell state, of shape (n_a, m, T_x)
    caches -- values needed for the backward pass (list of all the caches, x)
    """

    caches = []

    Wy = parameters['Wy']
    n_x, m, T_x = x.shape
    n_y, n_a = parameters['Wy'].shape

    a = np.zeros((n_a, m, T_x))
    c = np.zeros((n_a, m, T_x))
    y = np.zeros((n_y, m, T_x))

    a_next = a0
    c_next = np.zeros(a_next.shape)

    for t in range(T_x):
        a_next, c_next, yt, cache = lstm_cell_forward(x[:,:,t], a[:,:,t], c[:,:,t], parameters)
        a[:,:,t] = a_next
        y[:,:,t] = yt
        c[:,:,t] = c_next
        caches.append(cache)

    caches = (caches, x)

    return a, y, c, caches</code></pre>

    <h3 class="card-title">Bidirectional RNN</h3>
    <ul>
      <li>Getting information from the future</li>
      <ul>
        <li>Ex. He said, "Teddy bears are on sale!"</li>
        <li>Ex. He said, "Teddy Roosevelt was a great President"</li>
      </ul>
      <li>\( y^{&lt;t&gt;} = g(W_{y}[\overrightarrow{a}^{&lt;t&gt;}, \overleftarrow{a}^{&lt;t&gt;}] + b_{y}) \)</li>
    </ul>

    <!-- <h3 class="card-title">Dropout</h3>
    <ul>
      <li>In RNN, dropout is applied to input/output of each cell unit.</li>
      <li>Dropout decreases the amount of cell computation, preventing overfitting the data.</li>
    </ul> -->
  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://www.deeplearning.ai/deep-learning-specialization">Deep Learning Specialization</a>
  </div>
</div>
<!-- Recurrent neural network END -->

</div> <!-- /.col-md-12 -->
</div> <!-- /.row -->
</div> <!-- /.container -->

<include src="/footer.html"></include>

</body>

</html>