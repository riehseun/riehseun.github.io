<!DOCTYPE html>

<html lang="en">

<head>

<!-- Metadata -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="">
<meta name="author" content="Seungmoon Rieh">
<meta name="keywords" content="">

<!-- Title and image -->
<title>Seungmoon Rieh</title>
<link href="/img/seungmoonrieh.jpg" rel="icon">

<!-- CSS -->
<link href="/css/bootstrap.min.css" rel="stylesheet">
<link href="/css/monokai-sublime.css" rel="stylesheet">
<link href="/css/site.css" rel="stylesheet">

<!-- JavaScript -->
<script src="/js/jquery.min.js"></script>
<script src="/js/bootstrap.bundle.min.js"></script>
<script src="/js/highlight.pack.js"></script>
<script src="/js/include_html.js" type="text/javascript"></script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script src="/js/site.js" type="text/javascript"></script>

</head>

<body>

<include src="/header.html"></include>

<div class="container">
<div class="row">
<div class="col-md-12">
<h1 class="my-4">Machine Learning</h1>

<!-- Recurrent neural network BEGIN -->
<div class="card mb-4" id="subject">
  <div class="card-body">
    <h2 class="card-title">Recurrent neural network</h2>
    <ul class="list-unstyled mb-0">
      <li><a href="#recurrent-neural-network-1">Recurrent neural network</a></li>
    </ul>
  </div>
</div>

<div class="card mb-4" id="recurrent-neural-network-1">
  <div class="card-body">
    <h2 class="card-title">Recurrent neural network</h2>
    <ul>
      <li>Process the input (word in a sentence) timestep by timestep.</li>
    </ul>

    <h3 class="card-title">Why not standard network</h3>
    <ul>
      <li>Inputs and outputs can be of different lengths in different examples.</li>
      <li>Does not share features learned across different positions of text.</li>
    </ul>

    <!-- <ul>
      <li>First need to extract numerical features from text data. (Ex. bag of words, N-grams, word embeddings)</li>
      <li>Process the input (word in a sentence) timestep by timestep.</li>
      <li>Backprop needs separate layer for each time step with the same weights for all layers.</li>
      <li>Different loss per timestep.</li>
      <li>Gradients from multiple timesteps are added to calculate the final gradient.</li>
    </ul> -->

    <h3 class="card-title">Forward prop</h3>
    <ul>
      <li>\( a^{<0>} = 0 \)</li>
      <li>\( a^{<1>} = g(W_{aa}a^{<0>} + W_{ax}a^{<1>} + b_{a}) \)</li>
      <li>\( y^{<1>} = g(W_{ya}a^{<1>} + b_{y}) \)</li>
      <li>\( a^{&lt;t&gt;} = g(W_{aa}a^{&lt;t-1&gt;} + W_{ax}a^{&lt;t&gt;} + b_{a}) = g(W_{a}[a^{&lt;t-1&gt;}, x^{&lt;t&gt;}] + b_{a}) \)</li>
      <li>\( y^{&lt;t&gt;} = g(W_{ya}a^{&lt;t&gt;} + b_{y}) = g(W_{y}[a^{&lt;t&gt;}] + b_{y}) \)</li>
    </ul>

    <h3 class="card-title">Loss function</h3>
    <ul>
      <li>\( L^{&lt;t&gt;}(\hat{y}^{&lt;t&gt;}, y^{&lt;t&gt;}) = -y^{&lt;t&gt;}log\hat{y}^{&lt;t&gt;} - (1-y^{&lt;t&gt;})log(1-\hat{y}^{&lt;t&gt;}) \)</li>
      <li>\( L(\hat{y},y) = \displaystyle\sum_{t=1}^{T_{y}} L^{&lt;t&gt;}(\hat{y}^{&lt;t&gt;}, y^{&lt;t&gt;}) \)</li>
    </ul>

    <h3 class="card-title">RNN types</h3>
    <ul>
      <li>One to many (Ex. music generation)</li>
      <li>Many to one (Ex. sentiment classification)</li>
      <li>Many to many (Ex. named entity recognition)</li>
      <li>Many to many (Ex. machine translation)</li>
    </ul>

    <h3 class="card-title">Language model</h3>
    <ul>
      <li>Compute \( P(sentence) = P(y^{&lt;1&gt;} \dots y^{&lt;t&gt;}) \)</li>
      <li>Training set: large english corpus.</li>
    </ul>

    <h3 class="card-title">Long short term memory</h3>
    <ul>
      <li>Addresses vanishing gradient.</li>
      <li>If exploding gradient, apply gradient clipping.</li>
    </ul>

    <h4 class="card-title">Candidate value</h4>
    <ul>
      <li>Tensor containing values between -1 to 1.</li>
      <li>Information from current time step that may be stored in current cell state.</li>
      <li>\( \tilde{c}^{&lt;t&gt;} = tanh(W_{c}[a^{&lt;t-1&gt;}, x^{&lt;t&gt;}] + b_{c}) \)</li>
    </ul>

    <h4 class="card-title">Update gate</h4>
    <ul>
      <li>Tensor containing values between 0 and 1.</li>
      <ul>
        <li>If unit in update gate is close to 1, it allows the value of the candidate to be passed onto the hidden state.</li>
        <li>If unit in update gate is close to 0, it prevents the corresponding value in the candidate from being passed onto the hidden state.</li>
      </ul>
      <li>Decide what aspects of the candidate to add to the cell state.</li>
      <li>\( \Gamma_{u} = \sigma(W_{u}[a^{&lt;t-1&gt;}, x^{&lt;t&gt;}] + b_{u}) \)</li>
    </ul>

    <h4 class="card-title">Forget gate</h4>
    <ul>
      <li>Tensor containing values between 0 and 1.</li>
      <ul>
        <li>If unit in forget gate has value close to 0, LSTM will "forget" the stored state in the previous cell state.</li>
        <li>If unit in forget gate has value close to 1, LSTM will mostly remember the corresponding value in the stored state.</li>
      </ul>
      <li>\( \Gamma_{f} = \sigma(W_{f}[a^{&lt;t-1&gt;}, x^{&lt;t&gt;}] + b_{f}) \)</li>
    </ul>

    <h4 class="card-title">Output gate</h4>
    <ul>
      <li>Decides what gets sent as the prediction (output) of the time step.</li>
      <li>\( \Gamma_{o} = \sigma(W_{o}[a^{&lt;t-1&gt;}, x^{&lt;t&gt;}] + b_{o}) \)</li>
    </ul>

    <h4 class="card-title">Cell state</h4>
    <ul>
      <li>The "memory" that gets passed onto future time steps.</li>
      <li>The new cell state is a combination of the previous cell state and the candidate value.</li>
      <li>\( c^{&lt;t&gt;} = \Gamma_{u} * \tilde{c}^{&lt;t&gt;} + \Gamma_{f} c^{&lt;t-1&gt;} \)</li>
    </ul>

    <h4 class="card-title">Hidden state</h4>
    <ul>
      <li>Hidden state gets passed to the LSTM cell's next time step.</li>
      <li>Determines the three gates of the next time step.</li>
      <li>Hidden state is also used for the prediction.</li>
      <li>\( a^{&lt;t&gt;} = \Gamma_{o} * tanh(c^{&lt;t&gt;}) \)</li>
    </ul>

    <h3 class="card-title">Bidirectional RNN</h3>
    <ul>
      <li>\( y^{&lt;t&gt;} = g(W_{y}[\overrightarrow{a}^{&lt;t&gt;}, \overleftarrow{a}^{&lt;t&gt;}] + b_{y}) \)</li>
    </ul>

    <!-- <h3 class="card-title">Dropout</h3>
    <ul>
      <li>In RNN, dropout is applied to input/output of each cell unit.</li>
      <li>Dropout decreases the amount of cell computation, preventing overfitting the data.</li>
    </ul> -->
  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://www.deeplearning.ai/deep-learning-specialization/">Deep Learning Specialization</a> | <a href="https://www.educative.io/path/become-a-deep-learning-professional">Become a Deep Learning Professional</a>
  </div>
</div>
<!-- Recurrent neural network END -->

</div> <!-- /.col-md-12 -->
</div> <!-- /.row -->
</div> <!-- /.container -->

<include src="/footer.html"></include>

</body>

</html>