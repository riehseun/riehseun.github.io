<!DOCTYPE html>

<html lang="en">

<head>

<!-- Metadata -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="">
<meta name="author" content="Seungmoon Rieh">
<meta name="keywords" content="">

<!-- Title and image -->
<title>Seungmoon Rieh</title>
<link href="/img/seungmoonrieh.jpg" rel="icon">

<!-- CSS -->
<link href="/css/bootstrap.min.css" rel="stylesheet">
<link href="/css/monokai-sublime.css" rel="stylesheet">
<link href="/css/site.css" rel="stylesheet">

<!-- JavaScript -->
<script src="/js/jquery.min.js" type="text/javascript"></script>
<script src="/js/bootstrap.bundle.min.js" type="text/javascript"></script>
<script src="/js/highlight.pack.js" type="text/javascript"></script>
<script src="/js/include_html.js" type="text/javascript"></script>
<script src="/js/mathjax/tex-chtml.js" type="text/javascript"></script>
<script src="/js/site.js" type="text/javascript"></script>

</head>

<body>

<include src="/header.html"></include>

<div class="container">
<div class="row">
<div class="col-md-12">
<h1 class="my-4">Machine Learning</h1>

<!-- Transformer BEGIN -->
<div class="card mb-4" id="subject">
  <div class="card-body">
    <h2 class="card-title">Transformer</h2>
    <ul class="list-unstyled mb-0">
      <li><a href="#transformer-">Attention</a></li>
      <li><a href="#transformer-">Transformer</a></li>
      <li><a href="#transformer-">Transformer evaluation</a></li>
      <li><a href="#transformer-">T5</a></li>
    </ul>
  </div>
</div>

<div class="card mb-4" id="transformer-">
  <div class="card-body">
    <h2 class="card-title">Attention</h2>
    <ul>
      <li>Unlike RNN where tokens are analyzed in sequence, attention relates every token to every other tokens in a sequence</li>
      <li>Attention runs dot products between word vectors and find the strongest relationship of a word with all other words</li>
      <li>\( \text{Attention}(Q,K,V) = \text{softmax} \left( \dfrac{QK^{T}}{\sqrt{d_{k}}} \right) V \)</li>
      <li>\( Q \) - matrix where each query that has dimension \( d_{k} \)</li>
      <li>\( K \) - matrix where each key that has dimension \( d_{k} \)</li>
      <li>\( V \) - matrix where each value that has dimension \( d_{v} \)</li>
      <li>For large values of \( d_{k} \), dot product becomes big and softmax function can have very small gradient, so scale by \( \frac{1}{\sqrt{d_{k}}} \)</li>
    </ul>

    <h3 class="card-title">Multi-head attention</h3>
    <ul>
      <li>Run attentions in parallel, instead of single attention with \( d_{model} \) dimensional queries, key, values</li>
      <li>Divide \( d_{model} = 512 \) dimensions, into \( 8 \) of \( d_{model} = 64 \) dimensions for all input workds/tokens</li>
      <li>\( \text{Multi-head attention}(Q,K,V) = \text{Concat}(\text{Attention}_{1} \dots \text{Attention}_{h}) \)</li>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: Attention Is All You Need, Ashish Vaswani et al
  </div>
</div>

<div class="card mb-4" id="transformer-">
  <div class="card-body">
    <h2 class="card-title">Transformer</h2>
    <ul>
      <li>Original transformer was used for machine translation</li>
      <li>Training data consisted of 4.5M English-German sentence pairs and 3.6M English-French sentence pairs</li>
      <li>Training base model took 12 hours for 100k steps on a machine with 8 NVIDIA P100 GPUs</li>
      <li>Training big model took 3.5 days for 300k steps</li>
      <li>Adam optimizer was used with varying learning rates (linear during start, then decresing going forward)</li>
    </ul>

    <img class="img-fluid" class="card-img-top" src="/machine-learning/machine-learning/transformer1.png" style="width: 500px; height: 700px" alt="Card image cap">

    <ul>
      <li>Output of layer \( l \) is becomes the input of layer \( l+1 \) until prediction is reached</li>
      <li>There is no RNN or LSTM</li>
      <li>There are 6 layers of encoder stack and 6 layers of decoder stack</li>
      <li>Multi-head attention runs \( 8 \) attentions in parallel</li>
    </ul>

    <h3 class="card-title">Encoder stack</h3>

    <img class="img-fluid" class="card-img-top" src="/machine-learning/machine-learning/transformer2.png" style="width: 300px; height: 500px" alt="Card image cap">

    <ul>
      <li>Input embedding</li>
      <ul>
        <li>Converts input tokens to vector of \( d_{\text{model}} = 512 \) dimension using learned embeddings</li>
        <ul>
          <li>Tokenizer transforms a sentence into tokens</li>
          <li>An embedding method is used to convert each word to a vector of \( 512 \) numbers</li>
        </ul>
      </ul>
      <li>Positional encoding</li>
      <ul>
        <li>Considers position of a word in a sentence</li>
        <li>Add positional encoding to input embedding to produce final encoding</li>
<pre><code class="python">def encoding(pos, positional_vector)
    for i in range(0, 512,2):
        val =  pos / (10000**((2*i)/d_model))
        positional_vector[0][i] = math.sin(val)
        positional_vector[0][i+1] = math.cos(val)
        positional_encoding[0][i] = (embedding_vector[0][i]*math.sqrt(d_model)) + positional_vector[0][i]
        positional_encoding[0][i+1] = (embedding_vector[0][i+1]*math.sqrt(d_model)) + positional_vector[0][i+1]</code></pre>
      </ul>
      <li>Multi-head attention</li>
      <ul>
        <li>Represent the input</li>
        <ul>
          <li>Assume \( 3 \) input tokens</li>
          <li>\( d_{\text{model}} = 512 \), but for simplicity, assume \( d_{\text{model}} = 4 \)</li>
<pre><code class="python">X = np.array([[1.0, 0.0, 1.0, 0.0],   # Input token 1
              [0.0, 2.0, 0.0, 2.0],   # Input token 2
              [1.0, 1.0, 1.0, 1.0]])  # Input token 3</code></pre>
        </ul>
        <li>Initialize the weight matrices</li>
        <ul>
          <li>Each input token has three weight matrices</li>
          <ul>
            <li>\( W_{Q} \) - train the queries</li>
            <li>\( W_{K} \) - train the keys</li>
            <li>\( W_{V} \) - train the values</li>
          </ul>
          <li>\( d_{k} = 64 \), but for simplicity, assume \( d_{k} = 3 \)</li>
<pre><code class="python">W_query = np.array([[1, 0, 1],
                    [1, 0, 0],
                    [0, 0, 1],
                    [0, 1, 1]])
W_key = np.array([[0, 0, 1],
                  [1, 1, 0],
                  [0, 1, 0],
                  [1, 1, 0]])
W_value = np.array([[0, 2, 0],
                    [0, 3, 0],
                    [1, 0, 3],
                    [1, 1, 0]])</code></pre>
        </ul>
        <li>Compute \( Q, K, V \)</li>
        <ul>
          <li>Assume there is one set of \( W_{Q}, W_{K}, W_{V} \) for all inputs</li>
<pre><code class="python">Q = np.matmul(X, W_query)
K = np.matmul(X, W_key)
V = np.matmul(X, W_value)</code></pre>
        </ul>
        <li>Compute attention score</li>
        <ul>
          <li>\( \text{Attention}(Q,K,V) = \text{softmax} \left( \dfrac{QK^{T}}{\sqrt{d_{k}}} \right) V \)</li>
<pre><code class="python">d_k = 1   # Square root of k_d simplified to 1 for this example
attention_scores = (Q@K.transpose()) / d_k</code></pre>
          <li>Softmax score for each input token</li>
<pre><code class="python">attention_scores[0] = softmax(attention_scores[0])
attention_scores[1] = softmax(attention_scores[1])
attention_scores[2] = softmax(attention_scores[2])</code></pre>
          <li>Attention for each input token</li>
<pre><code class="python">attention1 = attention_scores[0].reshape(-1,1)
attention1 = attention_scores[0][0] * V[0]
attention2 = attention_scores[0][1] * V[1]
attention3 = attention_scores[0][2] * V[2]</code></pre>
          <li>Sum up the result</li>
<pre><code class="python">attention_input1 = attention1 + attention2 + attention3</code></pre>
        </ul>
      </ul>
      <li>Post-layer normalization</li>
      <ul>
        <li>Add</li>
        <ul>
          <li>Processes residual connections, which transports unprocessed input \( X \) to normalization</li>
          <li>This prevents key information, such as positional encoding, from being lost on the way</li>
        </ul>
        <li>Normalization</li>
        <ul>
          <li>\( \text{Normalization}(v) = \gamma \dfrac{v-\mu}{\sigma} \beta \)</li>
          <li>\( \mu \) is the mean of \( v \) of dimension \( d \)</li>
          <ul>
            <li>\( \mu = \dfrac{1}{d} \displaystyle\sum_{k=1}^{d} v_{k} \)</li>
          </ul>
          <li>\( \sigma \) is the standard deviation of \( v \) dimension \( d \)</li>
          <ul>
            <li>\( \sigma^{2} = \dfrac{1}{d} \displaystyle\sum_{k=1}^{d} (v_{k}-\mu) \)</li>
          </ul>
          <li>\( \gamma \) is a scaling vector</li>
          <li>\( \beta \) is a bias vector</li>
        </ul>
      </ul>
      <li>Feedforward network</li>
      <ul>
        <li>Contains two layers and applies ReLU</li>
        <li>Input and output has \( d_{\text{model}} = 512 \) but inner layer has \( d_{\text{ff}} = 2048 \)</li>
        <li>\( \text{FFN}(x) = max(0, xW_{1} + b_{1})W_{2} + b_{2} \)</li>
      </ul>
    </ul>

    <h3 class="card-title">Decoder stack</h3>

    <img class="img-fluid" class="card-img-top" src="/machine-learning/machine-learning/transformer3.png" style="width: 300px; height: 700px" alt="Card image cap">

    <ul>
      <li>Masked multi-head attention</li>
      <li></li>
      <li></li>
      <li></li>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: Transformers for Natural Language Processing, Denis Rothman
  </div>
</div>

<div class="card mb-4" id="transformer-">
  <div class="card-body">
    <h2 class="card-title">Transformer evaluation</h2>

    <h3 class="card-title">Human</h3>
    <ul>
      <li>Input - perception of raw events for layer 0</li>
      <li>Output - language</li>
      <li>Transduction - trial-and-error</li>
      <ul>
        <li>Take structure we perceive and represent them with patterns</li>
      </ul>
      <li>We learn that many events are related (Ex. sunrise-light, sunset-dark, etc)</li>
      <li>New borns are find-tuned for many tasks by previous generations (Ex. we are taught that fire burns us)</li>
    </ul>

    <h3 class="card-title">Transformer</h3>
    <ul>
      <li>Perform transductions connecting all the tokens (subwords) that occur together in language sequence</li>
      <li>Build inductions from these transductions</li>
      <li>Train those inductions based on tokens to produce patterns of tokens</li>
    </ul>

    <h3 class="card-title">Attention</h3>
    <ul>
      <li>Attention sublayers can process millions of example for their inductive thinking operations</li>
      <li>Attention sublayers find patterns in sequences through transduction and induction</li>
      <li>Attention sublayers memorize these patterns using parameters that are stored with their model</li>
    </ul>

    <h3 class="card-title">SuperGLUE (GLUE - General language understanding evaluation)</h3>
    <ul>
      <li>Choice of plausible answers (COPA)</li>
      <ul>
        <li>Transformer chooses most plausible answer to a question</li>
      </ul>
      <li>BoolQ</li>
      <ul>
        <li>Yes-or-no answer task</li>
      </ul>
      <li>Commitment bank (CB)</li>
      <ul>
        <li>Transformer reads a premise, then examine a hypothesis</li>
        <li>Hypothesis confirms the premise or contradict it</li>
        <li>Transformer must label the hypothesis as neutral, entailment, or contradiction of the premise</li>
      </ul>
      <li>Multi-sentence reading comprehension (MultiRC)</li>
      <ul>
        <li>Transformer reads a text and choose from several possible choices</li>
      </ul>
      <li>Reading comprehension with commonsense reasoning dataset (ReCoRD)</li>
      <ul>
        <li>Dataset contains over 120,000 queries for more than 70,000 news articles</li>
        <li>Transformer uses common-sense to solve the problems</li>
      </ul>
      <li>Recognizing textual entailment (RTE)</li>
      <ul>
        <li>Transformer must read the premise, examine a hypothesis, and predict the label of entailment hypothesis status</li>
      </ul>
      <li>Words in context (WiC)</li>
      <ul>
        <li>Transformer analyzes two sentences and determine whether the target word has the same meaning in both sentences</li>
      </ul>
      <li>The Winograd schemae challence (WSC)</li>
      <ul>
        <li>Each sentence contains an occupation, a participant, and a pronoun</li>
        <li>Transformer determines whether the pronoun is coreferent with the occupation or the participant</li>
      </ul>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: Transformers for Natural Language Processing, Denis Rothman
  </div>
</div>

<div class="card mb-4" id="transformer-">
  <div class="card-body">
    <h2 class="card-title">T5</h2>
    <ul>
      <li>Stands for text-to-text transfer transformer</li>
      <li>One input format for every task submitted to the transformer</li>
      <li>Same model, hyperparameter, optimizer for a wide range of tasks</li>
      <ul>
        <li>Add a prefix to input sequence</li>
        <li>Ex. translate English to German + [sequence] for machine translation problem</li>
        <li>Ex. cola sentence + [sequence] for corpus of linguistic acceptability</li>
        <li>Ex. stsb sentence + [sequence] for semantic textual similarity benchmarks</li>
        <li>Ex. summarize + [sequence] for text summarization problem</li>
      </ul>
      <li>Same architecture as original transformer except</li>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: Transformers for Natural Language Processing, Denis Rothman
  </div>
</div>
<!-- Transformer END -->

</div> <!-- /.col-md-12 -->
</div> <!-- /.row -->
</div> <!-- /.container -->

<include src="/footer.html"></include>

</body>

</html>