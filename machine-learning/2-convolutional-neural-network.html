<!DOCTYPE html>

<html lang="en">

<head>

<!-- Metadata -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="">
<meta name="author" content="Seungmoon Rieh">
<meta name="keywords" content="">

<!-- Title and image -->
<title>Seungmoon Rieh</title>
<link href="/img/seungmoonrieh.jpg" rel="icon">

<!-- CSS -->
<link href="/css/bootstrap.min.css" rel="stylesheet">
<link href="/css/monokai-sublime.css" rel="stylesheet">
<link href="/css/site.css" rel="stylesheet">

<!-- JavaScript -->
<script src="/js/jquery.min.js" type="text/javascript"></script>
<script src="/js/bootstrap.bundle.min.js" type="text/javascript"></script>
<script src="/js/highlight.pack.js" type="text/javascript"></script>
<script src="/js/include_html.js" type="text/javascript"></script>
<script src="/js/mathjax/tex-chtml.js" type="text/javascript"></script>
<script src="/js/site.js" type="text/javascript"></script>

</head>

<body>

<include src="/header.html"></include>

<div class="container">
<div class="row">
<div class="col-md-12">
<h1 class="my-4">Machine Learning</h1>

<!-- Convolutional neural network BEGIN -->
<div class="card mb-4" id="subject">
  <div class="card-body">
    <h2 class="card-title">Convolutional neural network</h2>
    <ul class="list-unstyled mb-0">
      <li><a href="#convolutional-neural-network-">Convolutional neural network</a></li>
      <li><a href="#convolutional-neural-network-">LeNet-5</a></li>
      <li><a href="#convolutional-neural-network-">AlexNet</a></li>
      <li><a href="#convolutional-neural-network-">VGG-16</a></li>
    </ul>
  </div>
</div>

<div class="card mb-4" id="convolutional-neural-network-">
  <div class="card-body">
    <h2 class="card-title">Convolutional neural network</h2>
    <ul>
      <li>Why convolution?</li>
      <ul>
        <li>Images have huge number of pixels. If pixels are used as input, neural network will have huge number of parameters where overfitting is unavoidable</li>
      </ul>
      <li>Convolutional layer</li>
      <ul>
        <li>In decision making, information needed is often spatially close</li>
        <li>We only need to take weighted sum of nearby inputs</li>
      </ul>
      <li>Parameter sharing</li>
      <ul>
        <li>Filter (used as feature detector) used in one part of image could be useful in other parts of the image</li>
      </ul>
      <li>Sparse connection</li>
      <ul>
        <li>Each output pixel value depends on only small part of input pixel values, making it less prone to overfitting</li>
      </ul>
    </ul>

    <h3 class="card-title">Edge detection</h3>
    <ul>
      <li>Use filter (Also called kernal)</li>
    </ul>

    <h4 class="card-title">Vertical edge detection</h4>
    <ul>
      <li>\( (n-f+1) \times (n-f+1) \)</li>
      <ul>
        <li>Ex. \( 6 \times 6 \) image * \( 3 \times 3 \) filter = \( 4 \times 4 \) image</li>
        <li>* is convolution where each element of image and filter are multipled and products are added up</li>
        <li>\( f \) is almost always odd number</li>
      </ul>
      <li>\( \begin{bmatrix}
              1 & 0 & 1 \\
              0 & 1 & 0 \\
              1 & 0 & 1
          \end{bmatrix} \)</li>
      <li>\( \begin{bmatrix}
              1 & 0 & -1 \\
              2 & 0 & -2 \\
              1 & 0 & -1
          \end{bmatrix} \) (Sobel filter)</li>
      <li>\( \begin{bmatrix}
              3 & 0 & -3 \\
              10 & 0 & -10 \\
              3 & 0 & -3
          \end{bmatrix} \) (Schors filter)</li>
    </ul>

    <h4 class="card-title">Horizontal edge detection</h4>
    <ul>
      <li>\( \begin{bmatrix}
              1 & 1 & 1 \\
              0 & 0 & 0 \\
              -1 & -1 & -1
          \end{bmatrix} \)</li>
    </ul>

    <h4 class="card-title">You can learn filter values</h4>
    <ul>
      <li>\( \begin{bmatrix}
              w_{1} & w_{2} & w_{3} \\
              w_{4} & w_{5} & w_{6} \\
              w_{7} & w_{8} & w_{9}
          \end{bmatrix} \)</li>
    </ul>

    <h3 class="card-title">Padding</h3>
    <ul>
      <li>Avoids shrinking output and throwing away information from edges</li>
      <li>\( (n+2p-f+1) \times (n+2p-f+1) \)</li>
      <ul>
        <li>Ex. given \( p = 1, (6+2) \times (6+2) \) image * \( 3 \times 3 \) filter = \( 6 \times 6 \) image</li>
      </ul>
      <li><strong>valid</strong> - no padding</li>
      <li><strong>same</strong> - pad so that output size is the same as the input size</li>
      <ul>
        <li>\( p = \dfrac{f-1}{2} \)</li>
      </ul>
    </ul>

<pre><code class="python">def zero_pad(X, pad):

    """
    Args:
    X -- batch of m images (m, n_H, n_W, n_C)
    pad -- integer

    Returns:
    X_pad -- (m, n_H + 2*pad, n_W + 2*pad, n_C)
    """

    X_pad = np.pad(X, ((0,0), (pad, pad), (pad, pad), (0,0)), mode='constant', constant_values = (0,0))

    return X_pad</code></pre>

    <h3 class="card-title">Stride</h3>
    <ul>
      <li>If \( s=2 \), when applying a filter, jump over \( 2 \) steps rather than \( 1 \) step</li>
      <li>In general, \( (\dfrac{n+2p-f}{s}+1) \times (\dfrac{n+2p-f}{s}+1) \)</li>
    </ul>

    <h3 class="card-title">Convolution over volumn</h3>
    <ul>
      <li>Convolution over RGB image, not grey-scale image</li>
      <li>Volume consists of height, width, number of channels</li>
      <li>Number of channels must match between input and filter</li>
      <li>\( 6 \times 6 \times 3 \) image  * \( 3 \times 3 \times 3 \) filter = \( 4 \times 4 \)</li>
      <ul>
        <li>\( 3 * 3 * 3 = 27 \) numbers are summed up for each pixel in the output</li>
      </ul>
      <li>\( 6 \times 6 \times 3 \) image  * two different \( 3 \times 3 \times 3 \) filters where one is vertical edge detector and the other is horizontal edge detector = \( 4 \times 4 \times 2 \)</li>
      <li>In general, \( (n-f+1) \times (n-f+1) \times n_{f} \) where \( n_{f} \) is the number of filters</li>
    </ul>

    <h4 class="card-title">1st CNN layer</h4>
    <ul>
      <li>\( a^{[0]} = X \) is the input image</li>
      <li>\( W^{[1]} \) is the filters</li>
      <li>\( W^{[1]}a^{[0]}  \) is the output images</li>
      <li>For example, two different \( 3 \times 3 \times 3 \) filters have \( 28 * 2 = 56 \) parameters</li>
    </ul>

    <h4 class="card-title">In layer \( l \)</h4>
    <ul>
      <li>\( f^{[l]} \) is filter size</li>
      <li>\( p^{[l]} \) is padding</li>
      <li>\( s^{[l]} \) is stride</li>
      <li>\( n^{[l]} = \frac{n^{[l-1]}+2p^{[l]}-f^{[l]}}{2}+1 \) is stride</li>
      <li>\( n_{H}^{[l-1]} \times n_{W}^{[l-1]} \times n_{c}^{[l-1]} \) is input image dimension</li>
      <li>\( f^{[l]} \times f^{[l]} \times n_{c}^{[l-1]} \) is filter dimension</li>
      <li>\( n_{H}^{[l]} \times n_{W}^{[l]} \times n_{f}^{[l]} \) is output image dimension</li>
      <li>\( m \times n_{H}^{[l]} \times n_{W}^{[l]} \times n_{f}^{[l]} \) is the activation dimension</li>
      <li>\( f^{[l]} \times f^{[l]} \times n_{c}^{[l-1]} \times n_{f}^{[l]} \) is the weight dimension</li>
      <li>\( n_{f}^{[l]} \) is the bias dimension</li>
      <li>Note that \( n_{f}^{[l]} = n_{c}^{[l]} \)</li>
    </ul>

    <h3 class="card-title">Pooling layers</h3>
    <ul>
      <li>Learn low-level features like lines</li>
      <li><strong>max pooling</strong> - take max number in each region</li>
      <li>No parameters to learn</li>
      <li>\( 2 \) by \( 2 \) with stride \( 2 \) is commonly used</li>
      <li>Output image dimension is \( (\dfrac{n_{H}-f}{s}+1) \times (\dfrac{n_{W}-f}{s}+1) \times n_{c} \)</li>
    </ul>

    <h3 class="card-title">Forward prop</h3>

<pre><code class="python">def conv_single_step(a_slice_prev, W, b):
    """
    Args:
    a_slice_prev -- (f, f, n_C_prev)
    W -- (f, f, n_C_prev)
    b -- (1, 1, 1)

    Returns:
    Z -- a scalar value
    """

    s = np.multiply(a_slice_prev, W)
    Z = np.sum(s)
    Z = Z + float(b)

    return Z</code></pre>

<pre><code class="python">def conv_forward(A_prev, W, b, hparameters):
    """
    Args:
    A_prev -- (m, n_H_prev, n_W_prev, n_C_prev)
    W -- (f, f, n_C_prev, n_C)
    b -- (1, 1, 1, n_C)
    hparameters -- contains "stride" and "pad"

    Returns:
    Z -- (m, n_H, n_W, n_C)
    cache -- needed for conv_backward()
    """

    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape
    (f, f, n_C_prev, n_C) = W.shape

    stride = hparameters["stride"]
    pad = hparameters["pad"]

    n_H = int((n_H_prev - f + 2*pad) / stride) + 1
    n_W = int((n_W_prev - f + 2*pad) / stride) + 1

    Z = np.zeros((m, n_H, n_W, n_C))
    A_prev_pad = zero_pad(A_prev, pad)

    for i in range(m):
        a_prev_pad = A_prev_pad[i, :, :, :]  # Select ith training example's padded activation

        for h in range(n_H):
            vert_start = stride * h
            vert_end = stride * h + f

            for w in range(n_W):
                horiz_start = stride * w
                horiz_end = stride * w + f

                for c in range(n_C):
                    a_slice_prev = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]
                    weights = W[:, :, :, c]
                    biases = b[:, :, :, c]
                    Z[i, h, w, c] = conv_single_step(a_slice_prev, weights, biases)

    assert(Z.shape == (m, n_H, n_W, n_C))
    cache = (A_prev, W, b, hparameters)

    return Z, cache</code></pre>

<pre><code class="python">def pool_forward(A_prev, hparameters, mode = "max"):
    """
    Args:
    A_prev -- (m, n_H_prev, n_W_prev, n_C_prev)
    hparameters -- contains "f" and "stride"
    mode -- "max" or "average"

    Returns:
    A -- (m, n_H, n_W, n_C)
    cache -- needed for pool_backward()
    """

    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape

    f = hparameters["f"]
    stride = hparameters["stride"]

    n_H = int(1 + (n_H_prev - f) / stride)
    n_W = int(1 + (n_W_prev - f) / stride)
    n_C = n_C_prev

    A = np.zeros((m, n_H, n_W, n_C))

    for i in range(m):
        for h in range(n_H):
            vert_start = stride * h
            vert_end = stride * h + f

            for w in range(n_W):
                horiz_start = stride*w
                horiz_end = stride*w + f

                for c in range (n_C):
                    a_prev_slice = A_prev[i, vert_start:vert_end, horiz_start:horiz_end, c]

                    if mode == "max":
                        A[i, h, w, c] = np.max(a_prev_slice)

                    elif mode == "average":
                        A[i, h, w, c] = np.mean(a_prev_slice)

    cache = (A_prev, hparameters)
    assert(A.shape == (m, n_H, n_W, n_C))

    return A, cache</code></pre>

    <h3 class="card-title">Backward prop</h3>
    <ul>
      <li>\( dA += \displaystyle\sum_{h=0}^{n_H} \displaystyle\sum_{w=0}^{n_W} W_c \times dZ_{hw} \)</li>
      <li>\( dW_{c} += \displaystyle\sum_{h=0}^{n_H} \displaystyle\sum_{w=0}^{n_W} a_{slice} \times dZ_{hw} \)</li>
      <li>\( db = \displaystyle\sum_{h} \displaystyle\sum_{w} dZ_{hw} \)</li>
    </ul>

<pre><code class="python">def conv_backward(dZ, cache):
    """
    Args:
    dZ -- (m, n_H, n_W, n_C)
    cache -- from conv_forward()

    Returns:
    dA_prev -- (m, n_H_prev, n_W_prev, n_C_prev)
    dW -- (f, f, n_C_prev, n_C)
    db -- (1, 1, 1, n_C)
    """

    (A_prev, W, b, hparameters) = cache
    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape
    (f, f, n_C_prev, n_C) = W.shape

    stride = hparameters["stride"]
    pad = hparameters["pad"]

    (m, n_H, n_W, n_C) = dZ.shape

    dA_prev = np.zeros((m, n_H_prev, n_W_prev, n_C_prev))
    dW = np.zeros((f, f, n_C_prev, n_C))
    db = np.zeros((1, 1, 1, n_C))

    A_prev_pad = zero_pad(A_prev, pad)
    dA_prev_pad = zero_pad(dA_prev, pad)

    for i in range(m):
        da_prev_pad = dA_prev_pad[i, :, :, :]

        for h in range(n_H):
            for w in range(n_W):
                for c in range(n_C):

                    vert_start = stride * h
                    vert_end = stride * h + f
                    horiz_start = stride * w
                    horiz_end = stride * w + f

                    a_slice = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]

                    da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += W[:, :, :, c] * dZ[i, h, w, c]
                    dW[:, :, :, c] += a_slice * dZ[i, h, w, c]
                    db[:, :, :, c] += dZ[i, h, w, c]

        dA_prev[i, :, :, :] = da_prev_pad[pad:-pad, pad:-pad, :]

    assert(dA_prev.shape == (m, n_H_prev, n_W_prev, n_C_prev))

    return dA_prev, dW, db</code></pre>

<pre><code class="python">def create_mask_from_window(x):
    """
    Args:
    x -- (f, f)

    Returns:
    mask -- contains a True at the position corresponding to the max entry of x.
    """

    mask = x == np.max(x)

    return mask</code></pre>

<pre><code class="python">def distribute_value(dz, shape):
    """
    Args:
    dz -- input scalar
    shape -- (n_H, n_W)

    Returns:
    a -- (n_H, n_W)
    """

    (n_H, n_W) = shape
    average = dz / (n_W*n_H)
    a = np.full((n_W, n_H), average)

    return a</code></pre>

<pre><code class="python">def pool_backward(dA, cache, mode = "max"):
    """
    Args:
    dA -- same shape as A
    cache -- from pool_forward()
    mode -- "max" or "average"

    Returns:
    dA_prev -- same shape as A_prev
    """

    (A_prev, hparameters) = cache

    stride = hparameters["stride"]
    f = hparameters["f"]

    m, n_H_prev, n_W_prev, n_C_prev = A_prev.shape
    m, n_H, n_W, n_C = dA.shape

    dA_prev = np.zeros(A_prev.shape)

    for i in range(m):
        a_prev = A_prev[i,:,:,:]

        for h in range(n_H):
            for w in range(n_W):
                for c in range(n_C):

                    vert_start = stride * h
                    vert_end = vert_start + f
                    horiz_start = stride * w
                    horiz_end = horiz_start + f

                    if mode == "max":
                        a_prev_slice = a_prev[vert_start:vert_end, horiz_start:horiz_end, c]
                        mask = create_mask_from_window(a_prev_slice)
                        dA_prev[i, vert_start: vert_end, horiz_start: horiz_end, c] += np.multiply(mask, dA[i, h, w, c])

                    elif mode == "average":
                        da = dA[i, h, w, c]
                        shape = (f,f)
                        dA_prev[i, vert_start: vert_end, horiz_start: horiz_end, c] +=  distribute_value(da, shape)

    assert(dA_prev.shape == A_prev.shape)

    return dA_prev</code></pre>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://www.deeplearning.ai/deep-learning-specialization/">Deep Learning Specialization</a> | <a href="https://www.educative.io/path/become-a-deep-learning-professional">Become a Deep Learning Professional</a>
  </div>
</div>

<div class="card mb-4" id="convolutional-neural-network-">
  <div class="card-body">
    <h2 class="card-title">LeNet-5</h2>
    <ul>
      <li>About 60K parameters</li>
      <li>Width and height go down but number of channels goes up</li>
    </ul>

    <table>
      <tr>
        <th></th>
        <th>Activation shape</th>
        <th>Activation size</th>
        <th>Number of parameters</th>
      </tr>
      <tr>
        <th>Input</th>
        <td>(32,32,3)</td>
        <td>3072</td>
        <td>0</td>
      </tr>
      <tr>
        <th>CONV1(f=5,s=1)</th>
        <td>(28,28,6)</td>
        <td>4704</td>
        <td>608</td>
      </tr>
      <tr>
        <th>POOL1(f=2,s=2)</th>
        <td>(14,14,6)</td>
        <td>1176</td>
        <td>0</td>
      </tr>
      <tr>
        <th>CONV2(f=5,s=1)</th>
        <td>(10,10,6)</td>
        <td>1600</td>
        <td>3216</td>
      </tr>
      <tr>
        <th>POOL2(f=2,s=2)</th>
        <td>(5,5,16)</td>
        <td>400</td>
        <td>0</td>
      </tr>
      <tr>
        <th>FC3</th>
        <td>(120,1)</td>
        <td>120</td>
        <td>48120</td>
      </tr>
      <tr>
        <th>FC4</th>
        <td>(84,1)</td>
        <td>84</td>
        <td>10164</td>
      </tr>
      <tr>
        <th>Softmax</th>
        <td>(10,1)</td>
        <td>10</td>
        <td>850</td>
      </tr>
    </table>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://www.deeplearning.ai/deep-learning-specialization/">Deep Learning Specialization</a>
  </div>
</div>

<div class="card mb-4" id="convolutional-neural-network-">
  <div class="card-body">
    <h2 class="card-title">AlexNet</h2>
    <ul>
      <li>About 60M parameters</li>
    </ul>

    <table>
      <tr>
        <th></th>
        <th>Activation shape</th>
      </tr>
      <tr>
        <th>Input</th>
        <td>(227,227,3)</td>
      </tr>
      <tr>
        <th>CONV(f=11,s=4)</th>
        <td>(55,55,96)</td>
      </tr>
      <tr>
        <th>POOL(f=3,s=2)</th>
        <td>(27,27,96)</td>
      </tr>
      <tr>
        <th>CONV(f=5,same)</th>
        <td>(27,27,256)</td>
      </tr>
      <tr>
        <th>POOL(f=3,s=2)</th>
        <td>(13,13,256)</td>
      </tr>
      <tr>
        <th>CONV(f=3,same)</th>
        <td>(13,13,384)</td>
      </tr>
      <tr>
        <th>CONV(f=3,same)</th>
        <td>(13,13,384)</td>
      </tr>
      <tr>
        <th>CONV(f=3,same)</th>
        <td>(13,13,256)</td>
      </tr>
      <tr>
        <th>POOL(f=3,s=2)</th>
        <td>(6,6,256)</td>
      </tr>
      <tr>
        <th>FC</th>
        <td>(4096,1)</td>
      </tr>
      <tr>
        <th>FC</th>
        <td>(4096,1)</td>
      </tr>
      <tr>
        <th>Softmax</th>
        <td>(1000,1)</td>
      </tr>
    </table>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://www.deeplearning.ai/deep-learning-specialization/">Deep Learning Specialization</a>
  </div>
</div>

<div class="card mb-4" id="convolutional-neural-network-">
  <div class="card-body">
    <h2 class="card-title">VGG-16</h2>
    <ul>
      <li>All CONVs are f=3,s=1,same</li>
      <li>All POOLs are f=2,s=2</li>
    </ul>

    <table>
      <tr>
        <th></th>
        <th>Activation shape</th>
      </tr>
      <tr>
        <th>Input</th>
        <td>(224,224,3)</td>
      </tr>
      <tr>
        <th>CONV(f=3, s=1, same)</th>
        <td>(224,224,64)</td>
      </tr>
      <tr>
        <th>CONV(f=3, s=1, same)</th>
        <td>(224,224,64)</td>
      </tr>
      <tr>
        <th>POOL(f=2, s=2)</th>
        <td>(112,112,64)</td>
      </tr>
      <tr>
        <th>CONV(f=3, s=1, same)</th>
        <td>(112,112,128)</td>
      </tr>
      <tr>
        <th>CONV(f=3, s=1, same)</th>
        <td>(112,112,128)</td>
      </tr>
      <tr>
        <th>POOL(f=2, s=2)</th>
        <td>(56,56,128)</td>
      </tr>
      <tr>
        <th>CONV(f=3, s=1, same)</th>
        <td>(56,56,256)</td>
      </tr>
      <tr>
        <th>CONV(f=3, s=1, same)</th>
        <td>(56,56,256)</td>
      </tr>
      <tr>
        <th>CONV(f=3, s=1, same)</th>
        <td>(56,56,256)</td>
      </tr>
      <tr>
        <th>POOL(f=2, s=2)</th>
        <td>(28,28,256)</td>
      </tr>
      <tr>
        <th>CONV(f=3, s=1, same)</th>
        <td>(28,28,512)</td>
      </tr>
      <tr>
        <th>CONV(f=3, s=1, same)</th>
        <td>(28,28,512)</td>
      </tr>
      <tr>
        <th>CONV(f=3, s=1, same)</th>
        <td>(28,28,512)</td>
      </tr>
      <tr>
        <th>POOL(f=2, s=2)</th>
        <td>(14,14,512)</td>
      </tr>
      <tr>
        <th>CONV(f=3, s=1, same)</th>
        <td>(14,14,512)</td>
      </tr>
      <tr>
        <th>CONV(f=3, s=1, same)</th>
        <td>(14,14,512)</td>
      </tr>
      <tr>
        <th>CONV(f=3, s=1, same)</th>
        <td>(14,14,512)</td>
      </tr>
      <tr>
        <th>POOL(f=2, s=2)</th>
        <td>(7,7,512)</td>
      </tr>
      <tr>
        <th>FC</th>
        <td>(4096,1)</td>
      </tr>
      <tr>
        <th>FC</th>
        <td>(4096,1)</td>
      </tr>
      <tr>
        <th>Softmax</th>
        <td>(1000,1)</td>
      </tr>
    </table>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://www.deeplearning.ai/deep-learning-specialization/">Deep Learning Specialization</a>
  </div>
</div>
<!-- Convolutional neural network END -->

</div> <!-- /.col-md-12 -->
</div> <!-- /.row -->
</div> <!-- /.container -->

<include src="/footer.html"></include>

</body>

</html>