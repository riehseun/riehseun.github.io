<!DOCTYPE html>

<html lang="en">

<head>

<!-- Metadata -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="">
<meta name="author" content="Seungmoon Rieh">
<meta name="keywords" content="">

<!-- Title and image -->
<title>Seungmoon Rieh</title>
<link href="/img/seungmoonrieh.jpg" rel="icon">

<!-- CSS -->
<link href="/css/bootstrap.min.css" rel="stylesheet">
<link href="/css/monokai-sublime.css" rel="stylesheet">
<link href="/css/site.css" rel="stylesheet">

<!-- JavaScript -->
<script src="/js/jquery.min.js" type="text/javascript"></script>
<script src="/js/bootstrap.bundle.min.js" type="text/javascript"></script>
<script src="/js/highlight.pack.js" type="text/javascript"></script>
<script src="/js/include_html.js" type="text/javascript"></script>
<script src="/js/mathjax/tex-chtml.js" type="text/javascript"></script>
<script src="/js/site.js" type="text/javascript"></script>

</head>

<body>

<include src="/header.html"></include>

<div class="container">
<div class="row">
<div class="col-md-12">
<h1 class="my-4">Machine Learning</h1>

<!-- Convolutional neural network BEGIN -->
<div class="card mb-4" id="subject">
  <div class="card-body">
    <h2 class="card-title">Convolutional neural network</h2>
    <ul class="list-unstyled mb-0">
      <li><a href="#convolutional-neural-network-">Convolutional neural network</a></li>
      <li><a href="#convolutional-neural-network-">Inspired by LeNet-5</a></li>
      <li><a href="#convolutional-neural-network-">AlexNet</a></li>
      <li><a href="#convolutional-neural-network-">VGG-16</a></li>
    </ul>
  </div>
</div>

<div class="card mb-4" id="convolutional-neural-network-">
  <div class="card-body">
    <h2 class="card-title">Convolutional neural network</h2>
    <ul>
      <li>Why convolution? Images have huge number of pixels. If pixels are used as input, neural network will have huge number of parameters where overfitting is unavoidable</li>
      <li>Parameter sharing - filter (used as feature detector) used in one part of image could be useful in other parts of the image</li>
      <li>Sparse connection - each output pixel value depends on only small part of input pixel values, making it less prone to overfitting</li>
      <!-- <li>Nearby pixels share similar characteristics. (Inductive bias)</li>
      <li>Focus on local structure instead of fully connected layers that take linear combinations of the input.</li>
      <li>Slide window (kernal) throughout the input image.</li>
      <li>Has ability to extract and learn complex features.</li>
      <li>Convolutional layer
        <ul>
          <li>In decision making, information needed is often spatially close.</li>
          <li>We only need to take weighted sum of nearby inputs.</li>
        </ul>
      </li>
      <li>Ex. Inception v3, Xception, DenseNet, AlexNet, VGG16, ResNet</li> -->
    </ul>

    <!-- h3 class="card-title">Batch normalization</h3>
    <ul>
      <li>Normalizes mean and standard deviation for each individual feature map, where feature map is the output after applying kernal.</li>
      <li>Tackles vanishing gradient problem.</li>
    </ul>

    <h3 class="card-title">Dropout</h3>
    <ul>
      <li>Tackles overfitting.</li>
    </ul>

    <h3 class="card-title">Skip connections</h3>
    <ul>
      <li>Tackles vanishing gradient problem.</li>
    </ul> -->

    <h3 class="card-title">Edge detection</h3>
    <ul>
      <li>Use filter (Also called kernal)</li>
    </ul>

    <h4 class="card-title">Vertical edge detection</h4>
    <ul>
      <li>Ex. \( 6 \times 6 \) image * \( 3 \times 3 \) filter = \( 4 \times 4 \) image</li>
      <li>In general, \( (n-f+1) \times (n-f+1) \)</li>
      <li>\( f \) is almost always odd number</li>
      <li>* is convolution where each element of image and filter are multipled and products are added up</li>
      <li>\( \begin{bmatrix}
              1 & 0 & -1 \\
              1 & 0 & -1 \\
              1 & 0 & -1
          \end{bmatrix} \)</li>
    </ul>

    <h4 class="card-title">Horizontal edge detection</h4>
    <ul>
      <li>\( \begin{bmatrix}
              1 & 1 & 1 \\
              0 & 0 & 0 \\
              -1 & -1 & -1
          \end{bmatrix} \)</li>
    </ul>

    <h4 class="card-title">You can learn filter values</h4>
    <ul>
      <li>\( \begin{bmatrix}
              w_{1} & w_{2} & w_{3} \\
              w_{4} & w_{5} & w_{6} \\
              w_{7} & w_{8} & w_{9}
          \end{bmatrix} \)</li>
    </ul>

    <h3 class="card-title">Padding</h3>
    <ul>
      <li>Avoids shrinking output and throwing away information from edges</li>
      <li>Ex. with \( p = 1, (6+2) \times (6+2) \) image * \( 3 \times 3 \) filter = \( 6 \times 6 \) image</li>
      <li>In general, \( (n+2p-f+1) \times (n+2p-f+1) \)</li>
      <li><strong>valid</strong> - no padding</li>
      <li><strong>same</strong> - pad so that output size is the same as the input size</li>
      <ul>
        <li>\( p = \dfrac{f-1}{2} \)</li>
      </ul>
    </ul>

<pre><code class="python">def zero_pad(X, pad):

    """
    Args:
    X -- batch of m images (m, n_H, n_W, n_C)
    pad -- integer

    Returns:
    X_pad -- (m, n_H + 2*pad, n_W + 2*pad, n_C)
    """

    X_pad = np.pad(X, ((0,0), (pad, pad), (pad, pad), (0,0)), mode='constant', constant_values = (0,0))

    return X_pad</code></pre>

    <h3 class="card-title">Stride</h3>
    <ul>
      <li>If \( s=2 \), when applying a filter, jump over \( 2 \) steps rather than \( 1 \) step</li>
      <li>In general, \( (\dfrac{n+2p-f}{s}+1) \times (\dfrac{n+2p-f}{s}+1) \)</li>
    </ul>

    <h3 class="card-title">Convolution over volumn</h3>
    <ul>
      <li>Convolution over RGB image, not grey-scale image</li>
      <li>Volume consists of height, width, number of channels</li>
      <li>Number of channels must match between input and filter</li>
      <li>\( 6 \times 6 \times 3 \) image  * \( 3 \times 3 \times 3 \) filter = \( 4 \times 4 \)</li>
      <ul>
        <li>\( 3 * 3 * 3 = 27 \) numbers are summed up for each pixel in the output</li>
      </ul>
      <li>\( 6 \times 6 \times 3 \) image  * two different \( 3 \times 3 \times 3 \) filters where one is vertical edge detector and the other is horizontal edge detector = \( 4 \times 4 \times 2 \)</li>
      <li>In general, \( (n-f+1) \times (n-f+1) \times n_{f} \) where \( n_{f} \) is the number of filters</li>
    </ul>

    <h4 class="card-title">1st CNN layer</h4>
    <ul>
      <li>\( a^{[0]} = X \) is the input image</li>
      <li>\( W^{[1]} \) is the filters</li>
      <li>\( W^{[1]}a^{[0]}  \) is the output images</li>
      <li>For example, two different \( 3 \times 3 \times 3 \) filters have \( 28 * 2 = 56 \) parameters</li>
    </ul>

    <h4 class="card-title">In layer \( l \)</h4>
    <ul>
      <li>\( f^{[l]} \) is filter size</li>
      <li>\( p^{[l]} \) is padding</li>
      <li>\( s^{[l]} \) is stride</li>
      <li>\( n^{[l]} = \frac{n^{[l-1]}+2p^{[l]}-f^{[l]}}{2}+1 \) is stride</li>
      <li>\( n_{H}^{[l-1]} \times n_{W}^{[l-1]} \times n_{c}^{[l-1]} \) is input image dimension</li>
      <li>\( f^{[l]} \times f^{[l]} \times n_{c}^{[l-1]} \) is filter dimension</li>
      <li>\( n_{H}^{[l]} \times n_{W}^{[l]} \times n_{f}^{[l]} \) is output image dimension</li>
      <li>\( m \times n_{H}^{[l]} \times n_{W}^{[l]} \times n_{f}^{[l]} \) is the activation dimension</li>
      <li>\( f^{[l]} \times f^{[l]} \times n_{c}^{[l-1]} \times n_{f}^{[l]} \) is the weight dimension</li>
      <li>\( n_{f}^{[l]} \) is the bias dimension</li>
      <li>Note that \( n_{f}^{[l]} = n_{c}^{[l]} \)</li>
    </ul>

    <h3 class="card-title">Pooling layers</h3>
    <ul>
      <li>Learn low-level features like lines</li>
      <li><strong>max pooling</strong> - take max number in each region</li>
      <li>No parameters to learn</li>
      <li>\( 2 \) by \( 2 \) with stride \( 2 \) is commonly used</li>
      <li>Output image dimension is \( (\dfrac{n_{H}-f}{s}+1) \times (\dfrac{n_{W}-f}{s}+1) \times n_{c} \)</li>
    </ul>

    <h3 class="card-title">Forward prop</h3>

<pre><code class="python">def conv_single_step(a_slice_prev, W, b):
    """
    Args:
    a_slice_prev -- (f, f, n_C_prev)
    W -- (f, f, n_C_prev)
    b -- (1, 1, 1)

    Returns:
    Z -- a scalar value
    """

    s = np.multiply(a_slice_prev, W)
    Z = np.sum(s)
    Z = Z + float(b)

    return Z</code></pre>

<pre><code class="python">def conv_forward(A_prev, W, b, hparameters):
    """
    Args:
    A_prev -- (m, n_H_prev, n_W_prev, n_C_prev)
    W -- (f, f, n_C_prev, n_C)
    b -- (1, 1, 1, n_C)
    hparameters -- contains "stride" and "pad"

    Returns:
    Z -- (m, n_H, n_W, n_C)
    cache -- needed for conv_backward()
    """

    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape
    (f, f, n_C_prev, n_C) = W.shape

    stride = hparameters["stride"]
    pad = hparameters["pad"]

    n_H = int((n_H_prev - f + 2*pad) / stride) + 1
    n_W = int((n_W_prev - f + 2*pad) / stride) + 1

    Z = np.zeros((m, n_H, n_W, n_C))
    A_prev_pad = zero_pad(A_prev, pad)

    for i in range(m):
        a_prev_pad = A_prev_pad[i, :, :, :]  # Select ith training example's padded activation

        for h in range(n_H):
            vert_start = stride * h
            vert_end = stride * h + f

            for w in range(n_W):
                horiz_start = stride * w
                horiz_end = stride * w + f

                for c in range(n_C):
                    a_slice_prev = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]
                    weights = W[:, :, :, c]
                    biases = b[:, :, :, c]
                    Z[i, h, w, c] = conv_single_step(a_slice_prev, weights, biases)

    assert(Z.shape == (m, n_H, n_W, n_C))
    cache = (A_prev, W, b, hparameters)

    return Z, cache</code></pre>

<pre><code class="python">def pool_forward(A_prev, hparameters, mode = "max"):
    """
    Args:
    A_prev -- (m, n_H_prev, n_W_prev, n_C_prev)
    hparameters -- contains "f" and "stride"
    mode -- "max" or "average"

    Returns:
    A -- (m, n_H, n_W, n_C)
    cache -- needed for pool_backward()
    """

    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape

    f = hparameters["f"]
    stride = hparameters["stride"]

    n_H = int(1 + (n_H_prev - f) / stride)
    n_W = int(1 + (n_W_prev - f) / stride)
    n_C = n_C_prev

    A = np.zeros((m, n_H, n_W, n_C))

    for i in range(m):
        for h in range(n_H):
            vert_start = stride * h
            vert_end = stride * h + f

            for w in range(n_W):
                horiz_start = stride*w
                horiz_end = stride*w + f

                for c in range (n_C):
                    a_prev_slice = A_prev[i, vert_start:vert_end, horiz_start:horiz_end, c]

                    if mode == "max":
                        A[i, h, w, c] = np.max(a_prev_slice)

                    elif mode == "average":
                        A[i, h, w, c] = np.mean(a_prev_slice)

    cache = (A_prev, hparameters)
    assert(A.shape == (m, n_H, n_W, n_C))

    return A, cache</code></pre>

    <h3 class="card-title">Backward prop</h3>

<pre><code class="python"></code></pre>

<pre><code class="python"></code></pre>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://www.deeplearning.ai/deep-learning-specialization/">Deep Learning Specialization</a> | <a href="https://www.educative.io/path/become-a-deep-learning-professional">Become a Deep Learning Professional</a>
  </div>
</div>

<div class="card mb-4" id="convolutional-neural-network-">
  <div class="card-body">
    <h2 class="card-title">Inspired by LeNet-5</h2>
    <ul>
      <li>Input: \( 32 \times 32 \times 3 \)</li>
      <ul>
        <li>Use \( f=5, s=1 \)</li>
      </ul>
      <li>Conv1: \( 28 \times 28 \times 6 \)</li>
      <ul>
        <li>Use \( f=2, s=2 \)</li>
      </ul>
      <li>Pool1: \( 14 \times 14 \times 6  \)</li>
      <ul>
        <li>Use \( f=5, s=1 \)</li>
      </ul>
      <li>Conv2: \( 10 \times 10 \times 16 \)</li>
      <ul>
        <li>Use \( f=2, s=2 \)</li>
      </ul>
      <li>Pool2: \( 5 \times 5 \times 16  \)</li>
      <li>Flatten: \( 400 \)</li>
      <li>FC3: \( 120 \) where weights are \( 120 \) by \( 400 \)</li>
      <li>FC4: \( 84 \)</li>
      <li>Softmax of \( 10 \) outputs</li>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://www.deeplearning.ai/deep-learning-specialization/">Deep Learning Specialization</a>
  </div>
</div>

<div class="card mb-4" id="convolutional-neural-network-">
  <div class="card-body">
    <h2 class="card-title">AlexNet</h2>
    <ul>
      <li>Input: \( 227 \times 227 \times 3 \)</li>
      <ul>
        <li>Use \( f=11, s=4 \)</li>
      </ul>
      <li>Conv1: \( 55 \times 55 \times 96 \)</li>
      <ul>
        <li>Use \( f=3, s=2 \)</li>
      </ul>
      <li>Pool1: \( 27 \times 27 \times 96  \)</li>
      <ul>
        <li>Use \( f=5, same \)</li>
      </ul>
      <li>Conv2: \( 27 \times 27 \times 256 \)</li>
      <ul>
        <li>Use \( f=3, s=2 \)</li>
      </ul>
      <li>Pool2: \( 13 \times 13 \times 256  \)</li>
      <ul>
        <li>Use \( f=3, same \)</li>
      </ul>
      <li>Conv3: \( 13 \times 13 \times 384 \)</li>
      <ul>
        <li>Use \( f=3, same \)</li>
      </ul>
      <li>Conv4: \( 13 \times 13 \times 384  \)</li>
      <ul>
        <li>Use \( f=3, same \)</li>
      </ul>
      <li>Conv5: \( 13 \times 13 \times 256  \)</li>
      <ul>
        <li>Use \( f=3, s=2 \)</li>
      </ul>
      <li>Pool3: \( 6 \times 6 \times 256  \)</li>
      <li>Flatten: \( 9216 \)</li>
      <li>FC3: \( 4096 \)</li>
      <li>FC4: \( 4096 \)</li>
      <li>Softmax of \( 1000 \) outputs</li>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://www.deeplearning.ai/deep-learning-specialization/">Deep Learning Specialization</a>
  </div>
</div>

<div class="card mb-4" id="convolutional-neural-network-">
  <div class="card-body">
    <h2 class="card-title">VGG-16</h2>
    <ul>
      <li>Input: \( 224 \times 224 \times 3 \)</li>
      <ul>
        <li>Use \( f=3, s=1, same \)</li>
      </ul>
      <li>Conv1, conv2: \( 224 \times 224 \times 64 \)</li>
      <ul>
        <li>Use \( f=2, s=2 \)</li>
      </ul>
      <li>Pool1: \( 112 \times 112 \times 64  \)</li>
      <ul>
        <li>Use \( f=3, s=1, same \)</li>
      </ul>
      <li>Conv3, conv4: \( 112 \times 112 \times 128 \)</li>
      <ul>
        <li>Use \( f=2, s=2 \)</li>
      </ul>
      <li>Pool2: \( 56 \times 56 \times 128  \)</li>
      <ul>
        <li>Use \( f=3, s=1, same \)</li>
      </ul>
      <li>Conv5, conv6, conv7: \( 56 \times 56 \times 256 \)</li>
      <ul>
        <li>Use \( f=2, s=2 \)</li>
      </ul>
      <li>Pool3: \( 28 \times 28 \times 256  \)</li>
      <ul>
        <li>Use \( f=3, s=1, same \)</li>
      </ul>
      <li>Conv8, conv9, conv10: \( 28 \times 28 \times 512  \)</li>
      <ul>
        <li>Use \( f=2, s=2 \)</li>
      </ul>
      <li>Pool4: \( 14 \times 14 \times 512  \)</li>
      <ul>
        <li>Use \( f=3, s=1, same \)</li>
      </ul>
      <li>Conv11, conv12, conv13: \( 14 \times 14 \times 512  \)</li>
      <ul>
        <li>Use \( f=2, s=2 \)</li>
      </ul>
      <li>Pool5: \( 7 \times 7 \times 512  \)</li>
      <li>FC3: \( 4096 \)</li>
      <li>FC4: \( 4096 \)</li>
      <li>Softmax of \( 1000 \) outputs</li>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://www.deeplearning.ai/deep-learning-specialization/">Deep Learning Specialization</a>
  </div>
</div>
<!-- Convolutional neural network END -->

</div> <!-- /.col-md-12 -->
</div> <!-- /.row -->
</div> <!-- /.container -->

<include src="/footer.html"></include>

</body>

</html>