<!DOCTYPE html>

<html lang="en">

<head>

<!-- Metadata -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="">
<meta name="author" content="Seungmoon Rieh">
<meta name="keywords" content="">

<!-- Title and image -->
<title>Seungmoon Rieh</title>
<link href="/img/seungmoonrieh.jpg" rel="icon">

<!-- CSS -->
<link href="/css/bootstrap.min.css" rel="stylesheet">
<link href="/css/monokai-sublime.css" rel="stylesheet">
<link href="/css/site.css" rel="stylesheet">

<!-- JavaScript -->
<script src="/js/jquery.min.js"></script>
<script src="/js/bootstrap.bundle.min.js"></script>
<script src="/js/highlight.pack.js"></script>
<script src="/js/include_html.js" type="text/javascript"></script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script src="/js/site.js" type="text/javascript"></script>

</head>

<body>

<include src="/header.html"></include>

<div class="container">
<div class="row">
<div class="col-md-12">
<h1 class="my-4">Machine Learning</h1>

<!-- Regularization BEGIN -->
<div class="card mb-4" id="subject">
  <div class="card-body">
    <h2 class="card-title">Regularization</h2>
    <p class="card-text"></p>
    <ul class="list-unstyled mb-0">
      <li><a href="#regularization-">Regularization</a></li>
      <li><a href="#regularization-">Dropout</a></li>
      <li><a href="#regularization-">Early stopping</a></li>
    </ul>
  </div>
</div>

<div class="card mb-4" id="regularization-">
  <div class="card-body">
    <h2 class="card-title">Regularization</h2>
    <ul>
      <li>Prevents overfitting by introducing a tuning parameter into a model (Penalize weights being large)</li>
      <li>L2 performs better than L1 in practice (L1 performs well on sparse data)</li>
      <li>L2 is useful when features are correlated</li>
      <li>L2 has analytical solution while L1 does not</li>
      <li>Feature normalization is necessary</li>
    </ul>

    <h3 class="card-title">Why regularization works</h3>
    <ul>
      <li>Larger \( \lambda \) leads to \( W \) to become smaller, reducing the impacts of hidden units</li>
      <li>Smaller \( W \) also leads \( Z \) to become smaller. The activation function could more or less be linear function when Z is small. Thus, network does not compute complex function</li>
    </ul>

    <h3 class="card-title">Regularization in logistic regression</h3>
    <ul>
      <li>Cost function for L2 norm</li>
      <ul>
        <li>\( J(w,b) = \dfrac{1}{m}\displaystyle\sum_{i=1}^{m} L(\hat{y}^{(i)},y^{(i)}) + \dfrac{\lambda}{2m}||w||_{2}^{2} \)</li>
        <li>\( ||w||^{2} = \displaystyle\sum_{j=1}^{n_{x}}w_{j}^{2} = w^{T}w \)</li>
      </ul>
      <li>Cost function for L1 norm</li>
      <ul>
        <li>\( J(w,b) = \dfrac{1}{m}\displaystyle\sum_{i=1}^{m} L(\hat{y}^{(i)},y^{(i)}) + \dfrac{\lambda}{2m}||w||_{1} \)</li>
      </ul>
    </ul>

    <h3 class="card-title">Regularization in neural network</h3>
    <ul>
      <li>Cost function for Frobenius norm</li>
      <ul>
        <li>\( J(w,b) = \dfrac{1}{m}\displaystyle\sum_{i=1}^{m} L(\hat{y}^{(i)},y^{(i)}) + \dfrac{\lambda}{2m}\displaystyle\sum_{l=1}^{m}||w^{[l]}||_{F}^{2} \)</li>
        <li>\( ||w^{[l]}||_{F}^{2} = \displaystyle\sum_{i=1}^{n^{[l-1]}}\displaystyle\sum_{j=1}^{n^{[l]}}(w_{ij}^{[l]})^{2} \)</li>
        <li>Add \( \dfrac{\lambda}{m}w^{[l]} \) to \( dw^{[l]} \)</li>
        <li>\( w^{[l]} = w^{[l]} - \alpha dw^{[l]} \) remains the same</li>
      </ul>
    </ul>

<pre><code class="python">def forward_propagation(X, parameters):

    W1 = parameters["W1"]
    b1 = parameters["b1"]
    W2 = parameters["W2"]
    b2 = parameters["b2"]
    W3 = parameters["W3"]
    b3 = parameters["b3"]

    # LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID
    Z1 = np.dot(W1, X) + b1
    A1 = relu(Z1)
    Z2 = np.dot(W2, A1) + b2
    A2 = relu(Z2)
    Z3 = np.dot(W3, A2) + b3
    A3 = sigmoid(Z3)

    cache = (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3)

    return A3, cache</code></pre>

<pre><code class="python">def compute_cost_with_regularization(A3, Y, parameters, lambd):

    m = Y.shape[1]
    W1 = parameters["W1"]
    W2 = parameters["W2"]
    W3 = parameters["W3"]

    cross_entropy_cost = compute_cost(A3, Y)

    L2_regularization_cost = lambd * ( np.sum(np.square(W1)) + np.sum(np.square(W2)) + np.sum(np.square(W3)) ) / (2 * m)

    cost = cross_entropy_cost + L2_regularization_cost

    return cost</code></pre>

<pre><code class="python">def backward_propagation_with_regularization(X, Y, cache, lambd):

    m = X.shape[1]
    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache

    dZ3 = A3 - Y

    dW3 = 1./m * np.dot(dZ3, A2.T) + (lambd * W3 / m)
    db3 = 1./m * np.sum(dZ3, axis=1, keepdims = True)

    dA2 = np.dot(W3.T, dZ3)
    dZ2 = np.multiply(dA2, np.int64(A2 > 0))
    dW2 = 1./m * np.dot(dZ2, A1.T) + (lambd * W2 / m)
    db2 = 1./m * np.sum(dZ2, axis=1, keepdims = True)

    dA1 = np.dot(W2.T, dZ2)
    dZ1 = np.multiply(dA1, np.int64(A1 > 0))
    dW1 = 1./m * np.dot(dZ1, X.T) + (lambd * W1 / m)
    db1 = 1./m * np.sum(dZ1, axis=1, keepdims = True)

    gradients = {"dZ3": dZ3, "dW3": dW3, "db3": db3,"dA2": dA2,
                 "dZ2": dZ2, "dW2": dW2, "db2": db2, "dA1": dA1,
                 "dZ1": dZ1, "dW1": dW1, "db1": db1}

    return gradients</code></pre>

<pre><code class="python">def model(X, Y, learning_rate = 0.3, num_iterations = 30000, lambd = 0):

    grads = {}
    costs = []
    m = X.shape[1]
    layers_dims = [X.shape[0], 20, 3, 1]

    parameters = initialize_parameters(layers_dims)

    for i in range(0, num_iterations):

        a3, cache = forward_propagation(X, parameters)

        cost = compute_cost_with_regularization(a3, Y, parameters, lambd)

        grads = backward_propagation_with_regularization(X, Y, cache, lambd)

        parameters = update_parameters(parameters, grads, learning_rate)

    return parameters</code></pre>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://github.com/iamtodor/data-science-interview-questions-and-answers">Data science interview questions with answers</a> | <a href="https://www.deeplearning.ai/deep-learning-specialization">Deep Learning Specialization</a>
  </div>
</div>

<div class="card mb-4" id="regularization-">
  <div class="card-body">
    <h2 class="card-title">Dropout</h2>
    <ul>
      <li>Most effective regularization in neural network</li>
      <li>Nodes in each layer in randomly deactivated in forward pass</li>
      <li>In each iteration, only \( 1-\rho \) trained (Neurons are deactivated with probability \( \rho \))</li>
    </ul>

    <h3 class="card-title">Inverted dropout</h3>
    <ul>
      <li>During training time</li>
      <ul>
        <li><code>keep_prob = 0.8</code> (20% chance that units will be shutdown)</li>
        <li><code>d = np.random.rand(a.shape[0], a.shape[1]) < keep_prob</code></li>
        <li><code>a = np.multiply(a,d)</code></li>
        <li><code>a /= keep_prod</code></li>
      </ul>
      <li>Each layer in the network could have different <code>keep_prob</code>, but it will increase the number of hyperparameters to tune</li>
      <li>Dropout is not to be used during test time</li>
      <li>Cost function is no longer well-defined</li>
    </ul>

    <h3 class="card-title">Why dropout works</h3>
    <ul>
      <li>For a paricular neuron, its inputs can get eliminiated</li>
      <li>Thus, the neuron become reluctant to put too much weight on one input feature</li>
      <li>Rather, it wants to spread out weights, leading to smaller weights</li>
    </ul>

<pre><code class="python">def forward_propagation_with_dropout(X, parameters, keep_prob = 0.5):

    np.random.seed(1)

    # retrieve parameters
    W1 = parameters["W1"]
    b1 = parameters["b1"]
    W2 = parameters["W2"]
    b2 = parameters["b2"]
    W3 = parameters["W3"]
    b3 = parameters["b3"]

    Z1 = np.dot(W1, X) + b1
    A1 = relu(Z1)

    D1 = np.random.rand(A1.shape[0], A1.shape[1])
    D1 = (D1 < keep_prob).astype(int)
    A1 = A1 * D1
    A1 = A1 / keep_prob

    Z2 = np.dot(W2, A1) + b2
    A2 = relu(Z2)

    D2 = np.random.rand(A2.shape[0], A2.shape[1])
    D2 = (D2 < keep_prob).astype(int)
    A2 = A2 * D2
    A2 = A2 / keep_prob
    Z3 = np.dot(W3, A2) + b3
    A3 = sigmoid(Z3)

    cache = (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3)

    return A3, cache</code></pre>

<pre><code class="python">def backward_propagation_with_dropout(X, Y, cache, keep_prob):

    m = X.shape[1]
    (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3) = cache

    dZ3 = A3 - Y
    dW3 = 1./m * np.dot(dZ3, A2.T)
    db3 = 1./m * np.sum(dZ3, axis=1, keepdims = True)
    dA2 = np.dot(W3.T, dZ3)

    dA2 = dA2 * D2
    dA2 = dA2 / keep_prob

    dZ2 = np.multiply(dA2, np.int64(A2 > 0))
    dW2 = 1./m * np.dot(dZ2, A1.T)
    db2 = 1./m * np.sum(dZ2, axis=1, keepdims = True)

    dA1 = np.dot(W2.T, dZ2)

    dA1 = dA1 * D1
    dA1 = dA1 / keep_prob

    dZ1 = np.multiply(dA1, np.int64(A1 > 0))
    dW1 = 1./m * np.dot(dZ1, X.T)
    db1 = 1./m * np.sum(dZ1, axis=1, keepdims = True)

    gradients = {"dZ3": dZ3, "dW3": dW3, "db3": db3,"dA2": dA2,
                 "dZ2": dZ2, "dW2": dW2, "db2": db2, "dA1": dA1,
                 "dZ1": dZ1, "dW1": dW1, "db1": db1}

    return gradients</code></pre>

<pre><code class="python">def model(X, Y, learning_rate = 0.3, num_iterations = 30000, print_cost = True, lambd = 0, keep_prob = 1):

    grads = {}
    costs = []
    m = X.shape[1]
    layers_dims = [X.shape[0], 20, 3, 1]

    parameters = initialize_parameters(layers_dims)

    for i in range(0, num_iterations):

        a3, cache = forward_propagation_with_dropout(X, parameters, keep_prob)

        cost = compute_cost(a3, Y)

        grads = backward_propagation_with_dropout(X, Y, cache, keep_prob)

        parameters = update_parameters(parameters, grads, learning_rate)

    return parameters</code></pre>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://www.deeplearning.ai/deep-learning-specialization">Deep Learning Specialization</a>
  </div>
</div>

<div class="card mb-4" id="regularization-">
  <div class="card-body">
    <h2 class="card-title">Early stopping</h2>
    <ul>
      <li>Plot training and dev error against the number of epocs</li>
      <li>Stop gradient descent when dev error starts to increase</li>
      <li>Mix optimizing the cost function and preventing overfitting at the same time rather than treating those tasks independently</li>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://www.deeplearning.ai/deep-learning-specialization">Deep Learning Specialization</a>
  </div>
</div>
<!-- Regularization END -->

</div> <!-- /.col-md-12 -->
</div> <!-- /.row -->
</div> <!-- /.container -->

<include src="/footer.html"></include>

</body>

</html>