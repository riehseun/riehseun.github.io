<!DOCTYPE html>

<html lang="en">

<head>

<!-- Metadata -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="">
<meta name="author" content="Seungmoon Rieh">
<meta name="keywords" content="">

<!-- Title and image -->
<title>Seungmoon Rieh</title>
<link href="/img/seungmoonrieh.jpg" rel="icon">

<!-- CSS -->
<link href="/css/bootstrap.min.css" rel="stylesheet">
<link href="/css/monokai-sublime.css" rel="stylesheet">
<link href="/css/site.css" rel="stylesheet">

<!-- JavaScript -->
<script src="/js/jquery.min.js"></script>
<script src="/js/bootstrap.bundle.min.js"></script>
<script src="/js/highlight.pack.js"></script>
<script src="/js/include_html.js" type="text/javascript"></script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script src="/js/site.js" type="text/javascript"></script>

</head>

<body>

<include src="/header.html"></include>

<div class="container">
<div class="row">
<div class="col-md-12">
<h1 class="my-4">Machine Learning</h1>

<!-- Regularization BEGIN -->
<div class="card mb-4" id="subject">
  <div class="card-body">
    <h2 class="card-title">Regularization</h2>
    <p class="card-text"></p>
    <ul class="list-unstyled mb-0">
      <li><a href="#regularization-">Regularization</a></li>
      <li><a href="#regularization-">Dropout</a></li>
      <li><a href="#regularization-">Early stopping</a></li>
    </ul>
  </div>
</div>

<div class="card mb-4" id="regularization-">
  <div class="card-body">
    <h2 class="card-title">Regularization</h2>
    <ul>
      <li>Prevents overfitting by introducing a tuning parameter into a model.</li>
      <li>L2 performs better than L1 in practice. (L1 performs well on sparse data)</li>
      <li>L2 is useful when features are correlated.</li>
      <li>L2 has analytical solution while L1 does not.</li>
      <li>Feature normalization is necessary.</li>
    </ul>

    <h3 class="card-title">Why regularization works</h3>
    <ul>
      <li>Larger \( \lambda \) leads to \( W \) to become smaller, reducing the impacts of hidden units.</li>
      <li>Smaller \( W \) also leads \( Z \) to become smaller. The activation function could more or less be linear function when Z is small. Thus, network does not compute complex function.</li>
    </ul>

    <h3 class="card-title">Regularization in logistic regression</h3>
    <ul>
      <li>Cost function for L2 norm: \( J(w,b) = \dfrac{1}{m}\displaystyle\sum_{i=1}^{m} L(\hat{y}^{(i)},y^{(i)}) + \dfrac{\lambda}{2m}||w||_{2}^{2} \)</li>
      <li>Cost function for L1 norm: \( J(w,b) = \dfrac{1}{m}\displaystyle\sum_{i=1}^{m} L(\hat{y}^{(i)},y^{(i)}) + \dfrac{\lambda}{2m}||w||_{1} \)</li>
    </ul>

    <h3 class="card-title">Regularization in neural network</h3>
    <ul>
      <li>Cost function for Frobenius norm: \( J(w,b) = \dfrac{1}{m}\displaystyle\sum_{i=1}^{m} L(\hat{y}^{(i)},y^{(i)}) + \dfrac{\lambda}{2m}\displaystyle\sum_{l=1}^{m}||w^{[l]}||_{F}^{2} \)</li>
      <li>Add \( \dfrac{\lambda}{m}w^{[l]} \) to \( \textbf{dW}^{[l]} \)</li>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://github.com/iamtodor/data-science-interview-questions-and-answers">Data science interview questions with answers</a> | <a href="https://www.deeplearning.ai/deep-learning-specialization/">Deep Learning Specialization</a>
  </div>
</div>

<div class="card mb-4" id="regularization-">
  <div class="card-body">
    <h2 class="card-title">Dropout</h2>
    <ul>
      <li>Most effective regularization in neural network.</li>
      <li>Nodes in each layer in randomly deactivated in forward pass.</li>
      <li>In each iteration, only 1-p trained. (Neurons are deactivated with probability p)</li>
    </ul>

    <h3 class="card-title">Inverted dropout</h3>
    <ul>
      <li>Training time</li>
      <ul>
        <li>d = np.random.rand(a.shape[0], a.shape[1]) < keep_prob</li>
        <li>a = np.multiply(a,d)</li>
        <li>a /= keep_prod</li>
      </ul>
      <li>Each layer in the network could have different keep_prob, but it will increase hyperparameters to tune.</li>
      <li>Dropout is not to be used during test time.</li>
      <li>Cost function is no longer well-defined.</li>
    </ul>

    <h3 class="card-title">Why dropout works</h3>
    <ul>
      <li>For a paricular neuron, its inputs can get eliminiated.</li>
      <li>Thus, the neuron become reluctant to put too much weight on one input feature.</li>
      <li>Rather, it wants to spread out weights, leading to smaller weights.</li>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://www.deeplearning.ai/deep-learning-specialization/">Deep Learning Specialization</a>
  </div>
</div>

<div class="card mb-4" id="regularization-">
  <div class="card-body">
    <h2 class="card-title">Early stopping</h2>
    <ul>
      <li>Plot training and dev error against the number of epocs.</li>
      <li>Stop gradient descent when dev error starts to increase.</li>
      <li>Mixing optimizing the cost function and preventing overfitting at the same time rather than treating those tasks independently.</li>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://www.deeplearning.ai/deep-learning-specialization/">Deep Learning Specialization</a>
  </div>
</div>
<!-- Regularization END -->

</div> <!-- /.col-md-12 -->
</div> <!-- /.row -->
</div> <!-- /.container -->

<include src="/footer.html"></include>

</body>

</html>