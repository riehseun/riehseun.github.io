<!DOCTYPE html>

<html lang="en">

<head>

<!-- Metadata -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="">
<meta name="author" content="Seungmoon Rieh">
<meta name="keywords" content="">

<!-- Title and image -->
<title>Seungmoon Rieh</title>
<link href="/img/seungmoonrieh.jpg" rel="icon">

<!-- CSS -->
<link href="/css/bootstrap.min.css" rel="stylesheet">
<link href="/css/monokai-sublime.css" rel="stylesheet">
<link href="/css/site.css" rel="stylesheet">

<!-- JavaScript -->
<script src="/js/jquery.min.js" type="text/javascript"></script>
<script src="/js/bootstrap.bundle.min.js" type="text/javascript"></script>
<script src="/js/highlight.pack.js" type="text/javascript"></script>
<script src="/js/include_html.js" type="text/javascript"></script>
<script src="/js/mathjax/tex-chtml.js" type="text/javascript"></script>
<script src="/js/site.js" type="text/javascript"></script>

</head>

<body>

<include src="/header.html"></include>

<div class="container">
<div class="row">
<div class="col-md-12">
<h1 class="my-4">Machine Learning</h1>

<!-- logistic regression BEGIN -->
<div class="card mb-4" id="subject">
  <div class="card-body">
    <h2 class="card-title">Logistic regression</h2>
    <ul class="list-unstyled mb-0">
      <li><a href="#logistic-regression-">Logistic regression</a></li>
      <li><a href="#logistic-regression-">Example - binary (torch)</a></li>
      <li><a href="#logistic-regression-">Example - multi-class softmax regression (torch)</a></li>
      <li><a href="#logistic-regression-">Example - binary (py)</a></li>
      <li><a href="#logistic-regression-">Derivation</a></li>
      <li><a href="#logistic-regression-">Softmax regression</a></li>
      <!-- <li><a href="#logistic-regression-">Logistic regression problems</a></li> -->
    </ul>
  </div>
</div>

<div class="card mb-4" id="logistic-regression-">
  <div class="card-body">
    <h2 class="card-title">Logistic regression</h2>
    <ul>
      <li>Used for binary classification</li>
      <li>Fits curve to data</li>
      <ul>
        <li>Still is linear model because there is linear relationship between input and output</li>
      </ul>
      <li>There is no layer (Thus no hidden units)</li>
    </ul>

    <h3 class="card-title">Steps</h3>
    <ul>
      <li>Load data</li>
      <li>If necessary</li>
      <ul>
        <li>Flatten matrix inputs into vectors</li>
        <li>Normalize the data</li>
      </ul>
      <li>Initialize parameters (Weights can be initialized to zero)</li>
      <li>Iterate</li>
      <ul>
        <li>Forward prop</li>
        <li>Compute cost</li>
        <li>Backward prop</li>
        <li>Retrive gradient</li>
        <li>Update weights</li>
      </ul>
      <li>Use the final weights and training data to do compute prediction</li>
      <li>Apply sigmoid to the prediction, which gives value either 0 or 1</li>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://www.coursera.org/specializations/deep-learning">Deep Learning Specialization</a>
  </div>
</div>

<div class="card mb-4" id="logistic-regression-">
  <div class="card-body">
    <h2 class="card-title">Example - binary (torch)</h2>

<pre><code class="python"># Load data
# Standardize data
# Intialize parameters
# Train
#   Compute Z, A (forward porp)
#   Compute cost
#   Compute gradient (backward prop)
#   Update weights
# Inference

from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from torch.utils.data import Dataset, DataLoader

import torch
import torch.nn as nn
import torch.nn.functional as F

# Data

class CustomDataset(Dataset):

    def __init__(self, X_train, y_train):
        self.x = F.normalize(torch.from_numpy(X_train))  # (m, n)
        self.y = torch.from_numpy(y_train) # (m)

    def __len__(self):
        return len(self.x)

    def __getitem__(self, index):
        return self.x[index].float(), self.y[index].float()

dataset = load_breast_cancer()
n = dataset.data.shape[1]
print(f"num_features: {n}")
m = len(dataset.data)
print(f"num_data: {m}")
X_train, X_test, y_train, y_test = train_test_split(dataset.data, dataset.target, test_size=0.2)
train_data = CustomDataset(X_train, y_train)
test_data = CustomDataset(X_test, y_test)
train_dataloader = DataLoader(train_data, batch_size=64, shuffle=True)
test_dataloader = DataLoader(test_data, batch_size=1, shuffle=True)

# Model
class LogisticRegression(nn.Module):

    def __init__(self, num_features):
        super().__init__()
        # X (m,n) => Y (m, 1)
        self.linear = nn.Linear(num_features, 1)

    def forward(self, x):
        x = self.linear(x)
        x = torch.sigmoid(x)
        x = torch.squeeze(x)
        return x

# Train
model = LogisticRegression(n)
learning_rate = 0.01
criterion = nn.BCELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

epoch = 1000

for key, val in model.state_dict().items():
    print(f"{key}: {val}")

for e in range(epoch):
    for x_batch, y_batch in train_dataloader:

        optimizer.zero_grad()  # Reset grads

        y_hat = model(x_batch)  # (m, 1)

        # print(f"x_batch: {x_batch.shape}")
        # print(f"y_batch: {y_batch.shape}")
        # print(f"y_hat: {y_hat.shape}")
        # print(y_batch)
        # print(y_hat)

        loss = criterion(y_hat, y_batch)  # Compute loss

        loss.backward()  # Compute gradient

        optimizer.step()  # Adjust learning rate

        print(f"epoch: {e+1}, loss = {loss.item()}")

for key, val in model.state_dict().items():
    print(f"{key}: {val}")

# Inference
correct = 0
incorrect = 0
with torch.no_grad():
    for x_batch, y_batch in test_dataloader:
        y_pred = model(x_batch)  # no need to call model.forward()
        if y_pred > 0.5:
            y_pred = 1
        else:
            y_pred = 0
        if y_pred == y_batch[0]:
            correct += 1
        else:
            incorrect += 1
    accuracy = correct/(correct+incorrect)
    print(f"accurach: {accuracy}")</code></pre>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href=""></a>
  </div>
</div>

<div class="card mb-4" id="logistic-regression-">
  <div class="card-body">
    <h2 class="card-title">Example - multi-class softmax regression (torch)</h2>
    <ul>
      <li>Generalization of logistic regression</li>
      <li>Number of units in the output layer is \( n^{L} = C \) the number of classes</li>
      <li>If \( C = 2 \), applying softmax reduces to logistic regression</li>
    </ul>

    <h3 class="card-title">Activation in the output layer</h3>
    <ul>
      <li>In the output layer \( z^{[L]} = w^{[L]}a^{[L-1]} + b^{[l]} \)</li>
      <li>The activation in the last layer for softmax is \( a^{[L]} = \dfrac{e^{z^{[L]}}}{\sum_{i=1}^{C}t_{i}} \) where \( t = e^{z^{[L]}} \)</li>
    </ul>

    <h3 class="card-title">Loss function</h3>
    <ul>
      <li>\( L(\hat{y},y) = -\displaystyle\sum_{j=1}^{C} y_{j}log\hat{y}_{j} \)</li>
    </ul>

    <h3 class="card-title">Gradient descent</h3>
    <ul>
      <li>\( \textbf{dZ} = \textbf{A} - \textbf{y} \)</li>
    </ul>

<pre><code class="python"># Load data
# Standardize data
# Intialize parameters
# Train
#   Compute Z, A (forward porp)
#   Compute cost
#   Compute gradient (backward prop)
#   Update weights
# Inference

from sklearn.datasets import load_wine
from sklearn.model_selection import train_test_split
from torch.utils.data import Dataset, DataLoader

import torch
import torch.nn as nn
import torch.nn.functional as F

# Data

class CustomDataset(Dataset):

    def __init__(self, X_train, y_train, c):
        self.x = F.normalize(torch.from_numpy(X_train))  # (m, n)
        self.y = torch.from_numpy(y_train) # (m)

    def __len__(self):
        return len(self.x)

    def __getitem__(self, index):
        return self.x[index].float(), self.y[index]

dataset = load_wine()
n = dataset.data.shape[1]
print(f"num_features: {n}")
c = 3
print(f"num_classes: {c}")
m = len(dataset.data)
print(f"num_data: {m}")
X_train, X_test, y_train, y_test = train_test_split(dataset.data, dataset.target, test_size=0.2)
train_data = CustomDataset(X_train, y_train, c)
test_data = CustomDataset(X_test, y_test, c)
train_dataloader = DataLoader(train_data, batch_size=64, shuffle=True)
test_dataloader = DataLoader(test_data, batch_size=1, shuffle=True)

# Model
class LogisticRegression(nn.Module):

    def __init__(self, num_features, num_classes):
        super().__init__()
        # X (m,n) => Y (m, num_classes)
        self.linear = nn.Linear(num_features, num_classes)

    def forward(self, x):
        x = self.linear(x)
        # x = torch.softmax(x, dim=1)
        return x

# Train
model = LogisticRegression(n, c)
learning_rate = 0.01
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

epoch = 1000

for key, val in model.state_dict().items():
    print(f"{key}: {val}")

for e in range(epoch):
    for x_batch, y_batch in train_dataloader:

        optimizer.zero_grad()  # Reset grads

        y_hat = model(x_batch)  # (m, 3)

        # print(f"x_batch: {x_batch.shape}")
        # print(f"y_batch: {y_batch.shape}")
        # print(f"y_hat: {y_hat.shape}")
        # print(y_hat)
        # print(y_batch)

        loss = criterion(y_hat, y_batch)  # Compute loss

        loss.backward()  # Compute gradient

        optimizer.step()  # Adjust learning rate

        print(f"epoch: {e+1}, loss = {loss.item()}")

for key, val in model.state_dict().items():
    print(f"{key}: {val}")

# Inference
correct = 0
incorrect = 0
with torch.no_grad():
    for x_batch, y_batch in test_dataloader:
        y_pred = model(x_batch)  # no need to call model.forward()
        # print(f"{torch.tensor(torch.argmax(y_pred))}:{y_batch}")
        if torch.argmax(y_pred) == y_batch[0]:
            correct += 1
        else:
            incorrect += 1
    accuracy = correct/(correct+incorrect)
    print(f"accurach: {accuracy}")</code></pre>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href=""></a>
  </div>
</div>

<div class="card mb-4" id="logistic-regression-">
  <div class="card-body">
    <h2 class="card-title">Example - binary (py)</h2>
    <ul>
      <li>\( m \) training examples</li>
      <li>\( \textbf{X} = \begin{bmatrix} \vdots & \vdots & \vdots \\ \vdots & \vdots & \vdots \\ x_{1} & x_{2} \ldots & x_{m} \\ \vdots & \vdots & \vdots \\ \vdots & \vdots & \vdots \end{bmatrix} \)</li>
      <li>\( \textbf{y} = \begin{bmatrix} y_{1} & y_{2} \ldots & y_{m} \end{bmatrix} \)</li>
    </ul>

    <h3 class="card-title">Parameters</h3>
    <ul>
      <li>Number of features \( n^{[0]} \)</li>
      <li>Number of output units \( n^{[1]} = 1 \)</li>
    </ul>

    <h3 class="card-title">Dimensions</h3>
    <ul>
      <li>\( X \).shape = \( (n^{[0]}, m) \)</li>
      <li>\( y \).shape = \( (1, m) \)</li>
    </ul>

    <h3 class="card-title">Forward prop</h3>
    <ul>
      <li>\( \textbf{Z} = \begin{bmatrix} \textbf{z}_{1} & \textbf{z}_{2} \ldots & \textbf{z}_{m} \end{bmatrix} = \begin{bmatrix} \textbf{w}^{T}\textbf{x}_{1}+b & \textbf{w}^{T}\textbf{x}_{2}+b \ldots & \textbf{w}^{T}\textbf{x}_{m}+b \end{bmatrix} = \textbf{W}^{T}\textbf{X} + \begin{bmatrix} b & b \ldots & b \end{bmatrix} \)</li>
      <li>\( \textbf{A} = \begin{bmatrix} \textbf{a}_{1} & \textbf{a}_{2} \ldots & \textbf{a}_{m} \end{bmatrix} = \sigma(\textbf{Z}) = \sigma (\textbf{W}^{T}\textbf{X}+\textbf{b}) \)</li>
      <ul>
        <li>Without \( \sigma \), it is just linear regression</li>
      </ul>
    </ul>

<pre><code class="python">Z = np.dot(w.T, X) + b
A = sigmoid(Z)</code></pre>

    <h3 class="card-title">Cost function</h3>
    <ul>
      <li>Loss function \( L(\hat{y},y) = -\left( ylog(\hat{y}) + (1-y)log(1-\hat{y}) \right) \)</li>
      <ul>
        <li>This loss function is convex, thus gradient descent can find the global optimum</li>
        <ul>
          <li>For example, \( L(\hat{y},y) = \frac{1}{2}(\hat{y}-y)^{2} \) something like this is not convex</li>
        </ul>
        <li>If \( y = 1 \), \( L(\hat{y}, y) = -log\hat{y} \)</li>
        <ul>
          <li>We want \( \hat{y} \) large as possible ( \( y \approx 1 \) )</li>
        </ul>
        <li>If \( y = 0 \), \( L(\hat{y}, y) = -log(1-\hat{y}) \)</li>
        <ul>
          <li>We want \( \hat{y} \) small as possible ( \( y \approx 0 \) )</li>
        </ul>
      </ul>
      <li>Cost function \( J = \dfrac{1}{m}\displaystyle\sum_{i=1}^{m} L(\hat{y}_{i},y_{i}) = -\dfrac{1}{m}\displaystyle\sum_{i=1}^{m} \left(y_{i}\log(\textbf{a}_{i}) + (1-y_{i})\log(1-\textbf{a}_{i})\right) \)</li>
    </ul>

<pre><code class="python">m = X.shape[1]
cost = -(np.dot(Y, np.log(A.T)) + np.dot((1-Y), np.log(1-A.T))) / m
cost = np.squeeze(cost)</code></pre>

    <h3 class="card-title">Backward prop</h3>
    <ul>
      <li>\( \textbf{dZ} = \begin{bmatrix} \textbf{dz}_{1} & \textbf{dz}_{2} \ldots & \textbf{dz}_{m} \end{bmatrix} = \begin{bmatrix} \textbf{a}_{1}-y_{1} & \textbf{a}_{2}-y_{2} \ldots & \textbf{a}_{m}-y_{m} \end{bmatrix} = \textbf{A} - \textbf{y} \)</li>
      <li>\( \textbf{dW} = \dfrac{1}{m}\textbf{X}\textbf{dZ}^{T} \)</li>
      <li>\( \textbf{db} = \dfrac{1}{m}np.sum(\textbf{dZ}) \)</li>
      <li>\( \textbf{W} := \textbf{W} - \alpha \textbf{dW} \)</li>
      <li>\( \textbf{b} := \textbf{b} - \alpha \textbf{db} \)</li>
    </ul>

    <h3 class="card-title">Backward prop derivation (consider m=1 and n=2)</h3>
    <ul>
      <li>Let</li>
      <ul>
        <li>\( \hat{y} = a, L(a,y) = -\left( ylog(a) + (1-y)log(1-a) \right) \)</li>
        <li>\( z = w_{1}x_{1} + w_{2}x_{2} + b \)</li>
      </ul>
      <li>\( da = \dfrac{dL}{da} = -\dfrac{y}{a} + \dfrac{1-y}{1-a} \)</li>
      <li>\( dz = \dfrac{dL}{dz} = \dfrac{dL}{da}\dfrac{da}{dz} =  \left(-\dfrac{y}{a} + \dfrac{1-y}{1-a}\right) a(1-a) = a - y \)</li>
      <li>Then</li>
      <ul>
        <li>\( dw_{1} = x_{1}dz, dw_{2} = x_{2}dz \)</li>
        <li>\( db = dz \)</li>
      </ul>
    </ul>

<pre><code class="python">m = X.shape[1]
dZ = A - Y
dw = np.dot(X, (dZ).T) / m
db = np.sum(dZ) / m</code></pre>

    <h3 class="card-title">Full implementation</h3>

<pre><code class="python">def propagate(w, b, X, Y):

    # Forward prop
    m = X.shape[1]
    Z = np.dot(w.T, X) + b
    A = sigmoid(Z)

    # Compute cost
    cost = -(np.dot(Y, np.log(A.T)) + np.dot((1-Y), np.log(1-A.T))) / m
    cost = np.squeeze(cost)

    # Backward prop
    dZ = A - Y
    dw = np.dot(X, (dZ).T) / m
    db = np.sum(dZ) / m

    grads = {"dw": dw, "db": db}

    return grads, cost</code></pre>

<pre><code class="python">def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):

    for i in range(num_iterations):

        grads, cost = propagate(w, b, X, Y)

        dw = grads["dw"]
        db = grads["db"]

        w = w - learning_rate * dw
        b = b - learning_rate * db

    params = {"w": w, "b": b}
    grads = {"dw": dw, "db": db}

    return params, grads, costs</code></pre>

<pre><code class="python">def predict(w, b, X):

    m = X.shape[1]
    Y_prediction = np.zeros((1,m))
    w = w.reshape(X.shape[0], 1)

    A = sigmoid(np.dot(w.T, X) + b)

    for i in range(A.shape[1]):

        # Convert probabilities A[0,i] to actual predictions p[0,i]
        if A[0, i] <= 0.5:
            Y_prediction[0, i] = 0
        else:
            Y_prediction[0, i] = 1

    return Y_prediction</code></pre>

<pre><code class="python">def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):

    w, b = initialize_with_zeros(X_train.shape[0])
    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost = False)

    w = parameters["w"]
    b = parameters["b"]

    Y_prediction_test = predict(w, b, X_test)
    Y_prediction_train = predict(w, b, X_train)

    d = {"costs": costs,
         "Y_prediction_test": Y_prediction_test,
         "Y_prediction_train" : Y_prediction_train,
         "w" : w,
         "b" : b,
         "learning_rate" : learning_rate,
         "num_iterations": num_iterations}

    return d</code></pre>

<pre><code class="python"># Load the data (cat/non-cat)
train_set_location = 'data/train_catvnoncat.h5'
test_set_location = 'data/test_catvnoncat.h5'
train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset(train_set_location, test_set_location)

# Reshape the training and test examples such that matrices are flattened into vectors
train_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[0], -1).T
test_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[0], -1).T

# Center and standardize dataset dividing by the maximum value of a pixel channel
train_set_x = train_set_x_flatten / 255
test_set_x = test_set_x_flatten / 255

d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost = True)</code></pre>

    <h3 class="card-title">Helper functions</h3>

<pre><code class="python">def load_dataset(train_set_location, test_set_location):

    train_dataset = h5py.File(train_set_location, "r")
    # These data sets are to be pre-preprocessed (Thus, appended "_orig")
    train_set_x_orig = np.array(train_dataset["train_set_x"][:])  # Train set features
    train_set_y_orig = np.array(train_dataset["train_set_y"][:])  # Train set labels

    test_dataset = h5py.File(test_set_location, "r")
    # These data sets are to be pre-preprocessed (Thus, appended "_orig")
    test_set_x_orig = np.array(test_dataset["test_set_x"][:])  # Test set features
    test_set_y_orig = np.array(test_dataset["test_set_y"][:])  # Test set labels

    classes = np.array(test_dataset["list_classes"][:])  # List of classes

    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))
    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))

    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes</code></pre>

<pre><code class="python">def initialize_with_zeros(dim):

    w = np.zeros((dim, 1))
    b = 0

    return w, b</code></pre>

<pre><code class="python">def sigmoid(z):

    s = 1 / (1 + np.exp(-z))

    return s</code></pre>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://www.coursera.org/specializations/deep-learning">Deep Learning Specialization</a>
  </div>
</div>

<!-- <div class="card mb-4" id="logistic-regression-">
  <div class="card-body">
    <h2 class="card-title">Logistic regression problems</h2>

    <p class="card-text">What is a drawback of introducing more variables to better fit the model?</p>
    <ul>
      <li>Overfitting</li>
    </ul>

    <p class="card-text">What is meant by "odd of success"?</p>
    <ul>
      <li>Ratio betwen probability of success and probability of failure</li>
      <li>\( \dfrac{p}{1-p} \)</li>
    </ul>

    <p class="card-text">What is meant by "interaction"?</p>
    <ul>
      <li>Product of two predictor variables</li>
      <li>Ex. the last term of \( \beta_{0} + \beta_{1}X + \beta_{2}Z + \beta_{3}XZ \)</li>
    </ul>

    <p class="card-text">What is response variable?</p>
    <ul>
      <li>It is the log of the odds of being classified in one of two classes</li>
    </ul>

    <p class="card-text">How is response variable transformed to produce probability distribution?</p>
    <ul>
      <li>Sigmoid function is applied to \( Z \)</li>
    </ul>

    <p class="card-text">What is the relationship between logit and sigmoid function?</p>
    <ul>
      <li>They are inverse of each other</li>
      <li>\( logit(p) = log\left(\dfrac{p}{1-p}\right) \)</li>
      <li>\( p = \dfrac{exp(logit(p))}{1-exp(logit(p))} \)</li>
    </ul>

    <p class="card-text">Consider the following table</p>
    <table>
      <th>
        <td>Tumor eradicated</td>
        <td>Tumor not eradicated</td>
      </th>
      <tr>
        <td>Breast</td>
        <td>560</td>
        <td>260</td>
      </tr>
      <tr>
        <td>Lung</td>
        <td>69</td>
        <td>36</td>
      </tr>
    </table>

    <p class="card-text">What is the explanatory and response variable?</p>
    <ul>
      <li>Explanatory variable - cancer type</li>
      <li>Response variable - tumor eradication</li>
    </ul>

    <p class="card-text">What is the explanatory and response variable?</p>
    <ul>
      <li>Explanatory variable - cancer type</li>
      <li>Response variable - tumor eradication</li>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: Deep Learning Interviews, Shlomo Kashani
  </div>
</div> -->
<!-- Logistic regression END -->

</div> <!-- /.col-md-12 -->
</div> <!-- /.row -->
</div> <!-- /.container -->

<include src="/footer.html"></include>

</body>

</html>