<!DOCTYPE html>

<html lang="en">

<head>

<!-- Metadata -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="">
<meta name="author" content="Seungmoon Rieh">
<meta name="keywords" content="">

<!-- Title and image -->
<title>Seungmoon Rieh</title>
<link href="/img/seungmoonrieh.jpg" rel="icon">

<!-- CSS -->
<link href="/css/bootstrap.min.css" rel="stylesheet">
<link href="/css/monokai-sublime.css" rel="stylesheet">
<link href="/css/site.css" rel="stylesheet">

<!-- JavaScript -->
<script src="/js/jquery.min.js" type="text/javascript"></script>
<script src="/js/bootstrap.bundle.min.js" type="text/javascript"></script>
<script src="/js/highlight.pack.js" type="text/javascript"></script>
<script src="/js/include_html.js" type="text/javascript"></script>
<script src="/js/mathjax/tex-chtml.js" type="text/javascript"></script>
<script src="/js/site.js" type="text/javascript"></script>

</head>

<body>

<include src="/header.html"></include>

<div class="container">
<div class="row">
<div class="col-md-12">
<h1 class="my-4">Machine Learning</h1>

<!-- Machine learning BEGIN -->
<div class="card mb-4" id="subject">
  <div class="card-body">
    <h2 class="card-title">Machine learning</h2>
    <!-- <ul class="list-unstyled mb-0"> -->
    <ul>
      <li>Structure</li>
      <ul>
        <li>Fundamental - think about question for 30 seconds. Then, answer</li>
        <li>Coding - read question for 5 mins. Then, start coding</li>
      </ul>
      <li>ML breath - common <strong>(in-scope)</strong></li>
      <ul>
        <li><a href="#machine-learning-">Statistics and machine learning</a></li>
        <li><a href="#machine-learning-">Supervised learning</a></li>
        <li><a href="#machine-learning-">Feature engineering</a></li>
        <li><a href="#machine-learning-">Bias and variance</a></li>
        <li><a href="#machine-learning-">Classification</a></li>
        <li><a href="#machine-learning-">Ensemble</a></li>
        <li><a href="#machine-learning-">Unsupervised learning</a></li>
        <li><a href="#machine-learning-">Dimentionality reduction</a></li>
        <li><a href="#machine-learning-">Neural network</a></li>
        <li><a href="#machine-learning-">Activation</a></li>
        <li><a href="#machine-learning-">Normalization</a></li>
        <li><a href="#machine-learning-">Initialization</a></li>
        <li><a href="#machine-learning-">Regularization</a></li>
        <li><a href="#machine-learning-">Optimization</a></li>
        <li><a href="#machine-learning-">Learning rate</a></li>
        <!-- <li><a href="#machine-learning-">Engineering-data</a> (out-of-scope)</li>
        <li><a href="#machine-learning-">Engineering-discriminative</a> (out-of-scope)</li>
        <li><a href="#machine-learning-">Engineering-generative</a> (out-of-scope)</li>
        <li><a href="#machine-learning-">Engineering-non-functional</a> (out-of-scope)</li>
        <li><a href="#machine-learning-">Engineering-deployment</a> (out-of-scope)</li>
        <li><a href="#machine-learning-">Engineering-cache</a> (out-of-scope)</li>
        <li><a href="#machine-learning-">Engineering-load-balancing</a> (out-of-scope)</li>
        <li><a href="#machine-learning-">Engineering-distributed-system</a> (out-of-scope)</li>
        <li><a href="#machine-learning-">Engineering-recommendation-system</a> (out-of-scope)</li> -->
      </ul>
      <li>ML coding <strong>(in-scope)</strong></li>
      <ul>
        <li>Linear regression</li>
        <li>Logistic regression</li>
        <li>K-nearest neighbors</li>
        <li>Decision trees</li>
        <li>K-means clustering</li>
        <li>Neural networks</li>
        <li>Transformer</li>
      </ul>
      <li>ML breath - specialized</li>
      <ul>
        <li><a href="#machine-learning-">Time-series and sequential data</a>  (out-of-scope)</li>
        <li><a href="#machine-learning-">Natural language processing</a> <strong>(in-scope)</strong></li>
        <li><a href="#machine-learning-">Computer vision</a>  (out-of-scope)</li>
        <li><a href="#machine-learning-">Reinforcement learning</a>  (out-of-scope)</li>
      </ul>
      <li>ML depth (out-of-scope)</li>
    </ul>
  </div>
</div>

<div class="card mb-4" id="machine-learning-">
  <div class="card-body">
    <h2 class="card-title">Binary classification</h2>
    <ul>
      <li>\( m \) training examples</li>
      <li>\( \textbf{X} = \begin{bmatrix} \vdots & \vdots & \vdots \\ \vdots & \vdots & \vdots \\ x_{1} & x_{2} \ldots & x_{m} \\ \vdots & \vdots & \vdots \\ \vdots & \vdots & \vdots \end{bmatrix} \)</li>
      <li>\( \textbf{y} = \begin{bmatrix} y_{1} & y_{2} \ldots & y_{m} \end{bmatrix} \)</li>
    </ul>

    <h3 class="card-title">Parameters</h3>
    <ul>
      <li>Number of features \( n^{[0]} \)</li>
      <li>Number of output units \( n^{[1]} = 1 \)</li>
    </ul>

    <h3 class="card-title">Dimensions</h3>
    <ul>
      <li>\( X \).shape = \( (n^{[0]}, m) \)</li>
      <li>\( y \).shape = \( (1, m) \)</li>
    </ul>

    <h3 class="card-title">Forward prop</h3>
    <ul>
      <li>\( \textbf{Z} = \begin{bmatrix} \textbf{z}_{1} & \textbf{z}_{2} \ldots & \textbf{z}_{m} \end{bmatrix} = \begin{bmatrix} \textbf{w}^{T}\textbf{x}_{1}+b & \textbf{w}^{T}\textbf{x}_{2}+b \ldots & \textbf{w}^{T}\textbf{x}_{m}+b \end{bmatrix} = \textbf{W}^{T}\textbf{X} + \begin{bmatrix} b & b \ldots & b \end{bmatrix} \)</li>
      <li>\( \textbf{A} = \begin{bmatrix} \textbf{a}_{1} & \textbf{a}_{2} \ldots & \textbf{a}_{m} \end{bmatrix} = \sigma(\textbf{Z}) = \sigma (\textbf{W}^{T}\textbf{X}+\textbf{b}) \)</li>
      <ul>
        <li>Without \( \sigma \), it is just linear regression</li>
      </ul>
    </ul>

<pre><code class="python">Z = np.dot(w.T, X) + b
A = sigmoid(Z)</code></pre>

    <h3 class="card-title">Cost function</h3>
    <ul>
      <li>Loss function \( L(\hat{y},y) = -\left( ylog(\hat{y}) + (1-y)log(1-\hat{y}) \right) \)</li>
      <ul>
        <li>This loss function is convex, thus gradient descent can find the global optimum</li>
        <ul>
          <li>For example, \( L(\hat{y},y) = \frac{1}{2}(\hat{y}-y)^{2} \) something like this is not convex</li>
        </ul>
        <li>If \( y = 1 \), \( L(\hat{y}, y) = -log\hat{y} \)</li>
        <ul>
          <li>We want \( \hat{y} \) large as possible ( \( y \approx 1 \) )</li>
        </ul>
        <li>If \( y = 0 \), \( L(\hat{y}, y) = -log(1-\hat{y}) \)</li>
        <ul>
          <li>We want \( \hat{y} \) small as possible ( \( y \approx 0 \) )</li>
        </ul>
      </ul>
      <li>Cost function \( J = \dfrac{1}{m}\displaystyle\sum_{i=1}^{m} L(\hat{y}_{i},y_{i}) = -\dfrac{1}{m}\displaystyle\sum_{i=1}^{m} \left(y_{i}\log(\textbf{a}_{i}) + (1-y_{i})\log(1-\textbf{a}_{i})\right) \)</li>
    </ul>

<pre><code class="python">m = X.shape[1]
cost = -(np.dot(Y, np.log(A.T)) + np.dot((1-Y), np.log(1-A.T))) / m
cost = np.squeeze(cost)</code></pre>

    <h3 class="card-title">Backward prop</h3>
    <ul>
      <li>\( \textbf{dZ} = \begin{bmatrix} \textbf{dz}_{1} & \textbf{dz}_{2} \ldots & \textbf{dz}_{m} \end{bmatrix} = \begin{bmatrix} \textbf{a}_{1}-y_{1} & \textbf{a}_{2}-y_{2} \ldots & \textbf{a}_{m}-y_{m} \end{bmatrix} = \textbf{A} - \textbf{y} \)</li>
      <li>\( \textbf{dW} = \dfrac{1}{m}\textbf{X}\textbf{dZ}^{T} \)</li>
      <li>\( \textbf{db} = \dfrac{1}{m}np.sum(\textbf{dZ}) \)</li>
      <li>\( \textbf{W} := \textbf{W} - \alpha \textbf{dW} \)</li>
      <li>\( \textbf{b} := \textbf{b} - \alpha \textbf{db} \)</li>
    </ul>

    <h3 class="card-title">Backward prop derivation (consider m=1 and n=2)</h3>
    <ul>
      <li>Let</li>
      <ul>
        <li>\( \hat{y} = a, L(a,y) = -\left( ylog(a) + (1-y)log(1-a) \right) \)</li>
        <li>\( z = w_{1}x_{1} + w_{2}x_{2} + b \)</li>
      </ul>
      <li>\( da = \dfrac{dL}{da} = -\dfrac{y}{a} + \dfrac{1-y}{1-a} \)</li>
      <li>\( dz = \dfrac{dL}{dz} = \dfrac{dL}{da}\dfrac{da}{dz} =  \left(-\dfrac{y}{a} + \dfrac{1-y}{1-a}\right) a(1-a) = a - y \)</li>
      <li>Then</li>
      <ul>
        <li>\( dw_{1} = x_{1}dz, dw_{2} = x_{2}dz \)</li>
        <li>\( db = dz \)</li>
      </ul>
    </ul>

<pre><code class="python">m = X.shape[1]
dZ = A - Y
dw = np.dot(X, (dZ).T) / m
db = np.sum(dZ) / m</code></pre>

    <h3 class="card-title">Full implementation</h3>

<pre><code class="python">def propagate(w, b, X, Y):

    # Forward prop
    m = X.shape[1]
    Z = np.dot(w.T, X) + b
    A = sigmoid(Z)

    # Compute cost
    cost = -(np.dot(Y, np.log(A.T)) + np.dot((1-Y), np.log(1-A.T))) / m
    cost = np.squeeze(cost)

    # Backward prop
    dZ = A - Y
    dw = np.dot(X, (dZ).T) / m
    db = np.sum(dZ) / m

    grads = {"dw": dw, "db": db}

    return grads, cost</code></pre>

<pre><code class="python">def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):

    for i in range(num_iterations):

        grads, cost = propagate(w, b, X, Y)

        dw = grads["dw"]
        db = grads["db"]

        w = w - learning_rate * dw
        b = b - learning_rate * db

    params = {"w": w, "b": b}
    grads = {"dw": dw, "db": db}

    return params, grads, costs</code></pre>

<pre><code class="python">def predict(w, b, X):

    m = X.shape[1]
    Y_prediction = np.zeros((1,m))
    w = w.reshape(X.shape[0], 1)

    A = sigmoid(np.dot(w.T, X) + b)

    for i in range(A.shape[1]):

        # Convert probabilities A[0,i] to actual predictions p[0,i]
        if A[0, i] <= 0.5:
            Y_prediction[0, i] = 0
        else:
            Y_prediction[0, i] = 1

    return Y_prediction</code></pre>

<pre><code class="python">def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):

    w, b = initialize_with_zeros(X_train.shape[0])
    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost = False)

    w = parameters["w"]
    b = parameters["b"]

    Y_prediction_test = predict(w, b, X_test)
    Y_prediction_train = predict(w, b, X_train)

    d = {"costs": costs,
         "Y_prediction_test": Y_prediction_test,
         "Y_prediction_train" : Y_prediction_train,
         "w" : w,
         "b" : b,
         "learning_rate" : learning_rate,
         "num_iterations": num_iterations}

    return d</code></pre>

<pre><code class="python"># Load the data (cat/non-cat)
train_set_location = 'data/train_catvnoncat.h5'
test_set_location = 'data/test_catvnoncat.h5'
train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset(train_set_location, test_set_location)

# Reshape the training and test examples such that matrices are flattened into vectors
train_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[0], -1).T
test_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[0], -1).T

# Center and standardize dataset dividing by the maximum value of a pixel channel
train_set_x = train_set_x_flatten / 255
test_set_x = test_set_x_flatten / 255

d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost = True)</code></pre>

    <h3 class="card-title">Helper functions</h3>

<pre><code class="python">def load_dataset(train_set_location, test_set_location):

    train_dataset = h5py.File(train_set_location, "r")
    # These data sets are to be pre-preprocessed (Thus, appended "_orig")
    train_set_x_orig = np.array(train_dataset["train_set_x"][:])  # Train set features
    train_set_y_orig = np.array(train_dataset["train_set_y"][:])  # Train set labels

    test_dataset = h5py.File(test_set_location, "r")
    # These data sets are to be pre-preprocessed (Thus, appended "_orig")
    test_set_x_orig = np.array(test_dataset["test_set_x"][:])  # Test set features
    test_set_y_orig = np.array(test_dataset["test_set_y"][:])  # Test set labels

    classes = np.array(test_dataset["list_classes"][:])  # List of classes

    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))
    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))

    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes</code></pre>

<pre><code class="python">def initialize_with_zeros(dim):

    w = np.zeros((dim, 1))
    b = 0

    return w, b</code></pre>

<pre><code class="python">def sigmoid(z):

    s = 1 / (1 + np.exp(-z))

    return s</code></pre>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://www.coursera.org/specializations/deep-learning">Deep Learning Specialization</a>
  </div>
</div>

<div class="card mb-4" id="machine-learning-">
  <div class="card-body">
    <h2 class="card-title">Decision tree</h2>
    <ul>
      <li>Most commonly used XGBoost base learner</li>
      <li>Splits data by asking questions about columns</li>
      <li>Training set can technically have 100% accuracy but such will have overfitting problem</li>
    </ul>

    <h3 class="card-title">Gini index</h3>
    <ul>
      <li>Represents error</li>
      <li>\( \text{gini} = 1 - \displaystyle\sum_{i=1}^{c} (p_{i})^{2} \)</li>
      <ul>
        <li>\( p_{i} \) is probability that split results in the correct value</li>
        <li>\( c \) is total number of classes</li>
      </ul>
      <li>1 means all errors</li>
      <li>0 means no errors</li>
      <li>0.5 means prediction is no better than random guessing</li>
    </ul>

<pre><code class="python">import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')
from sklearn.model_selection import train_test_split

df_census = pd.read_csv('census_cleaned.csv')
X = df_census.iloc[:,:-1]
y = df_census.iloc[:,-1]
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2)</code></pre>

<pre><code class="python">from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

# Initialize classification model
clf = DecisionTreeClassifier(random_state=2)

# Fit model on training data
clf.fit(X_train, y_train)

# Make predictions for test data
y_pred = clf.predict(X_test)

# Calculate accuracy
accuracy_score(y_pred, y_test)</code></pre>

    <h3 class="card-title">Hyperparameters</h3>
    <ul>
      <li>GridSearchCV</li>
      <ul>
        <li>Search hyperparameters using cross-validation</li>
        <li><pre><code class="python">from sklearn.model_selection import GridSearchCV

def grid_search(params, reg=DecisionTreeRegressor(random_state=2)):

    # Instantiate GridSearchCV as grid_reg
    grid_reg = GridSearchCV(reg, params, scoring='neg_mean_squared_error', cv=5, n_jobs=-1)

    # Fit grid_reg on X_train and y_train
    grid_reg.fit(X_train, y_train)

    # Extract best params
    best_params = grid_reg.best_params_

    # Print best params
    print("Best params:", best_params)

    # Compute best score
    best_score = np.sqrt(-grid_reg.best_score_)

    # Print best score
    print("Training score: {:.3f}".format(best_score))

    # Predict test set labels
    y_pred = grid_reg.predict(X_test)

    # Compute rmse_test
    rmse_test = mean_squared_error(y_test, y_pred)**0.5

    # Print rmse_test
    print('Test score: {:.3f}'.format(rmse_test))</code></pre></li>
      </ul>
      <li><code>max_depth</code></li>
      <ul>
        <li>Depth of tree, determined by the number of splits</li>
        <li><pre><code class="python">grid_search(params={'max_depth':[None,2,3,4,6,8,10,20]})
# Best params: {'max_depth': 6}
# Training score: 951.398
# Test score: 864.670</code></pre></li>
      </ul>
      <li><code>min_samples_leaf</code></li>
      <ul>
        <li>Minimum number of samples that a leaf must have</li>
        <li><pre><code class="python">grid_search(params={'min_samples_leaf':[1,2,4,6,8,10,20,30]})
# Best params: {'min_samples_leaf': 8}
# Training score: 896.083
# Test score: 855.620</code></pre></li>
        <li>Combine <code>max_depth</code> and <code>min_samples_leaf</code></li>
        <li><pre><code class="python">grid_search(params={'max_depth':[None,2,3,4,6,8,10,20],'min_samples_leaf':[1,2,4,6,8,10,20,30]})
# Best params: {'max_depth': 6, 'min_samples_leaf': 2}
# Training score: 870.396
# Test score: 913.000</code></pre></li>
        <li>Test score has increased. Try limiting <code>max_depth</code> to values greater than 3</li>
        <li><pre><code class="python">grid_search(params={'max_depth':[None,2,3,4,6,8,10,20],'min_samples_leaf':[1,2,4,6,8,10,20,30]})
# Best params: {'max_depth': 9, 'min_samples_leaf': 7}
# Training score: 888.905
# Test score: 878.538</code></pre></li>
      </ul>
      <li><code>max_leaf_nodes</code></li>
      <ul>
        <li>Maximum total number of leaves</li>
      </ul>
      <li><code>max_features</code></li>
      <ul>
        <li>Instead of considering every possible feature for a split, chooese from a set of features each round</li>
        <li><code>auto</code> - no limitation (default option)</li>
        <li><code>sqrt</code> - square root of total number of features</li>
        <li><code>log2</code> - for example, 32 columns resolve to 5 (2^5 = 32)</li>
      </ul>
      <li><code>min_samples_split</code></li>
      <ul>
        <li>Number of samples need to be present before a split can be made</li>
        <li>2 by default</li>
      </ul>
      <li><code>splitter</code></li>
      <ul>
        <li>How to select feature to split each branch</li>
        <li><code>best</code> - selects feature that result in greatest gain of information</li>
        <li><code>random</code> - recommended choice for preventing overfitting</li>
      </ul>
      <li><code>criterion</code></li>
      <ul>
        <li>For each possible split, calculates a number for a possible split and compares it to other options</li>
        <li>Split with the best score wins</li>
        <li>Regression</li>
        <ul>
          <li><code>mse</code> - default option</li>
          <li><code>friedman_mse</code></li>
          <li><code>mae</code></li>
        </ul>
        <li>Classification</li>
        <ul>
          <li><code>gini</code></li>
          <li><code>entropy</code></li>
        </ul>
      </ul>
      <li><code>min_impurity_decrease</code></li>
      <ul>
        <li>Split is made when impurity is >= min_impurity_decreas</li>
        <li>Tree with 100% accuracy has impurity 0.0. Tree with 80% accuracy has impurity 0.2</li>
        <li>Throughout tree building, impurity decreases</li>
        <li>Split that results in greatest decrease of impurity is chosen for each node</li>
        <li>0.0 by default</li>
      </ul>
      <li><code>min_weight_fraction_leaf</code></li>
      <li><code>ccp_alpha</code></li>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: Hands-On Gradient Boosting with XGBoost and scikit-learn, Corey Wade
  </div>
</div>

<div class="card mb-4" id="machine-learning-">
  <div class="card-body">
    <h2 class="card-title">Decision tree - heart disease</h2>

<pre><code class="python">df_heart = pd.read_csv('heart_disease.csv')
X = df_heart.iloc[:,:-1]
y = df_heart.iloc[:,-1]
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2)</code></pre>

    <h3 class="card-title">Baseline model</h3>

<pre><code class="python"># Initialize Decision Tree Classifier
model = DecisionTreeClassifier(random_state=2)

# Obtain scores of cross-validation
scores = cross_val_score(model, X, y, cv=5)

# Display accuracy
print('Accuracy:', np.round(scores, 2))

# Display mean accuracy
print('Accuracy mean: %0.2f' % (scores.mean()))

# Accuracy: [0.74 0.85 0.77 0.73 0.7 ]
# Accuracy mean: 0.76</code></pre>

    <h3 class="card-title">RandomizedSearchCV</h3>
    <ul>
      <li>Instead of trying all hyperparameters (GridSearchCV), try random number of combinations</li>
    </ul>

<pre><code class="python">from sklearn.model_selection import RandomizedSearchCV

def randomized_search_clf(params, runs=20, clf=DecisionTreeClassifier(random_state=2)):

    # Instantiate GridSearchCV as grid_reg
    rand_clf = RandomizedSearchCV(clf, params, n_iter=runs,
                                  cv=5, n_jobs=-1, random_state=2)

    # Fit grid_reg on X_train and y_train
    rand_clf.fit(X_train, y_train)

    # Extract best estimator
    best_model = rand_clf.best_estimator_

    # Extract best score
    best_score = rand_clf.best_score_

    # Print best score
    print("Training score: {:.3f}".format(best_score))

    # Predict test set labels
    y_pred = best_model.predict(X_test)

    # Compute accuracy
    accuracy = accuracy_score(y_test, y_pred)

    # Print accuracy
    print('Test score: {:.3f}'.format(accuracy))

    # Return best model
    return best_model</code></pre>

    <h3 class="card-title">Initial search</h3>

<pre><code class="python">randomized_search_clf(params={'criterion':['entropy', 'gini'],
    'splitter':['random', 'best'],
    'min_weight_fraction_leaf':[0.0, 0.0025, 0.005, 0.0075, 0.01],
    'min_samples_split':[2, 3, 4, 5, 6, 8, 10],
    'min_samples_leaf':[1, 0.01, 0.02, 0.03, 0.04],
    'min_impurity_decrease':[0.0, 0.0005, 0.005, 0.05, 0.10, 0.15, 0.2],
    'max_leaf_nodes':[10, 15, 20, 25, 30, 35, 40, 45, 50, None],
    'max_features':['auto', 0.95, 0.90, 0.85, 0.80, 0.75, 0.70],
    'max_depth':[None, 2,4,6,8],
    'min_weight_fraction_leaf':[0.0, 0.0025, 0.005, 0.0075, 0.01, 0.05]
})

# Training score: 0.798
# Test score: 0.855
# DecisionTreeClassifier(criterion='entropy', max_depth=8, max_features=0.8,
#                        max_leaf_nodes=45, min_samples_leaf=0.04,
#                        min_samples_split=10, min_weight_fraction_leaf=0.05,
#                        random_state=2)</code></pre>

    <h3 class="card-title">Narrowed search</h3>

<pre><code class="python">randomized_search_clf(params={'max_depth':[None, 6, 7],
    'max_features':['auto', 0.78],
    'max_leaf_nodes':[45, None],
    'min_samples_leaf':[1, 0.035, 0.04, 0.045, 0.05],
    'min_samples_split':[2, 9, 10],
    'min_weight_fraction_leaf': [0.0, 0.05, 0.06, 0.07],
},
runs=100)

# Training score: 0.802
# Test score: 0.868
# DecisionTreeClassifier(max_depth=7, max_features=0.78, max_leaf_nodes=45,
#                        min_samples_leaf=0.045, min_samples_split=9,
#                        min_weight_fraction_leaf=0.06, random_state=2)</code></pre>

    <h3 class="card-title">Check against baseline model</h3>

<pre><code class="python"># Initialize Decision Tree Classifier
model = DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=7,
    max_features=0.78, max_leaf_nodes=45,
    min_impurity_decrease=0.0, min_impurity_split=None,
    min_samples_leaf=0.045, min_samples_split=9,
    min_weight_fraction_leaf=0.06, presort=False, random_state=2,
    splitter='best')

# Obtain scores of cross-validation
scores = cross_val_score(model, X, y, cv=5)

# Display accuracy
print('Accuracy:', np.round(scores, 2))

# Display mean accuracy
print('Accuracy mean: %0.2f' % (scores.mean()))

# Accuracy: [0.82 0.9  0.8  0.8  0.78]
# Accuracy mean: 0.82</code></pre>

    <h3 class="card-title">Feature importance</h3>

<pre><code class="python">import operator

best_clf = DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=7,
    max_features=0.78, max_leaf_nodes=45,
    min_impurity_decrease=0.0, min_impurity_split=None,
    min_samples_leaf=0.045, min_samples_split=9,
    min_weight_fraction_leaf=0.06, presort=False,
    random_state=2, splitter='best')
best_clf.fit(X, y)

# Zip columns and feature_importances_ into dict
feature_dict = dict(zip(X.columns, best_clf.feature_importances_))

# Sort dict by values (as list of tuples)
sorted(feature_dict.items(), key=operator.itemgetter(1), reverse=True)[0:3]

# [('cp', 0.4840958610240171),
# ('thal', 0.20494445570568706),
# ('ca', 0.18069065321397942)]</code></pre>
  </div>
  <div class="card-footer text-muted">
    Reference: Hands-On Gradient Boosting with XGBoost and scikit-learn, Corey Wade
  </div>
</div>

<div class="card mb-4" id="machine-learning-">
  <div class="card-body">
    <h2 class="card-title">Random forest</h2>
    <ul>
      <li>Bagging - aggregate predictions of bootstrapped decision trees</li>
      <li>Limitation - if all individual trees make the same mistake, random forest makes the mistake</li>
    </ul>

<pre><code class="python">import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')

df_census = pd.read_csv('census_cleaned.csv')
X_census = df_census.iloc[:,:-1]
y_census = df_census.iloc[:,-1]</code></pre>

<pre><code class="python">from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score

# Initialize the classifier
rf = RandomForestClassifier(n_estimators=10, random_state=2, n_jobs=-1)

# Obtain scores of cross-validation
scores = cross_val_score(rf, X_census, y_census, cv=5)</code></pre>

    <h3 class="card-title">Hyperparameters</h3>
    <ul>
      <li>Decision tree parameters are not as significant because random forest cut down on variance by design</li>
      <li><code>oob_score</code></li>
      <ul>
        <li>Use samples that are not chosen during bagging as test set</li>
        <li>True by default</li>
        <li><pre><code class="python">rf = RandomForestClassifier(oob_score=True, n_estimators=10, random_state=2, n_jobs=-1)
rf.fit(X_census, y_census)
rf.oob_score_  # Ouputs test score</code></pre></li>
      </ul>
      <li><code>n_estimators</code></li>
      <ul>
        <li>Number of trees</li>
        <li>100 by default</li>
      </ul>
      <li><code>warm_start</code></li>
      <ul>
        <li>Adding more trees does not require starting from scratch</li>
        <li>Could be used to plot various score with a range of <code>n_estimators</code></li>
        <li><pre><code class="python">import matplotlib.pyplot as plt
import seaborn as sns

sns.set()
oob_scores = []

rf = RandomForestClassifier(n_estimators=50, warm_start=True, oob_score=True, n_jobs=-1, random_state=2)
rf.fit(X_census, y_census)
oob_scores.append(rf.oob_score_)
est = 50
estimators=[est]

for i in range(9):
    est += 50
    estimators.append(est)
    rf.set_params(n_estimators=est)
    rf.fit(X_census, y_census)
    oob_scores.append(rf.oob_score_)

plt.figure(figsize=(15,7))
plt.plot(estimators, oob_scores)
plt.xlabel('Number of Trees')
plt.ylabel('oob_score_')
plt.title('Random Forest Warm Start', fontsize=15)
plt.savefig('Random_Forest_Warm_Start', dpi=325)
plt.show()</code></pre></li>
      </ul>
      <li><code>bootstrap</code></li>
      <li><code>verbose</code></li>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: Hands-On Gradient Boosting with XGBoost and scikit-learn, Corey Wade
  </div>
</div>

<div class="card mb-4" id="machine-learning-">
  <div class="card-body">
    <h2 class="card-title">Random forest - bike rental</h2>

<pre><code class="python">from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X_bikes, y_bikes, random_state=2)</code></pre>

<pre><code class="python"># Initalize Random Forest as rf with 50 estimators, warm_start=True, and oob_score=True
rf = RandomForestRegressor(n_estimators=50, warm_start=True, n_jobs=-1, random_state=2)

# Obtain scores of cross-validation using num_splits and mean squared error
scores = cross_val_score(rf, X_bikes, y_bikes, scoring='neg_mean_squared_error', cv=10)

# Take square root of the scores
rmse = np.sqrt(-scores)

# Display accuracy
print('RMSE:', np.round(rmse, 3))

# Display mean score
print('RMSE mean: %0.3f' % (rmse.mean()))

# RMSE: [ 836.482  541.898  533.086  812.782  894.877  881.117  794.103  828.968  772.517  2128.148]
# RMSE mean: 902.398</code></pre>

    <h3 class="card-title">RandomizedSearchCV</h3>

<pre><code class="python">from sklearn.model_selection import RandomizedSearchCV

def randomized_search_reg(params, runs=16, reg=RandomForestRegressor(random_state=2, n_jobs=-1)):

    # Instantiate RandomizedSearchCV as grid_reg
    rand_reg = RandomizedSearchCV(reg, params, n_iter=runs, scoring='neg_mean_squared_error',
                                  cv=10, n_jobs=-1, random_state=2)

    # Fit grid_reg on X_train and y_train
    rand_reg.fit(X_train, y_train)

    # Extract best estimator
    best_model = rand_reg.best_estimator_

    # Extract best params
    best_params = rand_reg.best_params_

    # Print best params
    print("Best params:", best_params)

    # Compute best score
    best_score = np.sqrt(-rand_reg.best_score_)

    # Print best score
    print("Training score: {:.3f}".format(best_score))

    # Predict test set labels
    y_pred = best_model.predict(X_test)

    # Import mean_squared_error from sklearn.metrics as MSE
    from sklearn.metrics import mean_squared_error as MSE

    # Compute rmse_test
    rmse_test = MSE(y_test, y_pred)**0.5

    # Print rmse_test
    print('Test set score: {:.3f}'.format(rmse_test))</code></pre>

<pre><code class="python"></code></pre>
  </div>
  <div class="card-footer text-muted">
    Reference: Hands-On Gradient Boosting with XGBoost and scikit-learn, Corey Wade
  </div>
</div>

<div class="card mb-4" id="machine-learning-">
  <div class="card-body">
    <h2 class="card-title">Gradient boosting</h2>
    <ul>
      <li>AdaBoost</li>
      <ul>
        <li>Each new tree adjusts its weights based on errors from the previous trees</li>
      </ul>
      <li>Gradient boosting</li>
      <ul>
        <li>Fits each new tree entirely based on errors from the previous trees</li>
        <li>Computes the residuals of each tree's predictions and sums all the residuals to score the model</li>
      </ul>
    </ul>

<pre><code class="python">import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')
from sklearn.model_selection import train_test_split

df_bikes = pd.read_csv('bike_rentals_cleaned.csv')
X_bikes = df_bikes.iloc[:,:-1]
y_bikes = df_bikes.iloc[:,-1]
X_train, X_test, y_train, y_test = train_test_split(X_bikes, y_bikes, random_state=2)</code></pre>

    <h3 class="card-title">Hyperparameters</h3>
    <ul>
      <li><code>learning_rate</code></li>
      <ul>
        <li><pre><code class="python">n_estimators = [30, 300, 3000]
learning_rates = [0.001, 0.01, 0.05, 0.1, 0.15, 0.2, 0.3, 0.5, 1.0]
for i in n_estimators:
    for j in learning_rates:
        gbr = GradientBoostingRegressor(max_depth=2, n_estimators=i, random_state=2, learning_rate=i)
        gbr.fit(X_train, Y_train)
        y_pred = gbr.predict(X_test)
        rmse = MSE(y_test, y_pred) ** 0.5</code></pre></li>
      </ul>
      <li><code>max_depth</code></li>
      <li><code>subsample</code></li>
      <ul>
        <li>Each tree only select certain percentages of samples</li>
        <li>When subsample is not 1.0, it is considered as stochastic gradient descent</li>
      </ul>
      <li>RandomizedSearchCV</li>
      <ul>
        <li><pre><code class="python">from sklearn.model_selection import RandomizedSearchCV
params = {'subsample': [0.65, 0.7, 0.75], 'n_estimators': [300, 500, 1000], 'learning_rate': [0.05, 0.075, 0.1]}
gbr = GradientBoostingRegressor(max_depth=3, random_state=2)
rand_reg = RandomizedSearchCV(gbr, params, n_iter=10, scoring='neg_mean_squared_error', cv=5, n_jobs=1, random_state=2)
rand_reg.fit(X_train, Y_train)
best_model = rand_reg.best_estimator_
best_params = rand_reg.best_params_
best_score = np.sqrt(-rand_reg.best_score_)
y_pred = best_model.predict(X_test)
rmse_test = MSE(y_test, y_pred) ** 0.5</code></pre></li>
      </ul>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: Hands-On Gradient Boosting with XGBoost and scikit-learn, Corey Wade
  </div>
</div>

<div class="card mb-4" id="machine-learning-">
  <div class="card-body">
    <h2 class="card-title">XGBoost</h2>
    <ul>
      <li>When given a missing data point, XGBoost scores different split options and chooses the one with the best result</li>
      <li>Includes regularization as part of learning objective (As opposed to gradient boosting and random forest)</li>
    </ul>

    <h3 class="card-title">XGBoost features</h3>
    <ul>
      <li>Approximate split-finding algorithm</li>
      <ul>
        <li></li>
      </ul>
      <li>Sparsity aware split-finding</li>
      <ul>
        <li></li>
      </ul>
      <li>Parallel computing</li>
      <ul>
        <li></li>
      </ul>
      <li>Cache-aware access</li>
      <ul>
        <li></li>
      </ul>
      <li>Block compression and sharding</li>
      <ul>
        <li></li>
      </ul>
    </ul>

    <h3 class="card-title">Cost function</h3>
    <ul>
      <li>\( J = \displaystyle\sum_{i=1}^{n} (y_{i} - \hat{y}_{i})^{2} + \text{regularization} \)</li>
      <li>Prediction of ith tree = sum of the predictions of all previous trees + prediction for the new tree</li>
      <ul>
        <li>\( \hat{y}_{i}^{t} = \hat{y}_{i}^{t-1} + f_{t}(x_{i}) \)</li>
      </ul>
      <li>\( J = \displaystyle\sum_{i=1}^{n} \left(y_{i} - (\hat{y}_{i}^{t-1} + f_{t}(x_{i}))\right)^{2} + \text{regularization} \)</li>
      <li>\( J = \displaystyle\sum_{i=1}^{n} \left(y_{i} - \hat{y}_{i}^{t-1} - f_{t}(x_{i})\right)^{2} + \text{regularization} \)</li>
      <li>\( J = \displaystyle\sum_{i=1}^{n} (y_{i} - \hat{y}_{i}^{t-1})^{2} -2(y_{i}-\hat{y}_{i}^{t-1})f_{t}(x_{i}) + f_{t}(x_{i})^{2} + \text{regularization} \)</li>
      <li>Regularization</li>
      <ul>
        <li>\( \gamma T + \dfrac{1}{2} \lambda \displaystyle\sum_{i=1}^{T} w_{j}^{2} \)</li>
        <ul>
          <li>\( w \) - vector space of leaves</li>
        </ul>
      </ul>
    </ul>

    <h3 class="card-title">Hyperparameter</h3>
    <ul>
      <li><code>gamma</code></li>
      <ul>
        <li>Threshold that nodes must surpass before making further splits</li>
        <li>Increasing gamma results in more conservative model</li>
        <li>0 by default</li>
      </ul>
      <li><code>min_child_weight</code></li>
      <ul>
        <li>Minimum sum of weights required for a node to split into a child</li>
        <li>Reduces overfitting</li>
      </ul>
      <li><code>subsample</code></li>
      <ul>
        <li>Limits the percentage of training instances (rows) for each boosting round</li>
      </ul>
      <li><code>colsample_bytree</code></li>
      <ul>
        <li>Randomly selects particular columns according to given percentage</li>
      </ul>
    </ul>

    <h3 class="card-title">gblinear</h3>
    <ul>
      <li>XGBoost uses gbtree as base leanrer, which is optimal for non-linear data</li>
      <li>gblinear should be used as based learner for linear data</li>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: Hands-On Gradient Boosting with XGBoost and scikit-learn, Corey Wade
  </div>
</div>

<div class="card mb-4" id="machine-learning-">
  <div class="card-body">
    <h2 class="card-title">Test</h2>
    
  </div>
  <div class="card-footer text-muted">
    Reference: 
  </div>
</div>

<div class="card mb-4" id="machine-learning-">
  <div class="card-body">
    <h2 class="card-title">Test</h2>
    
  </div>
  <div class="card-footer text-muted">
    Reference: 
  </div>
</div>

<div class="card mb-4" id="machine-learning-">
  <div class="card-body">
    <h2 class="card-title">Test</h2>
    
  </div>
  <div class="card-footer text-muted">
    Reference: 
  </div>
</div>

<div class="card mb-4" id="machine-learning-">
  <div class="card-body">
    <h2 class="card-title">Test</h2>
    
  </div>
  <div class="card-footer text-muted">
    Reference: 
  </div>
</div>

<div class="card mb-4" id="machine-learning-">
  <div class="card-body">
    <h2 class="card-title">Test</h2>
    
  </div>
  <div class="card-footer text-muted">
    Reference: 
  </div>
</div>
<!-- Machine learning END -->

</div> <!-- /.col-md-12 -->
</div> <!-- /.row -->
</div> <!-- /.container -->

<include src="/footer.html"></include>

</body>

</html>