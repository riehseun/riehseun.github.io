<!DOCTYPE html>

<html lang="en">

<head>

<!-- Metadata -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="">
<meta name="author" content="Seungmoon Rieh">
<meta name="keywords" content="">

<!-- Title and image -->
<title>Seungmoon Rieh</title>
<link href="/img/seungmoonrieh.jpg" rel="icon">

<!-- CSS -->
<link href="/css/bootstrap.min.css" rel="stylesheet">
<link href="/css/monokai-sublime.css" rel="stylesheet">
<link href="/css/site.css" rel="stylesheet">

<!-- JavaScript -->
<script src="/js/jquery.min.js" type="text/javascript"></script>
<script src="/js/bootstrap.bundle.min.js" type="text/javascript"></script>
<script src="/js/highlight.pack.js" type="text/javascript"></script>
<script src="/js/include_html.js" type="text/javascript"></script>
<script src="/js/mathjax/tex-chtml.js" type="text/javascript"></script>
<script src="/js/site.js" type="text/javascript"></script>

</head>

<body>

<include src="/header.html"></include>

<div class="container">
<div class="row">
<div class="col-md-12">
<h1 class="my-4">Machine Learning</h1>

<!-- Decision tree BEGIN -->
<div class="card mb-4" id="subject">
  <div class="card-body">
    <h2 class="card-title">Decision tree</h2>
    <ul class="list-unstyled mb-0">
      <li><a href="#decision-tree-">Decision tree</a></li>
      <li><a href="#decision-tree-">Hyperparameters</a></li>
      <li><a href="#decision-tree-">Example - heart disease</a></li>
    </ul>
  </div>
</div>

<div class="card mb-4" id="decision-tree-">
  <div class="card-body">
    <h2 class="card-title">Decision tree</h2>
    <ul>
      <li>Most commonly used XGBoost base learner</li>
      <li>Splits data by asking questions about columns</li>
      <li>Training set can technically have 100% accuracy but such will have overfitting problem</li>
    </ul>

    <h3 class="card-title">Gini index</h3>
    <ul>
      <li>Represents error</li>
      <li>\( \text{gini} = 1 - \displaystyle\sum_{i=1}^{c} (p_{i})^{2} \)</li>
      <ul>
        <li>\( p_{i} \) is probability that split results in the correct value</li>
        <li>\( c \) is total number of classes</li>
      </ul>
      <li>1 means all errors</li>
      <li>0 means no errors</li>
      <li>0.5 means prediction is no better than random guessing</li>
    </ul>

<pre><code class="python">import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')

# Load dataset 'census_cleaned.csv'
df_census = pd.read_csv('census_cleaned.csv')

# Split data into X and y
X = df_census.iloc[:,:-1]
y = df_census.iloc[:,-1]

# Import train_test_split
from sklearn.model_selection import train_test_split

# Split data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2)</code></pre>

<pre><code class="python">from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

# Initialize classification model
clf = DecisionTreeClassifier(random_state=2)

# Fit model on training data
clf.fit(X_train, y_train)

# Make predictions for test data
y_pred = clf.predict(X_test)

# Calculate accuracy
accuracy_score(y_pred, y_test)</code></pre>
  </div>
  <div class="card-footer text-muted">
    Reference: Hands-On Gradient Boosting with XGBoost and scikit-learn, Corey Wade
  </div>
</div>

<div class="card mb-4" id="decision-tree-">
  <div class="card-body">
    <h2 class="card-title">Hyperparameters</h2>
    <ul>
      <li>GridSearchCV</li>
      <ul>
        <li>Search hyperparameters using cross-validation</li>
        <li><pre><code class="python">from sklearn.model_selection import GridSearchCV

def grid_search(params, reg=DecisionTreeRegressor(random_state=2)):

    # Instantiate GridSearchCV as grid_reg
    grid_reg = GridSearchCV(reg, params, scoring='neg_mean_squared_error', cv=5, n_jobs=-1)

    # Fit grid_reg on X_train and y_train
    grid_reg.fit(X_train, y_train)

    # Extract best params
    best_params = grid_reg.best_params_

    # Print best params
    print("Best params:", best_params)

    # Compute best score
    best_score = np.sqrt(-grid_reg.best_score_)

    # Print best score
    print("Training score: {:.3f}".format(best_score))

    # Predict test set labels
    y_pred = grid_reg.predict(X_test)

    # Compute rmse_test
    rmse_test = mean_squared_error(y_test, y_pred)**0.5

    # Print rmse_test
    print('Test score: {:.3f}'.format(rmse_test))</code></pre></li>
      </ul>
      <li><code>max_depth</code></li>
      <ul>
        <li>Depth of tree, determined by the number of splits</li>
        <li><pre><code class="python">grid_search(params={'max_depth':[None,2,3,4,6,8,10,20]})
# Best params: {'max_depth': 6}
# Training score: 951.398
# Test score: 864.670</code></pre></li>
      </ul>
      <li><code>min_samples_leaf</code></li>
      <ul>
        <li>Minimum number of samples that a leaf must have</li>
        <li><pre><code class="python">grid_search(params={'min_samples_leaf':[1,2,4,6,8,10,20,30]})
# Best params: {'min_samples_leaf': 8}
# Training score: 896.083
# Test score: 855.620</code></pre></li>
        <li>Combine <code>max_depth</code> and <code>min_samples_leaf</code></li>
        <li><pre><code class="python">grid_search(params={'max_depth':[None,2,3,4,6,8,10,20],'min_samples_leaf':[1,2,4,6,8,10,20,30]})
# Best params: {'max_depth': 6, 'min_samples_leaf': 2}
# Training score: 870.396
# Test score: 913.000</code></pre></li>
        <li>Test score has increased. Try limiting <code>max_depth</code> to values greater than 3</li>
        <li><pre><code class="python">grid_search(params={'max_depth':[None,2,3,4,6,8,10,20],'min_samples_leaf':[1,2,4,6,8,10,20,30]})
# Best params: {'max_depth': 9, 'min_samples_leaf': 7}
# Training score: 888.905
# Test score: 878.538</code></pre></li>
      </ul>
      <li><code>max_leaf_nodes</code></li>
      <ul>
        <li>Maximum total number of leaves</li>
      </ul>
      <li><code>max_features</code></li>
      <ul>
        <li>Instead of considering every possible feature for a split, chooese from a set of features each round</li>
        <li><code>auto</code> - no limitation (default option)</li>
        <li><code>sqrt</code> - square root of total number of features</li>
        <li><code>log2</code> - for example, 32 columns resolve to 5 (2^5 = 32)</li>
      </ul>
      <li><code>min_samples_split</code></li>
      <ul>
        <li>Number of samples need to be present before a split can be made</li>
        <li>2 by default</li>
      </ul>
      <li><code>splitter</code></li>
      <ul>
        <li>How to select feature to split each branch</li>
        <li><code>best</code> - selects feature that result in greatest gain of information</li>
        <li><code>random</code> - recommended choice for preventing overfitting</li>
      </ul>
      <li><code>criterion</code></li>
      <ul>
        <li>For each possible split, calculates a number for a possible split and compares it to other options</li>
        <li>Split with the best score wins</li>
        <li>Regression</li>
        <ul>
          <li><code>mse</code> - default option</li>
          <li><code>friedman_mse</code></li>
          <li><code>mae</code></li>
        </ul>
        <li>Classification</li>
        <ul>
          <li><code>gini</code></li>
          <li><code>entropy</code></li>
        </ul>
      </ul>
      <li><code>min_impurity_decrease</code></li>
      <ul>
        <li>Split is made when impurity is >= min_impurity_decreas</li>
        <li>Tree with 100% accuracy has impurity 0.0. Tree with 80% accuracy has impurity 0.2</li>
        <li>Throughout tree building, impurity decreases</li>
        <li>Split that results in greatest decrease of impurity is chosen for each node</li>
        <li>0.0 by default</li>
      </ul>
      <li><code>min_weight_fraction_leaf</code></li>
      <li><code>ccp_alpha</code></li>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: Hands-On Gradient Boosting with XGBoost and scikit-learn, Corey Wade
  </div>
</div>

<div class="card mb-4" id="decision-tree-">
  <div class="card-body">
    <h2 class="card-title">Example - heart disease</h2>

<pre><code class="python">df_heart = pd.read_csv('heart_disease.csv')

# Split data into X and y
X = df_heart.iloc[:,:-1]
y = df_heart.iloc[:,-1]

# Split data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2)</code></pre>

    <h3 class="card-title">Baseline model</h3>

<pre><code class="python"># Initialize Decision Tree Classifier
model = DecisionTreeClassifier(random_state=2)

# Obtain scores of cross-validation
scores = cross_val_score(model, X, y, cv=5)

# Display accuracy
print('Accuracy:', np.round(scores, 2))

# Display mean accuracy
print('Accuracy mean: %0.2f' % (scores.mean()))

# Accuracy: [0.74 0.85 0.77 0.73 0.7 ]
# Accuracy mean: 0.76</code></pre>

    <h3 class="card-title">RandomizedSearchCV</h3>
    <ul>
      <li>Instead of trying all hyperparameters (GridSearchCV), try random number of combinations</li>
    </ul>

<pre><code class="python">from sklearn.model_selection import RandomizedSearchCV

def randomized_search_clf(params, runs=20, clf=DecisionTreeClassifier(random_state=2)):

    # Instantiate GridSearchCV as grid_reg
    rand_clf = RandomizedSearchCV(clf, params, n_iter=runs,
                                  cv=5, n_jobs=-1, random_state=2)

    # Fit grid_reg on X_train and y_train
    rand_clf.fit(X_train, y_train)

    # Extract best estimator
    best_model = rand_clf.best_estimator_

    # Extract best score
    best_score = rand_clf.best_score_

    # Print best score
    print("Training score: {:.3f}".format(best_score))

    # Predict test set labels
    y_pred = best_model.predict(X_test)

    # Compute accuracy
    accuracy = accuracy_score(y_test, y_pred)

    # Print accuracy
    print('Test score: {:.3f}'.format(accuracy))

    # Return best model
    return best_model</code></pre>

    <h3 class="card-title">Initial search</h3>

<pre><code class="python">randomized_search_clf(params={'criterion':['entropy', 'gini'],
    'splitter':['random', 'best'],
    'min_weight_fraction_leaf':[0.0, 0.0025, 0.005, 0.0075, 0.01],
    'min_samples_split':[2, 3, 4, 5, 6, 8, 10],
    'min_samples_leaf':[1, 0.01, 0.02, 0.03, 0.04],
    'min_impurity_decrease':[0.0, 0.0005, 0.005, 0.05, 0.10, 0.15, 0.2],
    'max_leaf_nodes':[10, 15, 20, 25, 30, 35, 40, 45, 50, None],
    'max_features':['auto', 0.95, 0.90, 0.85, 0.80, 0.75, 0.70],
    'max_depth':[None, 2,4,6,8],
    'min_weight_fraction_leaf':[0.0, 0.0025, 0.005, 0.0075, 0.01, 0.05]
})

# Training score: 0.798
# Test score: 0.855
# DecisionTreeClassifier(criterion='entropy', max_depth=8, max_features=0.8,
#                        max_leaf_nodes=45, min_samples_leaf=0.04,
#                        min_samples_split=10, min_weight_fraction_leaf=0.05,
#                        random_state=2)</code></pre>

    <h3 class="card-title">Narrowed search</h3>

<pre><code class="python">randomized_search_clf(params={'max_depth':[None, 6, 7],
    'max_features':['auto', 0.78],
    'max_leaf_nodes':[45, None],
    'min_samples_leaf':[1, 0.035, 0.04, 0.045, 0.05],
    'min_samples_split':[2, 9, 10],
    'min_weight_fraction_leaf': [0.0, 0.05, 0.06, 0.07],
},
runs=100)

# Training score: 0.802
# Test score: 0.868
# DecisionTreeClassifier(max_depth=7, max_features=0.78, max_leaf_nodes=45,
#                        min_samples_leaf=0.045, min_samples_split=9,
#                        min_weight_fraction_leaf=0.06, random_state=2)</code></pre>

    <h3 class="card-title">Check against baseline model</h3>

<pre><code class="python"># Initialize Decision Tree Classifier
model = DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=7,
            max_features=0.78, max_leaf_nodes=45,
            min_impurity_decrease=0.0, min_impurity_split=None,
            min_samples_leaf=0.045, min_samples_split=9,
            min_weight_fraction_leaf=0.06, presort=False, random_state=2,
            splitter='best')

# Obtain scores of cross-validation
scores = cross_val_score(model, X, y, cv=5)

# Display accuracy
print('Accuracy:', np.round(scores, 2))

# Display mean accuracy
print('Accuracy mean: %0.2f' % (scores.mean()))

# Accuracy: [0.82 0.9  0.8  0.8  0.78]
# Accuracy mean: 0.82</code></pre>

    <h3 class="card-title">Feature importance</h3>

<pre><code class="python">import operator

best_clf = DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=7,
    max_features=0.78, max_leaf_nodes=45,
    min_impurity_decrease=0.0, min_impurity_split=None,
    min_samples_leaf=0.045, min_samples_split=9,
    min_weight_fraction_leaf=0.06, presort=False,
    random_state=2, splitter='best')
best_clf.fit(X, y)

# Zip columns and feature_importances_ into dict
feature_dict = dict(zip(X.columns, best_clf.feature_importances_))

# Sort dict by values (as list of tuples)
sorted(feature_dict.items(), key=operator.itemgetter(1), reverse=True)[0:3]

# [('cp', 0.4840958610240171),
# ('thal', 0.20494445570568706),
# ('ca', 0.18069065321397942)]</code></pre>
  </div>
  <div class="card-footer text-muted">
    Reference: Hands-On Gradient Boosting with XGBoost and scikit-learn, Corey Wade
  </div>
</div>
<!-- Decision tree END -->

</div> <!-- /.col-md-12 -->
</div> <!-- /.row -->
</div> <!-- /.container -->

<include src="/footer.html"></include>

</body>

</html>