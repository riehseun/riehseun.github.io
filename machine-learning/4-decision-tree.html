<!DOCTYPE html>

<html lang="en">

<head>

<!-- Metadata -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="">
<meta name="author" content="Seungmoon Rieh">
<meta name="keywords" content="">

<!-- Title and image -->
<title>Seungmoon Rieh</title>
<link href="/img/seungmoonrieh.jpg" rel="icon">

<!-- CSS -->
<link href="/css/bootstrap.min.css" rel="stylesheet">
<link href="/css/monokai-sublime.css" rel="stylesheet">
<link href="/css/site.css" rel="stylesheet">

<!-- JavaScript -->
<script src="/js/jquery.min.js" type="text/javascript"></script>
<script src="/js/bootstrap.bundle.min.js" type="text/javascript"></script>
<script src="/js/highlight.pack.js" type="text/javascript"></script>
<script src="/js/include_html.js" type="text/javascript"></script>
<script src="/js/mathjax/tex-chtml.js" type="text/javascript"></script>
<script src="/js/site.js" type="text/javascript"></script>

</head>

<body>

<include src="/header.html"></include>

<div class="container">
<div class="row">
<div class="col-md-12">
<h1 class="my-4">Machine Learning</h1>

<!-- Decision tree BEGIN -->
<div class="card mb-4" id="subject">
  <div class="card-body">
    <h2 class="card-title">Decision tree</h2>
    <ul class="list-unstyled mb-0">
      <li><a href="#attention-">Decision tree</a></li>
    </ul>
  </div>
</div>

<div class="card mb-4" id="attention-">
  <div class="card-body">
    <h2 class="card-title">Decision tree</h2>
    <ul>
      <li>Splits data by asking questions about columns</li>
      <li>Gini index</li>
      <ul>
        <li>Represents error</li>
        <li>\( \text{gini} = 1 - \displaystyle\sum_{i=1}^{c} (p_{i})^{2} \)</li>
        <ul>
          <li>\( p_{i} \) is probability that split results in the correct value</li>
          <li>\( c \) is total number of classes</li>
        </ul>
        <li>1 means all errors</li>
        <li>0 means no errors</li>
        <li>0.5 means prediction is no better than random guessing</li>
      </ul>
    </ul>

    <h3 class="card-title">Hyperparameters</h3>
    <ul>
      <li>max_depth</li>
      <ul>
        <li>Depth of tree, determined by the number of splits</li>
        <li><pre><code class="python">from sklearn.model_selection import GridSearchCV
params = {'max_depth': [None,2,3,4,6,8,10,20]}
reg = DecisionTreeRegressor(random_state=2)
grid_reg = GridSearchCV(reg, params, scoring='neg_mean_squared_error', cv=5, n_jobs=-1)
grid_reg.fit(X_train, Y_train)
best_params = grid_reg.best_params_</code></pre></li>
      </ul>
      <li>min_samples_leaf</li>
      <ul>
        <li>Minimum number of samples that a leaf must have</li>
      </ul>
      <li>max_leaf_nodes</li>
      <ul>
        <li>Maximum total number of leaves</li>
      </ul>
      <li>max_features</li>
      <ul>
        <li>Instead of considering every possible feature for a split, chooese from a set of features each round</li>
        <li><code>auto</code> - no limitation (default option)</li>
        <li><code>sqrt</code> - square root of total number of features</li>
        <li><code>log2</code> - for example, 32 columns resolve to 5 (2^5 = 32)</li>
      </ul>
      <li>min_samples_split</li>
      <ul>
        <li>Number of samples need to be present before a split can be made</li>
        <li>2 by default</li>
      </ul>
      <li>splitter</li>
      <ul>
        <li>How to select feature to split each branch</li>
        <li><code>best</code> - selects feature that result in greatest gain of information</li>
        <li><code>random</code> - recommended choice for preventing overfitting</li>
      </ul>
      <li>criterion</li>
      <ul>
        <li>For each possible split, calculates a number for a possible split and compares it to other options</li>
        <li>Split with the best score wins</li>
        <li>Regression</li>
        <ul>
          <li><code>mse</code> - default option</li>
          <li><code>friedman_mse</code></li>
          <li><code>mae</code></li>
        </ul>
        <li>Classification</li>
        <ul>
          <li><code>gini</code></li>
          <li><code>entropy</code></li>
        </ul>
      </ul>
      <li>min_impurity_decrease</li>
      <ul>
        <li>Split is made when impurity is >= min_impurity_decreas</li>
        <li>Tree with 100% accuracy has impurity 0.0. Tree with 80% accuracy has impurity 0.2</li>
        <li>Throughout tree building, impurity decreases</li>
        <li>Split that results in greatest decrease of impurity is chosen for each node</li>
        <li>0.0 by default</li>
      </ul>
      <li>min_weight_fraction_leaf</li>
      <li>ccp_alpha</li>
    </ul>

    <!-- <img class="img-fluid" class="card-img-top" src="/img/machine-learning/attention1.png" style="width: 500px; height: 500px" alt="Card image cap"> -->

<pre><code class="python"></code></pre>

  </div>
  <div class="card-footer text-muted">
    Reference: Hands-On Gradient Boosting with XGBoost and scikit-learn, Corey Wade
  </div>
</div>
<!-- Decision tree END -->

</div> <!-- /.col-md-12 -->
</div> <!-- /.row -->
</div> <!-- /.container -->

<include src="/footer.html"></include>

</body>

</html>