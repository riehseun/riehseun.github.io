<!DOCTYPE html>

<html lang="en">

<head>

<!-- Metadata -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="">
<meta name="author" content="Seungmoon Rieh">
<meta name="keywords" content="">

<!-- Title and image -->
<title>Seungmoon Rieh</title>
<link href="/img/seungmoonrieh.jpg" rel="icon">

<!-- CSS -->
<link href="/css/bootstrap.min.css" rel="stylesheet">
<link href="/css/monokai-sublime.css" rel="stylesheet">
<link href="/css/site.css" rel="stylesheet">

<!-- JavaScript -->
<script src="/js/jquery.min.js" type="text/javascript"></script>
<script src="/js/bootstrap.bundle.min.js" type="text/javascript"></script>
<script src="/js/highlight.pack.js" type="text/javascript"></script>
<script src="/js/include_html.js" type="text/javascript"></script>
<script src="/js/mathjax/tex-chtml.js" type="text/javascript"></script>
<script src="/js/site.js" type="text/javascript"></script>

</head>

<body>

<include src="/header.html"></include>

<div class="container">
<div class="row">
<div class="col-md-12">
<h1 class="my-4">Machine Learning</h1>

<!-- Model validation BEGIN -->
<div class="card mb-4" id="subject">
  <div class="card-body">
    <h2 class="card-title">Model validation</h2>
    <p class="card-text"></p>
    <ul class="list-unstyled mb-0">
      <li><a href="#model-validation-1">Bias and variance</a></li>
      <li><a href="#model-validation-2">Orthogonalization</a></li>
      <li><a href="#model-validation-3">Cross valdidation</a></li>
      <li><a href="#model-validation-4">Classification</a></li>
    </ul>
  </div>
</div>

<div class="card mb-4" id="model-validation-1">
  <div class="card-body">
    <h2 class="card-title">Bias and variance</h2>
    <ul>
      <li>Need to find right balance without overfitting or underfitting the data</li>
    </ul>

    <h3 class="card-title">Bias</h3>
    <ul>
      <li>Train set error</li>
      <li>How far off model prediction is from correct value</li>
      <li>Error from approximately true underlying function</li>
      <li>Difference between predicted and actual value</li>
    </ul>

    <h3 class="card-title">Variance</h3>
    <ul>
      <li>Dev set error</li>
      <li>Variability of model prediction for given data point</li>
      <li>Sensitivity to changes in training data</li>
      <li>Overfitting - model works well on training data, but doesn't generalize well on unseen data</li>
    </ul>

    <h3 class="card-title">Ex. election survey</h3>
    <ul>
      <li>Surveying from a phonebook is source of bias</li>
      <li>Small sample size is source of variance</li>
    </ul>

    <h3 class="card-title">Tackle bias</h3>
    <ul>
      <li>Bigger network</li>
      <li>Train longer</li>
    </ul>

    <h3 class="card-title">Tackle variance</h3>
    <ul>
      <li>Get more data</li>
      <li>Regularization</li>
    </ul>

    <!-- <h3 class="card-title">Why human-level performance</h3>
    <ul>
      <li>While ML is worse than human, you can</li>
      <ul>
        <li>Get labeled data from human.</li>
        <li>Gain insight from manual error analysis. (why did a person get this right?)</li>
        <li>Better analysis of bias/variance.</li>
      </ul>
    </ul> -->

    <!-- <h3 class="card-title">Avoidable bias</h3>
    <ul>
      <li>Human error as a proxy for bays error.</li>
      <li>Gap between human and training error: avoidable bias.</li>
      <li>Gap between training and dev error: variance.</li>
    </ul> -->

    <!-- <h3 class="card-title">Two fundamental assumptions of supervised learning</h3>
    <ul>
      <li>You can fit the training set pretty well ~ avoidable bias.</li>
      <li>Training set performance generalizes pretty well to dev/test set ~ variance.</li>
      <li>Increasing lambda decrease variance. Decreasing lambda decrease bias.</li>
      <li>More features decrease bias but increases variance. Less features decreases variance but increases bias.</li>
    </ul> -->

    <!-- <h3 class="card-title">Approaches</h3>
    <ul>
      <li>Linear model</li>
      <ul>
        <li>Regularization is used to decrease variance at the cost of increasing bias.</li>
      </ul>
      <li>Neural network</li>
      <ul>
        <li>Variance increases and bias decreases with number of hidden units. Regularization is used.</li>
      </ul>
      <li>K-nearest neighbor</li>
      <ul>
        <li>High K leads to high bias and low variance.</li>
      </ul>
      <li>Decision tree</li>
      <ul>
        <li>Depth of trees increases variance. Trees are pruned to control variance.</li>
      </ul>
    </ul> -->
  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://www.coursera.org/specializations/deep-learning?">Deep Learning Specialization</a>
  </div>
</div>

<div class="card mb-4" id="model-validation-2">
  <div class="card-body">
    <h2 class="card-title">Orthogonalization</h2>
    <ul>
      <li>Models are trained on train set</li>
      <ul>
      	<li>Bigger network/model</li>
      	<li>Better optimization algorithm</li>
        <li>Train longer</li>
        <li>Better neural network architecture</li>
        <li>Hyperparameter search</li>
      </ul>
      <li>Hyperpameters are selected on validation set</li>
      <ul>
      	<li>Regularization</li>
      	<li>Bigger training set</li>
        <li>Better neural network architecture</li>
        <li>Hyperparameter search</li>
      	<li>As we do this, we are leaking information from validation set to training set. We may end up overfitting to validation data</li>
      </ul>
      <li>Final evaluation is done on test set</li>
      <ul>
      	<li>Bigger dev set</li>
      	<li>Dev and test set must come from the same distribution!</li>
        <li>Test set may not be needed</li>
      </ul>
      <li>Perform in real world.</li>
      <ul>
      	<li>Change dev set</li>
      	<li>Change cost function</li>
      </ul>
    </ul>

    <h3 class="card-title">Training and dev/test on different distribution</h3>

    <h4 class="card-title">Example</h4>
    <ul>
      <li>Assume 200,000 is from web image and 10,000 is from mobile image</li>
      <li>Goal is to do image recognition on mobile device</li>
      <li>Then, use 2,500 mobile images for dev set and another 2,500 mobile images for test set</li>
      <li>Use 200,000 + 5,000 for the training set</li>
    </ul>

    <h4 class="card-title">Error analysis</h4>
    <ul>
      <li>If dev error >> training error, it could be because both variance and distribution mismatch</li>
      <li>Training-dev set: same distribution as training set, but not used in training</li>
      <li>If training-dev error >> training error, it is variance problem</li>
      <li>If dev error >> training-dev error, it is not a variance problem. It is data mismatch problem</li>
    </ul>

    <h4 class="card-title">Handle data mismatch</h4>
    <ul>
      <li>Make training data similar to dev/test set via synthesizing data</li>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://www.deeplearning.ai/deep-learning-specialization/">Deep Learning Specialization</a>
  </div>
</div>

<div class="card mb-4" id="model-validation-3">
  <div class="card-body">
    <h2 class="card-title">Cross valdidation</h2>

    <h3 class="card-title">K-fold cross validation</h3>
    <ul>
    	<li>Separate data into parts. Select one set as validation and all other sets as training. Repeat this process for all sets</li>
    	<li>Must not be used in time-series data</li>
    	<li>k in practice should be 4-5</li>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://github.com/alexeygrigorev/data-science-interviews/blob/master/theory.md">Theoretical interview questions</a>
  </div>
</div>

<div class="card mb-4" id="model-validation-4">
  <div class="card-body">
    <h2 class="card-title">Classification</h2>
    <ul>
      <li>Finite number of outputs</li>
      <li>Ex. logistic regression, decision tree, random forests</li>
    </ul>

    <h3 class="card-title">Imbalanced data in classification</h3>
    <ul>
      <li>Collect more data</li>
      <li>Undersample from over-represented class</li>
      <li>Change performance metric</li>
      <ul>
        <li>Accuracy is not the right metric to use when data is imbalanced</li>
        <li>Look at precision, recall, F1 score</li>
      </ul>
      <li>Data augmentation</li>
      <ul>
        <li>For example, crop/rotate images</li>
      </ul>
    </ul>

    <h3 class="card-title">Evaluate classification model</h3>
    <ul>
      <li>True Negative: ground truth was negative and prediction was negative</li>
      <li>True Positive: ground truth was positive and prediction was positive</li>
      <li>False Negative: ground truth was positive but prediction was negative</li>
      <li>False Positive: ground truth was negative but prediction was positive</li>
      <li>Confusion table shows TP, FP, TN, FN</li>
      <li>In perfectly separable data, both precision and recall can be 1</li>
      <li>But in real world, shifting decision boundary increase one but decrease the other</li>
      <li>Precision</li>
      <ul>
        <li>Correctness on predicted positive</li>
        <li>What percentage of positive predictions were correct?</li>
        <ul>
          <li>Ex. of examples recognized as cat, what % actually are cats?</li>
        </ul>
        <li>True Positive / (True Positive + False Positive)</li>
      </ul>
      <li>Recall</li>
      <ul>
        <li>Correctness of actual positive</li>
        <li>What percentage of positive cases did you catch?</li>
        <ul>
          <li>Ex. what % of actual cats are correctly recognized</li>
        </ul>
        <li>True Positive / (True Positive + False Negative)</li>
      </ul>
      <li>F1 score</li>
      <ul>
        <li>Average of precision and recall</li>
        <li>\( \dfrac{2}{\frac{1}{P} + \frac{1}{R}} \)</li>
      </ul>
      <li>Accuracy</li>
      <ul>
        <li>What percentage of predictions were correct?</li>
        <li>(True Positive + True Negative) / (True Negative + True Positive + False Negative + False Positive)</li>
      </ul>
      <li>False Positive Vs. False Negative</li>
      <ul>
        <li>In medical exam, False Negative is threatening to patients. Thus, False Positive is preferred</li>
        <li>In spam filtering, False Positive is annoying to users. Thus, False Negative is preferred</li>
      </ul>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href=""></a>
  </div>
</div>
<!-- Model validation END -->

</div> <!-- /.col-md-12 -->
</div> <!-- /.row -->
</div> <!-- /.container -->

<include src="/footer.html"></include>

</body>

</html>