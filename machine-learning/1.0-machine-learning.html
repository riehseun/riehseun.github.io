<!DOCTYPE html>

<html lang="en">

<head>

<!-- Metadata -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="">
<meta name="author" content="Seungmoon Rieh">
<meta name="keywords" content="">

<!-- Title and image -->
<title>Seungmoon Rieh</title>
<link href="/img/seungmoonrieh.jpg" rel="icon">

<!-- CSS -->
<link href="/css/bootstrap.min.css" rel="stylesheet">
<link href="/css/monokai-sublime.css" rel="stylesheet">
<link href="/css/site.css" rel="stylesheet">

<!-- JavaScript -->
<script src="/js/jquery.min.js" type="text/javascript"></script>
<script src="/js/bootstrap.bundle.min.js" type="text/javascript"></script>
<script src="/js/highlight.pack.js" type="text/javascript"></script>
<script src="/js/include_html.js" type="text/javascript"></script>
<script src="/js/mathjax/tex-chtml.js" type="text/javascript"></script>
<script src="/js/site.js" type="text/javascript"></script>

</head>

<body>

<include src="/header.html"></include>

<div class="container">
<div class="row">
<div class="col-md-12">
<h1 class="my-4">Machine Learning</h1>

<!-- Machine learning BEGIN -->
<div class="card mb-4" id="subject">
  <div class="card-body">
    <h2 class="card-title">Machine learning</h2>
    <ul class="list-unstyled mb-0">
      <li><a href="#machine-learning-">Machine learning</a></li>
      <ul>
        <!-- <li><a href="#machine-learning-">Statistics</a></li> -->
        <!-- <li><a href="#machine-learning-">Linear regression</a></li> -->
        <li><a href="#machine-learning-">Bias and variance</a></li>
        <li><a href="#machine-learning-">Classification</a></li>
        <!-- <li><a href="#machine-learning-">Dimensionality</a></li> -->
        <li><a href="#machine-learning-">Hyperparameter tuning</a></li>
        <li><a href="#machine-learning-">Orthogonalization</a></li>
        <!-- <li><a href="#machine-learning-">Recommender system</a></li> -->
        <!-- <li><a href="#machine-learning-">Tranfer learning</a></li> -->
        <!-- <li><a href="#machine-learning-">Multitask learning</a></li> -->
        <!-- <li><a href="#machine-learning-">Project workflow</a></li> -->
        <!-- <li><a href="#machine-learning-">Decision tree</a></li> -->
      </ul>
      <li><a href="#machine-learning-">Neural network</a></li>
      <ul>
        <li><a href="#machine-learning-">Logistic regression</a></li>
        <li><a href="#machine-learning-">Neural network</a></li>
        <li><a href="#machine-learning-">Initialization</a></li>
        <li><a href="#machine-learning-">Regularization</a></li>
        <li><a href="#machine-learning-">Normalization</a></li>
        <li><a href="#machine-learning-">Optimization</a></li>
      </ul>
      <!-- <li><a href="#machine-learning-">Computer vision</a></li>
      <ul>
        <li><a href="#machine-learning-">Convolutional neural network</a></li>
        <li><a href="#machine-learning-">Residual neural network</a></li>
        <li><a href="#machine-learning-">Object recognition</a></li>
        <li><a href="#machine-learning-">Neural style transfer</a></li>
        <li><a href="#machine-learning-">Inception network</a></li>
        <li><a href="#machine-learning-">Vision transformer</a></li>
      </ul> -->
      <li><a href="#machine-learning-">Natural language understanding</a></li>
      <ul>
        <!-- <li><a href="#machine-learning-">Recurrent neural network</a></li> -->
        <!-- <li><a href="#machine-learning-">Long short term memory</a></li> -->
        <li><a href="#machine-learning-">Word embedding</a></li>
        <li><a href="#machine-learning-">Transformer</a></li>
        <li><a href="#machine-learning-">BERT</a></li>
        <li><a href="#machine-learning-">RoBERTa</a></li>
        <li><a href="#machine-learning-">Machine translation</a></li>
        <li><a href="#machine-learning-">GPT-3</a></li>
      </ul>
      <li><a href="#machine-learning-">Tree</a></li>
      <ul>
        <li><a href="#machine-learning-">Decision tree</a></li>
        <li><a href="#machine-learning-">Random forest</a></li>
        <li><a href="#machine-learning-">Gradient boosting</a></li>
        <li><a href="#machine-learning-">XGBoost</a></li>
      </ul>
    </ul>
  </div>
</div>

<!-- <div class="card mb-4" id="machine-learning-">
  <div class="card-body">
    <h2 class="card-title">Machine learning</h2>
    <ul>
      <li>There are patterns to learn, which are complex</li>
      <li>Data is available or it is possible to collect data</li>
      <li>Learn pattern from data and use this knowledge to perform tasks</li>
    </ul>

    <h3 class="card-title">Types</h3>
    <ul>
      <li>Supervised - train with labeled data</li>
      <ul>
        <li>Regression - predict variables based on other (dependent) variables</li>
      </ul>
      <li>Unsupervised - detect patterns in data without labels</li>
      <ul>
        <li>Clustering - group objects such that objects in same group are similar and objects in different group are dissimilar</li>
      </ul>
      <li>Reinforcement - learn from repeated trial-and-error</li>
    </ul>

    <h3 class="card-title">Problems</h3>
    <ul>
      <li>Other</li>
      <ul>
        <li>Statistics</li>
        <ul>
          <li>Explain difference between Type I and Type II error</li>
          <ul>
            <li>Type 1 error - rejecting the null hypothesis when it is actually true</li>
            <li>Type 2 error - failing to reject the null hypothesis when it is actually false</li>
          </ul>
          <li>Explain correlation</li>
          <li>Explain correlation coefficient</li>
          <li>Explain Pearson’s correlation coefficient</li>
          <li>Explain Spearman’s correlation coefficient</li>
          <li>Explain central limit theorem</li>
          <li>Explain Hypothesis testing and p-value</li>
          <li>Explain statistical power</li>
          <ul>
            <li>Likelyhood that study will find effect when in fact there is effect</li>
            <li>Higher statistical power, less likely to make false negative</li>
          </ul>
          <li>Explain how to deal with anomaly<li>
          <ul>
            <li>68% of data is one std away</li>
            <li>95% of data is two std away</li>
            <li>9% of data is three std away</li>
            <li>Statistical method - consider data point with 3 std away as outlier and likely anomaly</li>
            <li>Metric method - a point is considered anomaly if removing it significantly improves the model</li>
          </ul>
        </ul>
        <li>CNN</li>
        <ul>
          <li>Explain how convolution works, when inputs are gray-scale or RGB</li>
          <li>Explain why we use convolution layer rather than just FC layers</li>
          <li>Explain why we use many small kernels like 3 by 3 rather than few large kernels</li>
          <li>Explain why we use max-pooling and how it is different from average pooling</li>
          <li>Explain ResNet and skip connection</li>
          <li>Explain non-max suppression</li>
          <li>Explain data augmentation</li>
          <li>Explain autoencoder</li>
        </ul>
        <li>Reinforcement learning</li>
        <ul>
          <li>Explain Markov Decision Process for reinforcement learning</li>
          <li>Explain main component of reinforcement learning agents</li>
          <li>Explain how to define reward function</li>
          <li>Explain difference between model-based and model-free reinforcement learning</li>
          <li>Explain the role of discount factor</li>
          <li>Explain exploration-exploitation trade-off</li>
          <li>Explain difference between policy gradient and value iteration method</li>
          <li>Explain State (Q) and Action-Value (Q) functions</li>
          <li>Explain how to handle continuous action space</li>
          <li>Explain deep reinforcement learning</li>
          <li>Explain how to ensure convergence of reinforcement learning</li>
          <li>Explain how multi-agent reinforcement learning system work</li>
        </ul>
        <li>Dimension reduction</li>
        <ul>
          <li>Explain how to tackle curse of dimensionality</li>
          <ul>
            <li>High dimensional data is extremely sparse</li>
            <li>It's hard to do machine learning on sparse data</li>
          </ul>
          <li>Explain principal component analysis</li>
          <ul>
            <li>Special type of SVD</li>
            <li>Left matrix and right matrix are eigenvectors</li>
            <li>Diagonal matrix is eigenvalues</li>
          </ul>
          <li>Explain t-SNE</li>
          <li>Explain sigular value decomposition</li>
          <ul>
            <li>Refactor a matrix into three pieces - left matrix, diagonal matrix, right matrix</li>
          </ul>
        </ul>
        <li>Recommendation system</li>
        <ul>
          <li>Explain collaborative filtering</li>
          <ul>
            <li>Recommendation is based on historical user-item interaction</li>
            <li>Cold start problem - cannot make recommendation for new item, cannot find similarity with other users for new user</li>
            <li>Item-based - rate an item based on ratings by users similar to current user</li>
            <li>User-based - rate an item based on similar items that current user rated</li>
          </ul>
          <li>Explain content-based filtering</li>
          <ul>
            <li>An approach to solve cold start problem</li>
            <li>Recommend items that are similar to items that user liked already</li>
            <li>Do not take other users into consideration</li>
          </ul>
        </ul>
      </ul>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: Designing Machine Learning Systems, Chip Huyen
  </div>
</div> -->

<div class="card mb-4" id="machine-learning-">
  <div class="card-body">
    <h2 class="card-title">MLP</h2>

    <h3 class="card-title">Machine learning</h3>
    <ul>
      <li>Explain how to explain machine learning to a kid</li>
      <ul>
        <li>Computers watch what others do, learn and copy that actions</li>
        <li>Computers learn from previous mistakes</li>
      </ul>
      <li>Explain difference between supervised, unsupervised, and reinforcement learning</li>
      <ul>
        <li>Supervised - data is labeld (regression/classification, NN/tree)</li>
        <li>Unsupervised - data is not labeled, learn pattern from data (clustering/dimensionaity-reduction)</li>
        <li>Reinforcement - learn to maximize reward via trial and error</li>
      </ul>
      <li>Explain regression algorithm</li>
      <ul>
        <li>Predict continuous variable from a set of features</li>
        <li>Ex. linear regression</li>
        <li>Evaluation - MSE (computation is simpler), MAE (more robust to outliers)</li>
      </ul>
      <li>Explain classification algorithm</li>
      <ul>
        <li>Predict category or class from a set of features</li>
        <li>Ex. logistic regression</li>
        <li>Evaluation - precision, recall, accuracy, F1</li>
        <li>Dataset is bernoulli (binary) or multinomial (multi-class)</li>
      </ul>
      <li>Explain why regression cannot be used for classification tasks</li>
      <ul>
        <li>Because regression outputs continuous variable</li>
      </ul>
      <li>Explain linear regression</li>
      <ul>
        <li>Learn coefficients from training data and inference using only independant variables</li>
        <li>Assume linear relationship between features and target</li>
        <li>Assume errors are normally distributed</li>
        <li>Assume data are independent</li>
      </ul>
      <li>Explain logistic regression</li>
      <ul>
        <li>Assume linear relationship between input features and log-odds of outputs</li>
        <li>Uses sigmoid/softmax</li>
      </ul>
      <li>Explain neural network algorithm</li>
      <ul>
        <li>Data is fed to input layer</li>
        <li>Each neuron in hidden layer calculates based on input, weight, and activation function</li>
        <li>Output layer generates prediction</li>
        <li>Loss (or cost, which is sum of losses) is computed using prediction and label</li>
        <li>Gradient of loss w.r.t. weights and bias (which minimizes the cost) are computed</li>
        <li>Weights and bias are updated using gradients</li>
        <li>This process iterates over many epoch</li>
      </ul>
      <li>Explain KNN</li>
      <ul>
        <li>From a point, find k closest number of points</li>
        <li>No training needed</li>
        <li>High K leads to high bias and low variance</li>
        <li>Classification - return majority vote of nearest neighbors</li>
        <li>Regression - return average value of nearest neighbors</li>
      </ul>
      <li>Explain ANN</li>
      <li>Explain clustering algorithm</li>
      <ul>
        <li>Unsupervised learning</li>
        <li>Group data points into clusters based on their similarity</li>
        <li>Ex. K-means, DBSCAN</li>
      </ul>
      <li>Explain k-means clustering</li>
      <ul>
        <li>Partition data into K clusters</li>
        <li>Randomly pick centroids of each cluster</li>
        <li>Assign points to closest cluster, then update centroid</li>
        <li>Repeat until convergence</li>
      </ul>
      <li>Explain convergence in k-means clustering</li>
      <ul>
        <li>Algorithm finished grouping data points into k clusters</li>
        <li>It is when the centroid of last two clusters are very similar</li>
      </ul>
      <li>Explain how to choose optimul number of clusters</li>
      <ul>
        <li>Elbow method - plot variation (within-cluster sum of squared errors) against number of clsuters, when the curve elbows, adding more cluster does not benefit</li>
        <li>Silhouette score - measures how similar an object is to its own cluster versus other clusters (iterate over different values of k to see which k results in best average score), more computationally expensive than elbow method</li>
      </ul>
    </ul>

    <h3 class="card-title">Feature engineering</h3>
    <ul>
      <li>Explain why we do feature selection</li>
      <ul>
        <li>Too many features can cause overfitting</li>
        <li>Become technical debt</li>
        <li>Slower inference if dynamic feature computation is needed</li>
        <li>Increase memory requirement</li>
      </ul>
      <li>Explain feature selection technique</li>
      <ul>
        <li>Filter - evaludate each feature with target one by one, but this ignores feature interactions (Ex. chi-squared test)</li>
        <li>Wrapper - try different combination of features (Ex. recursive feature elimination)</li>
        <li>Embedded - during training, model decides which features are important (Ex. LASSO regression)</li>
      </ul>
      <li>Explain SHAP (SHapley Additive exPlanations)</li>
      <ul>
        <li>Contribution of each feature to prediction</li>
        <li>Compute combination of features and their predictions</li>
      </ul>
      <li>Explain feature scaling</li>
      <ul>
        <li>Help gradient descent to find optimum faster</li>
        <li>Ex. standardization (z-score normalization) where mean is 0 and std is 1</li>
        <li>Assumes data is normallly distributed</li>
      </ul>
      <li>Explain how to encode categorical data</li>
      <ul>
        <li>Integer encoding - assign integer to each category</li>
        <li>One-hot encoding - dense features, assign binary vector to each category</li>
        <li>Embedding - sparse features, either learn vector for each categorical value or use pre-trained model</li>
      </ul>
      <li>Explain how to handle missing data</li>
      <ul>
        <li>Delete - data is lost</li>
        <li>Imputation - data gets noisy (supplyment with mean, median, etc)</li>
        <li>Use model that can handel missing data (Ex. XGBoost)</li>
      </ul>
      <li>Explain how to handle imbalanced dataset</li>
      <ul>
        <li>Undersample/oversample</li>
        <li>Data augmentation - crop/rotate images</li>
        <li>Loss function - higher weights to under-represented data</li>
        <li>Change performance metric - use precision, recall, F1 score rather than accuracy</li>
      </ul>
      <li>Explain how to avoid data leakage</li>
      <ul>
        <li>Split time-correlated data by time, not random (otherwise, information from future is leaked to training set)</li>
        <li>Split data first, then scaling (otherwise, statistics of test set can be leaked into training set)</li>
        <li>Do not fill missing data from statistics of test set</li>
        <li>Oversample data only after splitting data</li>
      </ul>
    </ul>

    <h3 class="card-title">Bias and variance</h3>
    <ul>
      <li>Explain bias-variance trade-off</li>
      <ul>
        <li>Challenge to fight both underfitting and overfitting the model to data</li>
        <li>High bias and low variance - linear regression</li>
        <li>Low bias and high variance - neural network</li>
      </ul>
      <li>Explain underfitting and how to tackle it</li>
      <ul>
        <li>High bias, training error, model does not work well on training data</li>
        <li>More data</li>
        <li>Better/bigger model</li>
        <li>Train longer</li>
      </ul>
      <li>Explain overfitting and how to tackle it</li>
      <ul>
        <li>High variance, validation error, model works well on training data but not well on unseen data</li>
        <li>More data</li>
        <li>Less features</li>
        <li>Reduce model complexity</li>
        <li>Regularization</li>
        <li>Make sure validation/test set come from same distribution</li>
      </ul>
      <li>Explain why we use validation and test set</li>
      <ul>
        <li>Validation set - used for hyparameters tunung, use data unseen in training set</li>
        <li>Test set - used to test model performance, use data unseen in training and validation set</li>
      </ul>
      <li>Explain cross-validation</li>
      <ul>
        <li>Split training set into different bins</li>
        <li>One bin is set aside as test set and all others as training set</li>
        <li>Train on training set using test set as label</li>
        <li>Average accuracies to compute the final accuracy score</li>
        <li>Base population must not co-exist between bins</li>
      </ul>
      <li>Explain why training loss might decreases while validation loss increases</li>
      <ul>
        <li>Overfitting - model works well predicting based on training data but not well on unseen (validation) data</li>
      </ul>
      <li>Explain how to debug model not performing well on test data</li>
      <ul>
        <li>Not enough data</li>
        <li>Data that is not representative</li>
        <li>Data with outlier, error, noise, missing values</li>
        <li>Irrelevant features</li>
      </ul>
      <li>Explain how to perfrom error analysis</li>
      <ul>
        <li>Training-dev set - same distribution as training set, but not used in training</li>
        <li>If dev error >> training error, it could be because both variance and distribution mismatch</li>
        <li>If training-dev error >> training error, it is variance problem</li>
        <li>If dev error >> training-dev error, it is not a variance problem. It is data mismatch problem</li>
      </ul>
    </ul>

    <h3 class="card-title">Classification</h3>
    <ul>
      <li>Explain confusion matrix</li>
      <ul>
        <li>Table showing predicted versus actual (true positive, false negative, false positive, true negative)</li>
      </ul>
      <li>Explain difference between precision and recall</li>
      <ul>
        <li>Precision - how often model is correct when predicting target class, true positive / (true positive + false positive)</li>
        <li>Recall - whether model can find target class, true positive / (true positive + false negative)</li>
      </ul>
      <li>Explain difference between PR-AUC and ROC-AUC</li>
      <ul>
        <li>ROC-AUC - true positive vs false positive, area beneath the curve (If 0.5, random guess, If 1, perfect model)</li>
        <li>PR-AUC - precision vs recall, area beneath the curve</li>
      </ul>
      <li>Explain accuracy</li>
      <ul>
        <li>What percentage of predictions were correct?</li>
        <li>(True Positive + True Negative) / (True Negative + True Positive + False Negative + False Positive)</li>
      </ul>
      <li>Explain F1 score</li>
      <ul>
        <li>Average of precision and recall</li>
      </ul>
      <li>Explain how to find threshold for classifier</li>
      <ul>
        <li>ROC-AUC</li>
        <li>PR-AUC</li>
      </ul>
      <li>Explain when to use which metric</li>
      <ul>
        <li>Precision - when missing target class is less critical than incorrectly labeling target class (Ex. spam detection)</li>
        <li>Recall - when incorrect lable is less important than missing target class (Ex. fraud detection)</li>
        <li>Both - (Ex. cancer detection)</li>
      </ul>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: 
  </div>
</div>

<div class="card mb-4" id="machine-learning-">
  <div class="card-body">
    <h2 class="card-title">NN</h2>

    <h3 class="card-title">Neural network</h3>
    <ul>
      <li>Explain cost function</li>
      <ul>
        <li>Loss is a function that computes difference between predicted and label</li>
        <li>Cost is sum of losses over all training batch</li>
      </ul>
      <li>Explain difference between epoch, batch, and iteration</li>
      <ul>
        <li>Epoch - number of times to loop through all training data</li>
        <li>Batch - amount of training data to make a single gradient update</li>
        <li>Iteration - number of times to train model through all epoch under a set of hyperparameters</li>
      </ul>
      <li>Explain difference between single-layer perception and multi-layer perception</li>
      <ul>
        <li>Single-layer perception - single layer network with linear activation, which is equivalent to linear regression</li>
        <li>Multi-layer percention - has at least one hidden layer</li>
      </ul>
      <li>Explain what is meant by depth in neural network</li>
      <ul>
        <li>Number of layers (input layer plus hidden layers, not counting output layer)</li>
      </ul>
      <li>Explain significance of bias term in neural network</li>
      <ul>
        <li>Without bias term, activation function curve always passes through origin</li>
        <li>Bias allows activation function to be shifted, making it more flexible to fit data well</li>
      </ul>
      <li>Explain challenges with very deep neural network</li>
      <ul>
        <li>Exploding and vanishing gradient - use gradient clipping</li>
        <li>Overfitting - use regularization</li>
        <li>Slow convergence - use adaptive learning rate and learning rate scheduler</li>
      </ul>
      <li>Explain why we shuffle training data after each epoch</li>
      <ul>
        <li>Model may learn pattern from ordering of data</li>
      </ul>
      <li>Explain hyperparameter tuning</li>
      <ul>
        <li>Hold out validation set</li>
        <li>Try combinations of hyperparameters and test model performance on validation set</li>
        <li>Scale parameters using log scale (Ex. 0.0001 to 1, use 0.0001, 0.001, 0.01, 0.1, 1)</li>
        <li>Ex. grid search, random search, bayesian optimization</li>
        <li>Iterate and pick the best combination</li>
      </ul>
      <li>Explain which hyperparameter are important</li>
      <ul>
        <li>Learning rate is most important</li>
        <li>Mini-batch size, the number of hidden units are second important</li>
        <li>The number of layers and learning rate decay are third important</li>
      </ul>
      <li>Explain logits</li>
      <ul>
        <li>Unnormalized output of model</li>
      </ul>
    </ul>

    <h3 class="card-title">Activation</h3>
    <ul>
      <li>Explain activation function</li>
      <ul>
        <li>Function that determines outputs of neuron</li>
        <li>Add non-linearity to allow network to learn more complex pattern</li>
      </ul>
      <li>Explain why ReLU is used more than sigmoid in neural network</li>
      <ul>
        <li>Sigmoid - function flattens near zero, leading to vanishing gradient</li>
        <li>ReLU - neuron does not active for negative input, thus addresses vanishing gradient</li>
      </ul>
    </ul>

    <h3 class="card-title">Normalization</h3>
    <ul>
      <li>Explain normalization and why we use it</li>
      <ul>
        <li>If features are not scaled, contours can be assymmetric, gradient descent can take long time</li>
        <li>If features are scaled, contours can be symmetric, gradient decent can find optimum faster</li>
      </ul>
      <li>Explain difference between normalization and standardization</li>
      <ul>
        <li>Normalization - values are between 0 and 1, does not change distribution</li>
        <li>Standardization - values have mean 0 and std 1, changes to normal distribution</li>
      </ul>
      <li>Explain difference between batch normalization and layer normalization</li>
      <ul>
        <li>Batch normalization - take a batch and normalize feature by feature</li>
        <li>Layer normalization - take all features and normalize data point by data point</li>
      </ul>
    </ul>

    <h3 class="card-title">Initialization</h3>
    <ul>
      <li>Explain weight initialization methods like Xavier, He</li>
      <ul>
        <li>He - initialize weights to small random values (If values are too large or too small, it can lead to exploding/vanish gradient)</li>
      </ul>
      <li>Explain what happens if weights are initialized to same values</li>
      <ul>
        <li>All neurons in each layer output the same value</li>
      </ul>
      <li>Explain weight constraints and how it can benefit training</li>
      <ul>
        <li>During training, if weights exceed certain value, keep it as threshold</li>
        <li>This helps preventing overfitting</li>
      </ul>
    </ul>

    <h3 class="card-title">Regularization</h3>
    <ul>
      <li>Explain regularization and why we use it</li>
      <ul>
        <li>For small Z, activation function becomes close to linear, and not become complex</li>
        <li>To have small Z, need small W, thus need lambda</li>
      </ul>
      <li>Explain difference between L1 and L2 regularization</li>
      <ul>
        <li>L1 - add sum of absolute weights to loss function, works well on sparse data, features can be removed</li>
        <li>L2 - add sum of squares of weights to loss function, works well when features are corelated</li>
      </ul>
      <li>Explain dropout</li>
      <ul>
        <li>Neurons are ramdonly shut down with probability p</li>
        <li>Neurons become reluctant to put too much weight on one feature</li>
        <li>Weight values can be smaller and more spread out</li>
      </ul>
      <li>Explain how early stopping prevents overfitting</li>
      <ul>
        <li>Stop training when validation error increases</li>
        <li>Tackle bias and variance at the same time (rather than one at a time)</li>
      </ul>
    </ul>

    <h3 class="card-title">Optimization</h3>
    <ul>
      <li>Explain batch size and how it is related to convergence</li>
      <ul>
        <li>Small batch - frequent and noisy gradient updates, can escape local optima well but too much osciallation and slow down convergence</li>
        <li>Large batch - stable gradient updates, may get stuck in local optima</li>
        <li>Balanced batch is needed</li>
      </ul>
      <li>Explain vanishing and exploding gradient</li>
      <ul>
        <li>Vanishing - gradient becomes very small</li>
        <li>Exploding - gradient becomes very large</li>
      </ul>
      <li>Explain how to deal with vanishing gradient</li>
      <ul>
        <li>ReLu</li>
        <li>Weight initialization</li>
        <li>Small learning rate</li>
        <li>Residual connection</li>
        <li>Batch/layer normalization</li>
      </ul>
      <li>Explain how to deal with exploding gradient</li>
      <ul>
        <li>Gradient clipping</li>
        <li>Weight initialization</li>
        <li>Small learning rate</li>
        <li>Residual connection</li>
        <li>Batch/layer normalization</li>
      </ul>
      <li>Explain momentum</li>
      <ul>
        <li>On vertical axis (b), we want slower learning to reduce oscilation</li>
        <li>On horizontal axis (W), we want faster learning</li>
        <li>Apply additional computation when updating W and b with velocity of W and b</li>
      </ul>
      <li>Explain RMSProp</li>
      <ul>
        <li>Similar idea from momentum, but different calculation</li>
      </ul>
      <li>Explain ADAM optimizer</li>
      <ul>
        <li>Combine calculations from momentum and RMSProp</li>
      </ul>
      <li>Explain difference between batch, min-batch, and stochastic gradient descent</li>
      <ul>
        <li>Batch - use all training data to make a single gradient update, stable but slow</li>
        <li>Min-batch - use subset of training data</li>
        <li>Stachastic - use a single data, fast but final parameters may not be optimal</li>
      </ul>
      <li>Explain difference between global optima and local optima</li>
      <ul>
        <li>Global - optimum value of function in its entire span</li>
        <li>Local - optimum value of function in its subrange</li>
      </ul>
      <li>Explain gradient clipping and how it help training</li>
      <ul>
        <li>If gradient exceed certain value, set it to threshold</li>
        <li>Prevents exploding gradient</li>
      </ul>
    </ul>

    <h3 class="card-title">Learning rate</h3>
    <ul>
      <li>Explain learning rate</li>
      <ul>
        <li>In gradient descent, size of step you want to take in particular direction</li>
      </ul>
      <li>Explain adaptive learning rate</li>
      <ul>
        <li>Bigger learning rate in the beginning and smaller near the optimum</li>
      </ul>
      <li>Explain learning rate scheduler</li>
      <ul>
        <li>Similar to adaptive learning rate, but uses pre-defined schedule</li>
        <li>Normally consists of warm up, constant hold, and exponential phases</li>
      </ul>
      <li>Explain why learning rate is considered as most important hyperparameter</li>
      <ul>
        <li>It directly controls how quickly model adjusts its weights during training</li>
      </ul>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: 
  </div>
</div>

<div class="card mb-4" id="machine-learning-">
  <div class="card-body">
    <h2 class="card-title">NLP</h2>

    <h3 class="card-title">NLP</h3>
    <ul>
      <li>Explain RNN</li>
      <ul>
        <li>Neural network that process words timestep by timestep</li>
        <li>Uses previous words to predict the next word</li>
      </ul>
      <li>Explain difference between feed forward network and recurrent network</li>
      <ul>
        <li>Feed forward - suitable for problems having single output, neurons have different weights</li>
        <li>RNN - suitable for sequential input data, all neurons in a layer have the same weights</li>
      </ul>
      <li>Explain LSTM</li>
      <ul>
        <li>RNN that captures long-term dependencies in sequential data</li>
        <li>Uses memory cells and gates to remember or forget information</li>
      </ul>
      <li>Explain how LSTM handle vanishing gradient</li>
      <ul>
        <li>Memory cells keep gradient consistant over long sequence</li>
      </ul>
      <li>Explain word embedding and how it is different from one-hot encoding</li>
      <ul>
        <li>Representing word by numerical vector such that words with similar meaning has close distance</li>
        <li>One-hot encoding can be high dimensional but embedding can be low dimensional</li>
      </ul>
      <li>Explain sentence embedding</li>
      <ul>
        <li>Tokenize sentence into words</li>
        <li>Compute word embeddings and aggregate</li>
      </ul>
      <li>Explain tokenization</li>
      <ul>
        <li>Split paragraphs or sentences into words (removing punctation, lower casing, etc)</li>
        <li>Rare words can be broken down into subwords while common words can be kept as they are</li>
        <li>High quality data (No abbrevation, spelling mistakes) is needed</li>
      </ul>
      <li>Explain Word2Vec</li>
      <ul>
        <li>NLP technique to learn embeddings of words</li>
        <li>For example, CBOW and skip-gram</li>
      </ul>
      <li>Explain CBOW</li>
      <ul>
        <li>Predict word by surrounding words, fill-in-the-blank task</li>
      </ul>
      <li>Explain Skip-gram</li>
      <ul>
        <li>Predict surrounding words by target word</li>
      </ul>
    </ul>

    <h3 class="card-title">LLM</h3>
    <ul>
      <li>Explain transfer learning and its benefits</li>
      <ul>
        <li>Task A and B have the same input</li>
        <li>Task A has a lot more data than task B</li>
        <li>Delete the last layer and weights that feed to the last layer</li>
        <li>Create new randomly initialized weights for the last layer</li>
        <li>Retain the network with dataset for task B</li>
      </ul>
      <li>Explain pre-training and fine-tuning</li>
      <ul>
        <li>In this case, training the network for task A is called <strong>pre-training</strong></li>
        <li>In this case, training the network for task B is called <strong>fine-tuning</strong></li>
      </ul>
      <li>Explain self-attention</li>
      <ul>
        <li>Every word (token) is related to every other word (token)</li>
        <li>Input sentence is tokenized and represented by Q,K,V whose dimension is batch_size, seq_len, d_model</li>
        <li>Q - words that we are focusing on, K - words that we are comparing against, V - multipled to softmax output</li>
      </ul>
      <li>Explain attention mask</li>
      <ul>
        <li>Separate important and unimportant tokens in input</li>
      </ul>
      <li>Explain transformer</li>
      <ul>
        <li>Model architecture used to process sequence data, particularly relating input and output sequences</li>
        <li>Ex. machine translation</li>
        <li>Encoder - input embedding, positional encoding, multi-head attention, feedforward, residual connection</li>
        <li>Decoder - output embedding, positional encoding, masked multi-head attention, multi-head attention, feedforward, residual connection</li>
      </ul>
      <li>Explain drawbacks of transformer</li>
      <ul>
        <li>Self attention runs in \( O(N^{2}D )\) where \( N \) is sequence length and \( D \) is dimension of words embedding</li>
        <li>Capturing word/token interaction requires large number of parameters, thus large memory</li>
      </ul>
      <li>Explain regularization used in transformer</li>
      <ul>
        <li>Dropout applied to fully connected layer</li>
      </ul>
      <li>Explain normalization used in transformer</li>
      <ul>
        <li>Layer normalization is typically applied just before self-attention layer</li>
        <li>Batch normalization is not normally used due to varying sequence length</li>
      </ul>
      <li>Explain how to achieve fast inference on transformer model</li>
      <ul>
        <li>Sparse attention or local attention</li>
        <li>Data parallelism or model parallelism</li>
        <li>Lower precision floating format</li>
        <li>Dynamic padding sequence</li>
      </ul>
      <li>Explain BERT</li>
      <ul>
        <li>Bi-drectional encoder representation from transformer</li>
        <li>Model architecture used to understand contexts of words</li>
        <li>Ex. question answering, sentiment analysis, text classification</li>
        <li>Encoder only</li>
      </ul>
      <li>Explain GPT-3</li>
      <ul>
        <li>Generative pre-trained transformers</li>
        <li>Model architecture used to generate texts</li>
        <li>Ex. text generation</li>
        <li>Decoder only</li>
      </ul>
      <li>Explain RAG</li>
      <ul>
        <li>Retrieval augmented generation</li>
        <li>Populate DB with information not available in LLM</li>
        <li>Using query embedding, search relevant information by embedding similairty</li>
        <li>Provide LLM with query and context retrieved from DB</li>
      </ul>
      <li>Explain LoRA</li>
      <ul>
        <li>Low ranking adaption</li>
        <li>Fine-tuning LLM with lower number of parameters</li>
        <li>Freeze weights of pre-trained model and train low rank matrix (adapter)</li>
      </ul>
      <li>Explain PPO</li>
      <ul>
        <li>Proximal policy optimization</li>
        <li>Policy gradient method that trains agent via rules</li>
        <li>Can be used in fune-tuning LLM to generate better dialogue</li>
      </ul>
      <li>Explain RLHF</li>
      <ul>
        <li>Reinforment learning from human feedback</li>
        <li>Human evaluates LLM outputs (rating, text summary, etc)</li>
        <li>Train reward model to predict how human would evaluate LLM response</li>
        <li>Fine-tune LLM via reinforment learning with reward model</li>
      </ul>
      <li>Explain multi-modal models</li>
      <ul>
        <li>Model that supports multiple types of inputs (text, image, audio, video)</li>
      </ul>
      <li>Explain how stability diffusion model use LLM to understand complex text prompts</li>
      <ul>
        <li>Generate image based on text description</li>
        <li>Combine response from LLM and stability diffusion model to provide final response with both text and image</li>
      </ul>
      <li>Explain how to train LLM with billions of parameters</li>
      <ul>
        <li>Text cleaning and tokenization</li>
        <li>Distribute workload among GPU nodes</li>
        <li>Optimize learning rate</li>
      </ul>
      <li>Explain how to train LLM to prevent hallucinations</li>
      <ul>
        <li>Improve data quality and diversity</li>
        <li>Alternatively, RAG/RLHF can be used</li>
      </ul>
      <li>Explain how to prevent bias and harmful prompt</li>
      <ul>
        <li>Use diverse and unharmful data</li>
        <li>Alternatively, RLHF can be used</li>
      </ul>
      <li>Explain how knowledge distillation benefits LLM</li>
      <ul>
        <li>Create smaller model from large model without losing much knowledge</li>
      </ul>
      <li>Explain few shot learning in LLM</li>
      <ul>
        <li>Provide few examples (explaining how LLM should respond to the prompt) in the prompt</li>
      </ul>
      <li>Explain how to evaluate LLM performance</li>
      <ul>
        <li>BLEU (bilingual evaluation understudy) to measure embedding similarity between LLM output and human-generated text</li>
        <li>Hallucination rate - human decides whether LLM hallucinates or not</li>
        <li>Factual accuracy - human decides whether each claim LLM makes in reponse is factual or not</li>
      </ul>
      <li>Explain how to improve factual accuracy of LLM</li>
      <ul>
        <li>Improve data quality</li>
        <li>Fine-tune on domain specific data</li>
        <li>Clear and step-by-step prompt</li>
      </ul>
      <li>Explain how to detect drift in LLM performance over time</li>
      <ul>
        <li>Monitor distribution of prompts and generated response</li>
        <li>Statistical tests (Kolmogorov–Smirnov, Mann–Whitney U-test)</li>
      </ul>
      <li>Explain how to curate dataset for training LLM</li>
      <ul>
        <li>Text cleanup - lower case, remove punctuation</li>
        <li>Tokenize - split sentences into words</li>
        <li>Token to ID - convert each word into ID</li>
      </ul>
      <li>Explain how to fine-tune LLM for domain specific applications like finance</li>
      <ul>
        <li>Prepare dataset for fune-tuning</li>
        <li>Tokenize prepared dataset</li>
        <li>Load pre-trained model</li>
        <li>Freeze weights in pre-trained model and initialize weights for fine-tuning</li>
        <li>Determine hyperparameters</li>
      </ul>
    </ul>        
  </div>
  <div class="card-footer text-muted">
    Reference: 
  </div>
</div>

<div class="card mb-4" id="machine-learning-">
  <div class="card-body">
    <h2 class="card-title">Tree</h2>

    <h3 class="card-title">Decision tree</h3>
    <ul>
      <li>Explain decision tree</li>
      <ul>
        <li>Depth of trees increases variance, trees are pruned to control variance</li>
        <li>Main parameters - maximum tree depth, minimum samples per tree node, impurity criterion</li>
        <li>Classification - return majority vote of trees (branch - test outcome, leaf - class label)</li>
        <li>Regression - return average value of trees</li>
      </ul>
      <li>Explain bagging</li>
      <ul>
        <li>Train models independently in parallel and combine the result (Ex. random forest)</li>
        <li>Less prone to overfitting, faster training, less hyperparameter tuning</li>
      </ul>
      <li>Explain boosting</li>
      <ul>
        <li>Train models in sequence where later models learn from mistakes from previous models (Ex. gradient boosting)</li>
        <li>Tree is built starting from root, for each leaf, split selected to minimize MSE</li>
        <li>Predictions of individual trees are summed</li>
        <li>More accurate, works well on unbalanced datatset</li>
      </ul>
      <li>Explain information gain and entropy</li>
      <ul>
        <li>Metrics used to determine best way to split the tree</li>
        <li>Entropy is measure of uncertainty, information gain calculates reduction in entropy</li>
        <li>Select variable that maximizes the information gain and split dataset into groups based on that variable</li>
      </ul>
      <li>Explain why emsembles have higher scores than individual models</li>
      <ul>
        <li>Because they gather strengths from many models</li>
      </ul>
      <li>Explain gradient boosting or GBM, and its pros and cons</li>
      <ul>
        <li>Train decision tress in sequence</li>
      </ul>
      <li>Explain why XGBoost is not suitable for continual learning</li>
      <ul>
        <li>Any new data requires the model to be re-trained</li>
        <li>Incremental learning, where updating model without re-training, is not possible</li>
        <li>Model training involves assessing entire dataset, which is slow</li>
      </ul>
      <li>Explain why XGBoost is good at dealing with sparse data</li>
      <ul>
        <li>It can handle one-hot encoded categorical features with any transformation</li>
        <li>Trees can split based on missing values or can treat missing values as distinct feature</li>
        <li>It does not focus on features that don't contribute much to target</li>
      </ul>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: 
  </div>
</div>

<!-- <div class="card mb-4" id="linear-regression-1">
  <div class="card-body">
    <h2 class="card-title">Linear regression</h2>
    <ul>
      <li>Fits line to data</li>
      <li>Dependent or target variable <strong>y</strong></li>
      <li>Independent or predictor variable <strong>x</strong></li>
      <li>Assumes errors are independently and identically normally distributed</li>
      <li>Single layer neural network with linear activation is identical with linear regression</li>
    </ul>

    <h3 class="card-title">Linear classifier</h3>
    <ul>
      <li>\( \textbf{y} = \textbf{W}\textbf{x} + \textbf{b} \)</li>
      <ul>
        <li>\( \textbf{y} \) is output of the classifier</li>
        <li>\( \textbf{x} \) is \( N \) dimensional vector</li>
        <li>\( \textbf{W} \) is \( N \times N \) matrix</li>
      </ul>
      <li>Data are provided in pairs \( (x,t) \)</li>
      <ul>
        <li>\( x \) is input to the classifer</li>
        <li>\( t \) is label</li>
      </ul>
    </ul>

    <h3 class="card-title">Linear classifier with 5 inputs and 10 outputs</h3>

<pre><code class="python">import torch
import torch.nn as nn

classifier = nn.Linear(5, 10)</code></pre>

    <h3 class="card-title">Mean squared error</h3>
    <ul>
      <li>\( \displaystyle\sum_{i=0}^n (y_{i} - t_{i})^2 \)</li>
      <li>Goal is to find \( \textbf{W} \) that minimizes the cost</li>
    </ul>

<pre><code class="python">import torch
import torch.nn as nn

model = nn.Linear(10,3)
loss = nn.MSELoss()

## Use random input x.
input_vector = torch.randn(10)

## Assume classiciation with 3 clases.
target = torch.tensor([0,0,1])

pred = model(input_vector)
output = loss(pred, target)
print("Prediction: " ,pred)
print("Output: " , output)</code></pre>

    <h3 class="card-title">Solve linear regression</h3>
    <ul>
      <li>Matrix algebra
        <ul>
          <li>Inverse of matrix may not be computable.</li>
          <li>Singular value decomposition could be used to compute the whole expression including the inverse.</li>
        </ul>
      </li>
      <li>Gradient descent</li>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://github.com/alexeygrigorev/data-science-interviews/blob/master/theory.md">Theoretical interview questions</a> | <a href="https://www.educative.io/path/become-a-deep-learning-professional">Become a Deep Learning Professional</a> | <a href="https://github.com/alexeygrigorev/data-science-interviews/blob/master/theory.md">Theoretical interview questions</a>
  </div>
</div> -->
<!-- Machine learning END -->

</div> <!-- /.col-md-12 -->
</div> <!-- /.row -->
</div> <!-- /.container -->

<include src="/footer.html"></include>

</body>

</html>