<!DOCTYPE html>

<html lang="en">

<head>

<!-- Metadata -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="">
<meta name="author" content="Seungmoon Rieh">
<meta name="keywords" content="">

<!-- Title and image -->
<title>Seungmoon Rieh</title>
<link href="/img/seungmoonrieh.jpg" rel="icon">

<!-- CSS -->
<link href="/css/bootstrap.min.css" rel="stylesheet">
<link href="/css/monokai-sublime.css" rel="stylesheet">
<link href="/css/site.css" rel="stylesheet">

<!-- JavaScript -->
<script src="/js/jquery.min.js" type="text/javascript"></script>
<script src="/js/bootstrap.bundle.min.js" type="text/javascript"></script>
<script src="/js/highlight.pack.js" type="text/javascript"></script>
<script src="/js/include_html.js" type="text/javascript"></script>
<script src="/js/mathjax/tex-chtml.js" type="text/javascript"></script>
<script src="/js/site.js" type="text/javascript"></script>

</head>

<body>

<include src="/header.html"></include>

<div class="container">
<div class="row">
<div class="col-md-12">
<h1 class="my-4">Machine Learning</h1>

<!-- Machine learning BEGIN -->
<div class="card mb-4" id="subject">
  <div class="card-body">
    <h2 class="card-title">Machine learning</h2>
    <ul class="list-unstyled mb-0">
      <li><a href="#machine-learning-">Machine learning</a></li>
      <ul>
        <!-- <li><a href="#machine-learning-">Statistics</a></li> -->
        <!-- <li><a href="#machine-learning-">Linear regression</a></li> -->
        <li><a href="#machine-learning-">Bias and variance</a></li>
        <li><a href="#machine-learning-">Classification</a></li>
        <!-- <li><a href="#machine-learning-">Dimensionality</a></li> -->
        <li><a href="#machine-learning-">Hyperparameter tuning</a></li>
        <li><a href="#machine-learning-">Orthogonalization</a></li>
        <!-- <li><a href="#machine-learning-">Recommender system</a></li> -->
        <!-- <li><a href="#machine-learning-">Tranfer learning</a></li> -->
        <!-- <li><a href="#machine-learning-">Multitask learning</a></li> -->
        <!-- <li><a href="#machine-learning-">Project workflow</a></li> -->
        <!-- <li><a href="#machine-learning-">Decision tree</a></li> -->
      </ul>
      <li><a href="#machine-learning-">Neural network</a></li>
      <ul>
        <li><a href="#machine-learning-">Logistic regression</a></li>
        <li><a href="#machine-learning-">Neural network</a></li>
        <li><a href="#machine-learning-">Initialization</a></li>
        <li><a href="#machine-learning-">Regularization</a></li>
        <li><a href="#machine-learning-">Normalization</a></li>
        <li><a href="#machine-learning-">Optimization</a></li>
      </ul>
      <!-- <li><a href="#machine-learning-">Computer vision</a></li>
      <ul>
        <li><a href="#machine-learning-">Convolutional neural network</a></li>
        <li><a href="#machine-learning-">Residual neural network</a></li>
        <li><a href="#machine-learning-">Object recognition</a></li>
        <li><a href="#machine-learning-">Neural style transfer</a></li>
        <li><a href="#machine-learning-">Inception network</a></li>
        <li><a href="#machine-learning-">Vision transformer</a></li>
      </ul> -->
      <li><a href="#machine-learning-">Natural language understanding</a></li>
      <ul>
        <!-- <li><a href="#machine-learning-">Recurrent neural network</a></li> -->
        <!-- <li><a href="#machine-learning-">Long short term memory</a></li> -->
        <li><a href="#machine-learning-">Word embedding</a></li>
        <li><a href="#machine-learning-">Transformer</a></li>
        <li><a href="#machine-learning-">BERT</a></li>
        <li><a href="#machine-learning-">RoBERTa</a></li>
        <li><a href="#machine-learning-">Machine translation</a></li>
        <li><a href="#machine-learning-">GPT-3</a></li>
      </ul>
      <li><a href="#machine-learning-">Tree</a></li>
      <ul>
        <li><a href="#machine-learning-">Decision tree</a></li>
        <li><a href="#machine-learning-">Random forest</a></li>
        <li><a href="#machine-learning-">Gradient boosting</a></li>
        <li><a href="#machine-learning-">XGBoost</a></li>
      </ul>
    </ul>
  </div>
</div>

<div class="card mb-4" id="machine-learning-">
  <div class="card-body">
    <h2 class="card-title">Machine learning</h2>
    <ul>
      <li>There are patterns to learn, which are complex</li>
      <li>Data is available or it is possible to collect data</li>
      <li>Learn pattern from data and use this knowledge to perform tasks</li>
    </ul>

    <h3 class="card-title">Types</h3>
    <ul>
      <li>Supervised - train with labeled data</li>
      <ul>
        <li>Regression - predict variables based on other (dependent) variables</li>
      </ul>
      <li>Unsupervised - detect patterns in data without labels</li>
      <ul>
        <li>Clustering - group objects such that objects in same group are similar and objects in different group are dissimilar</li>
      </ul>
      <li>Reinforcement - learn from repeated trial-and-error</li>
    </ul>

    <!-- <h3 class="card-title">Problems</h3>
    <ul>
      <li>Other</li>
      <ul>
        <li>Machine learning</li>
        <ul>
          <li>Explain how to explain machine learning to a kid</li>
          <li>Explain difference between supervised, unsupervised, and reinforcement learning</li>
          <li>Explain regression algorithm</li>
          <li>Explain clustering algorithm</li>
        </ul>
        <li>Statistics</li>
        <ul>
          <li>Explain difference between Type I and Type II error</li>
        </ul>
        <li>CNN</li>
        <ul>
          <li>Explain how convolution works, when inputs are gray-scale or RGB</li>
          <li>Explain why we use convolution layer rather than just FC layers</li>
          <li>Explain why we use many small kernels like 3 by 3 rather than few large kernels</li>
          <li>Explain why we use max-pooling and how it is different from average pooling</li>
          <li>Explain ResNet and skip connection</li>
          <li>Explain non-max suppression</li>
          <li>Explain data augmentation</li>
          <li>Explain autoencoder</li>
        </ul>
        <li>Reinforcement learning</li>
        <ul>
          <li>Explain Markov Decision Process for reinforcement learning</li>
          <li>Explain main component of reinforcement learning agents</li>
          <li>Explain how to define reward function</li>
          <li>Explain difference between model-based and model-free reinforcement learning</li>
          <li>Explain the role of discount factor</li>
          <li>Explain exploration-exploitation trade-off</li>
          <li>Explain difference between policy gradient and value iteration method</li>
          <li>Explain State (Q) and Action-Value (Q) functions</li>
          <li>Explain how to handle continuous action space</li>
          <li>Explain deep reinforcement learning</li>
          <li>Explain how to ensure convergence of reinforcement learning</li>
          <li>Explain how multi-agent reinforcement learning system work</li>
        </ul>
        <li>Dimension reduction</li>
        <ul>
          <li>Explain how to tackle curse of dimensionality</li>
          <li>Explain principal component analysis</li>
          <li>Explain t-SNE</li>
        </ul>
      </ul>
    </ul> -->
  </div>
  <div class="card-footer text-muted">
    Reference: Designing Machine Learning Systems, Chip Huyen
  </div>
</div>

<div class="card mb-4" id="machine-learning-">
  <div class="card-body">
    <h2 class="card-title">MLP</h2>

    <h3 class="card-title">Feature engineering</h3>
    <ul>
      <li>Explain why we do feature selection</li>
      <ul>
        <li>Too many features can cause overfitting</li>
        <li>Become technical debt</li>
        <li>Slower inference if dynamic feature computation is needed</li>
        <li>Increase memory requirement</li>
      </ul>
      <li>Explain feature selection technique</li>
      <li>Explain feature scaling</li>
      <ul>
        <li>Help gradient descent to find optimun faster</li>
        <li>Ex. standardization (z-score normalization) where mean is 0 and std is 1</li>
        <li>Assumes data is normallly distributed</li>
      </ul>
      <li>Explain how to encode categorical data</li>
      <ul>
        <li>One-hot encoding - dense features, assign binary to each category</li>
        <li>Embedding - sparse features, become hidden layer</li>
      </ul>
      <li>Explain how to handle missing data</li>
      <ul>
        <li>Delete - data gets less</li>
        <li>Imputation - data gets noisy</li>
      </ul>
      <li>Explain how to handle imbalanced dataset</li>
      <ul>
        <li>Undersample/oversample</li>
        <li>Loss function - higher weights to under-represented data</li>
      </ul>
    </ul>

    <h3 class="card-title">Bias and variance</h3>
    <ul>
      <li>Explain bias-variance trade-off</li>
      <ul>
        <li>Challenge to fight both underfitting and overfitting the model to data</li>
      </ul>
      <li>Explain overfitting and underfitting, and how to tackle them</li>
      <ul>
        <li>Underfitting - high bias (training error), more data, better/bigger model, train longer</li>
        <li>Overfitting - high variance (validation error), more data, regularization</li>
      </ul>
      <li>Explain why we use validation and test set</li>
      <ul>
        <li>Validation set - used for hyparameters tunung, use data unseen in training set</li>
        <li>Test set - used for model performance, use data unseen in training and validation set</li>
      </ul>
      <li>Explain cross-validation</li>
      <ul>
        <li>Split training set into different bins</li>
        <li>One bin is set aside as test set and all others as training set</li>
        <li>Train on training set using test set as label</li>
        <li>Base population must not co-exist in more than one bin</li>
      </ul>
      <li>Explain why training loss might decreases while validation loss increases</li>
      <ul>
        <li>Overfitting - model works well predicting based on training data but not well on unseen (validation) data</li>
      </ul>
    </ul>

    <h3 class="card-title">Classification</h3>
    <ul>
      <li>Explain difference between precision and recall</li>
      <ul>
        <li>Precision - assess percentage of correct prediction over all prediction, focus on false positive</li>
        <li>Recall - assess percentage of correct prediction over all relevant items</li>
      </ul>
      <li>Explain difference between PR-AUC and ROC-AUC</li>
      <ul>
        <li>ROC-AUC - true positive vs false positive</li>
        <li>PR-AUC - precision vs recall</li>
      </ul>
      <li>Explain accuracy</li>
      <ul>
        <li>Assess percentage of correctness</li>
      </ul>
      <li>Explain F1 score</li>
      <ul>
        <li>Average of precision and recall</li>
      </ul>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: 
  </div>
</div>

<div class="card mb-4" id="machine-learning-">
  <div class="card-body">
    <h2 class="card-title">NN</h2>

    <h3 class="card-title">Neural network</h3>
    <ul>
      <li>Explain cost function</li>
      <ul>
        <li>Function that computes difference between predicted and actual over all samples</li>
      </ul>
      <li>Explain difference between epoch, batch, and iteration</li>
      <ul>
        <li>Epoch - cadence to go over all training data</li>
        <li>Batch - amount of training data to make one gradient update</li>
        <li>Iteration - cadence to train model through all epoch under a set of hyperparameters</li>
      </ul>
      <li>Explain difference between single-layer perception and multi-layer perception</li>
      <ul>
        <li>Single-layer perception - single layer network with linear activation, which is equivalent to linear regression</li>
        <li>Multi-layer percention - has at least one hidden layer</li>
      </ul>
      <li>Explain what is meant by depth in neural network</li>
      <ul>
        <li>Number of layers (input layer plus hidden layers, not counting output layer)</li>
      </ul>
      <li>Explain significance of bias term in neural network</li>
      <ul>
        <li>Handle situations where inputs are zero</li>
      </ul>
      <li>Explain how backpropagation works</li>
      <ul>
        <li>Initially, weights are set to random values</li>
        <li>Model makes prediction with initial weights, and cost is computed</li>
        <li>Gradient of weights and bias are computed</li>
        <li>Gradient is updated using learning rate</li>
      </ul>
      <li>Explain weight constraints and how it can benefit training</li>
      <ul>
        <li>During training, if weights exceed certain value, keep it as threshold</li>
        <li>This helps preventing overfitting</li>
      </ul>
      <li>Explain challenges with very deep neural network</li>
      <ul>
        <li>Exploding and vanishing gradient - gradient clipping</li>
        <li>Overfitting - regularization</li>
        <li>Slow convergence - adaptive learning rate</li>
      </ul>
      <li>Explain why we shuffle training data after each epoch</li>
      <ul>
        <li>Model may learn pattern from ordering of data</li>
      </ul>
      <li>Explain hyperparameter tuning</li>
      <ul>
        <li>Hold out validation set</li>
        <li>Try combinations of hyperparameters and test model performance on validation set</li>
        <li>Iterate and pick the best combination</li>
      </ul>
    </ul>

    <h3 class="card-title">Activation</h3>
    <ul>
      <li>Explain activation function</li>
      <ul>
        <li>Non-linear function that generate outputs of neuron</li>
      </ul>
      <li>Explain why ReLu is used more than sigmoid in neural network</li>
      <ul>
        <li>Signmoid - function flattens near zero, leading to vanishing gradient</li>
        <li>ReLu - neuron does not active for negative input, thus addresses vanishing gradient</li>
      </ul>
    </ul>

    <h3 class="card-title">Normalization</h3>
    <ul>
      <li>Explain normalization and why we use it</li>
      <ul>
        <li>If features are not scaled, contours can be assymmetric, gradient descent can take long time</li>
        <li>If features are scaled, contours can be symmetric, gradient decent can find optimum faster</li>
      </ul>
      <li>Explain difference between normalization and standardization</li>
      <ul>
        <li>Normalization - values are between 0 and 1, does not change distribution</li>
        <li>Standardization - values have mean 0 and std 1, changes to normal distribution</li>
      </ul>
      <li>Explain difference between batch normalization and layer normalization</li>
      <ul>
        <li>Batch normalization - take a batch and normalize feature by feature</li>
        <li>Layer normalization - take all features and normalize data point by data point</li>
      </ul>
    </ul>

    <h3 class="card-title">Initialization</h3>
    <ul>
      <li>Explain weight initialization methods like Xavier, He</li>
      <ul>
        <li>He - initial weights to small values (If values are large, it can lead to exploding/vanish gradient)</li>
      </ul>
    </ul>

    <h3 class="card-title">Regularization</h3>
    <ul>
      <li>Explain regularization and why we use it</li>
      <ul>
        <li>For small Z, activation function becomes close to linear, and not become complex</li>
        <li>To have small Z, need small W, thus need lambda</li>
      </ul>
      <li>Explain difference between L1 and L2 regularization</li>
      <ul>
        <li>L1 - works well on sparse data</li>
        <li>L2 - works well when features are corelated</li>
      </ul>
      <li>Explain dropout</li>
      <ul>
        <li>Neurons are ramdonly shut down with probability p</li>
        <li>Neurons become reluctant to put too much weight on one feature</li>
        <li>Weight values can be more smaller and spread out</li>
      </ul>
      <li>Explain how early stopping prevents overfitting</li>
      <ul>
        <li>Stop training when validation error increases</li>
        <li>Tackle bias and variance at the same time (rather than one at a time)</li>
      </ul>
    </ul>

    <h3 class="card-title">Optimization</h3>
    <ul>
      <li>Explain batch size and how it is related to convergence</li>
      <ul>
        <li>Small batch - frequent and noisy gradient updates, can escape local optima well but too much osciallation an slow down convergence</li>
        <li>Large batch - stable gradient updates, may get stuck in local optima</li>
        <li>Balanced batch is needed</li>
      </ul>
      <li>Explain vanishing and exploding gradient</li>
      <ul>
        <li>Vanishing - gradient becomes very small</li>
        <li>Exploding - gradient becomes very large</li>
      </ul>
      <li>Explain momentum</li>
      <ul>
        <li>On vertical axis (b), we want slower learing to reduce oscilation</li>
        <li>On horizontal axis (W), we want faster learning</li>
        <li>Apply addition computation when updating W and b with velocity of W and b</li>
      </ul>
      <li>Explain RMSProp</li>
      <ul>
        <li>Similar idea from momentum, but different calculation</li>
      </ul>
      <li>Explain ADAM optimizer</li>
      <ul>
        <li>Combine calculations from momentum and RMSProp</li>
      </ul>
      <li>Explain difference between batch, min-batch, and stochastic gradient descent</li>
      <ul>
        <li>Batch - use all training data to make a single gradient update</li>
        <li>Min-batch - use subset of training data</li>
        <li>Stachastic - use a single data</li>
      </ul>
      <li>Explain difference between global optima and local optima</li>
      <ul>
        <li>Global - optimum value of function in its entire span</li>
        <li>Local - optimum value of function in its subrange</li>
      </ul>
      <li>Explain gradient clipping and how it help training</li>
      <ul>
        <li>If gradient exceed certain value, set it to threshold</li>
        <li>Prevents exploding gradient</li>
      </ul>
    </ul>

    <h3 class="card-title">Learning rate</h3>
    <ul>
      <li>Explain learning rate</li>
      <ul>
        <li>In gradient descent, size of step you want to take in particular direction</li>
      </ul>
      <li>Explain adaptive learning rate</li>
      <ul>
        <li>Bigger learning rate in the beginning and smaller near the optimum</li>
      </ul>
      <li>Explain learning rate scheduler</li>
      <ul>
        <li>Adjust learning rate based on number of epoch</li>
      </ul>
      <li>Explain why learning rate is considered as most important hyperparameter</li>
      <ul>
        <li>It directly controls how quickly model adjusts its weights during training</li>
      </ul>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: 
  </div>
</div>

<div class="card mb-4" id="machine-learning-">
  <div class="card-body">
    <h2 class="card-title">NLP</h2>

    <h3 class="card-title">RNN</h3>
    <ul>
      <li>Explain RNN</li>
      <li>Explain difference between feed forward network and recurrent network</li>
      <li>Explain LSTM</li>
      <li>Explain how LSTM handle vanishing gradient</li>
      <li>Explain word embedding and how it is different from one-hot encoding</li>
      <li>Explain Word2Vec</li>
      <li>Explain CBOW</li>
    </ul>

    <h3 class="card-title">LLM</h3>
    <ul>
      <li>Explain transfer learning and its benefits</li>
      <ul>
        <li>Task A and B have the same input</li>
        <li>Task A has a lot more data than task B</li>
        <li>Delete the last layer and weights that feed to the last layer</li>
        <li>Create new randomly initialized weights for the last layer</li>
        <li>Retain the network with dataset for task B</li>
      </ul>
      <li>Explain pre-training and fine-tuning</li>
      <ul>
        <li>In this case, training the network for task A is called <strong>pre-training</strong></li>
        <li>In this case, training the network for task B is called <strong>fine-tuning</strong></li>
      </ul>
      <li>Explain self-attention</li>
      <ul>
        <li>Every word (token) is related to every other word (token)</li>
        <li>Input sentence is tokenized and represented by Q,K,V whose dimension is batch_size, seq_len, d_model</li>
      </ul>
      <li>Explain transformer</li>
      <ul>
        <li>Model architecture used to process sequence data, particularly relating input and output sequences</li>
        <li>Ex. machine translation</li>
        <li>Encoder - input embedding, positional encoding, multi-head attention, feedforward, residual connection</li>
        <li>Decoder - output embedding, positional encoding, masked multi-head attention, multi-head attention, feedforward, residual connection</li>
      </ul>
      <li>Explain BERT</li>
      <ul>
        <li>Bi-drectional encoder representation from transformer</li>
        <li>Model architecture used to understand contexts of words</li>
        <li>Ex. question answering, sentiment analysis</li>
        <li>Encoder only</li>
      </ul>
      <li>Explain GPT-3</li>
      <ul>
        <li>Generative pre-trained transformers</li>
        <li>Model architecture used to generate texts</li>
        <li>Ex. text generation</li>
        <li>Decoder only</li>
      </ul>
      <li>Explain RAG</li>
      <ul>
        <li>Retrieval augmented generation</li>
        <li>Populate DB with information not available in LLM</li>
        <li>Using query embedding, search relevant information by embedding similairty</li>
        <li>Provide LLM with query and context retrieved from DB</li>
      </ul>
      <li>Explain LoRA</li>
      <ul>
        <li>Low ranking adaption</li>
        <li>Fine-tuning LLM with lower number of parameters</li>
        <li>Freeze weights of pre-trained model and train low rank matrix (adapter)</li>
      </ul>
      <li>Explain PPO</li>
      <ul>
        <li>Proximal policy optimization</li>
        <li>Policy gradient method that trains agent via rules</li>
        <li>Can be used in fune-tuning LLM to generate better dialogue</li>
      </ul>
      <li>Explain RLHF</li>
      <ul>
        <li>Reinforment learning from human feedback</li>
        <li>Human evaluates LLM outputs (rating, text summary, etc)</li>
        <li>Train reward model to predict how human would evaluate LLM response</li>
        <li>Fine-tune LLM via reinforment learning with reward model</li>
      </ul>
      <li>Explain multi-modal models</li>
      <ul>
        <li>Model that supports multiple types of inputs (text, image, audio, video)</li>
      </ul>
      <li>Explain how stability diffusion model use LLM to understand complex text prompts</li>
      <ul>
        <li>Generate image based on text description</li>
        <li>Combine response from LLM and stability diffusion model to provide final response with both text and image</li>
      </ul>
      <li>Explain how to train LLM with billions of parameters</li>
      <ul>
        <li>Text cleaning and tokenization</li>
        <li>Distribute workload among GPU nodes</li>
        <li>Optimize learning rate</li>
      </ul>
      <li>Explain how to train LLM to prevent hallucinations</li>
      <ul>
        <li>Improve data quality and diversity</li>
        <li>Alternatively, RAG/RLHF can be used</li>
      </ul>
      <li>Explain how to prevent bias and harmful prompt</li>
      <ul>
        <li>Use diverse and unharmfuldata</li>
        <li>Alternatively, RLHF can be used</li>
      </ul>
      <li>Explain how knowledge distillation benefits LLM</li>
      <ul>
        <li>Create smaller model from large model without losing much knowledge</li>
      </ul>
      <li>Explain few shot learning in LLM</li>
      <ul>
        <li>Provide few examples (explaining how LLM should respond to the prompt) in the prompt</li>
      </ul>
      <li>Explain how to evaluate LLM performance</li>
      <ul>
        <li>BLEU (bilingual evaluation understudy) to measure embedding similarity between LLM output and human-generated text</li>
        <li>Hallucination rate - human decides whether LLM hallucinates or not</li>
        <li>Factual accuracy - human decides whether each claim LLM makes in reponse is factual or not</li>
      </ul>
      <li>Explain how to improve factual accuracy of LLM</li>
      <ul>
        <li>Improve data quality</li>
        <li>Fine-tune on domain specific data</li>
        <li>Clear and step-by-step prompt</li>
      </ul>
      <li>Explain how to detect drift in LLM performance over time</li>
      <ul>
        <li>Monitor distribution of prompts and generated response</li>
        <li>Statistical tests (Kolmogorov–Smirnov, Mann–Whitney U-test)</li>
      </ul>
      <li>Explain how to curate dataset for training LLM</li>
      <ul>
        <li>Text cleanup - lower case, remove punctuation</li>
        <li>Tokenize - split sentences into words</li>
        <li>Token to ID - convert each word into ID</li>
      </ul>
      <li>Explain how to fine-tune LLM for domain specific applications like finance</li>
      <ul>
        <li>Prepare dataset for fune-tuning</li>
        <li>Tokenize prepared dataset</li>
        <li>Load pre-trained model</li>
        <li>Freeze weights in pre-trained model and initialize weights for fine-tuning</li>
        <li>Determine hyperparameters</li>
      </ul>
    </ul>        
  </div>
  <div class="card-footer text-muted">
    Reference: 
  </div>
</div>

<div class="card mb-4" id="machine-learning-">
  <div class="card-body">
    <h2 class="card-title">Tree</h2>

    <h3 class="card-title">Decision tree</h3>
    <ul>
      <li>Explain difference between bagging and boosting</li>
      <ul>
        <li>Bagging - train models independently in parallel and combine the result</li>
        <li>Boosting - train models in sequence where later models learn from mistakes from previous models</li>
      </ul>
      <li>Explain why emsembles have higher scores than individual models</li>
      <ul>
        <li>Because they gather strengths from many models</li>
      </ul>
      <li>Explain gradient boosting or GBM</li>
      <ul>
        <li>Train decision tress in sequence</li>
      </ul>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: 
  </div>
</div>

<div class="card mb-4" id="machine-learning-">
  <div class="card-body">
    <h2 class="card-title">Implement linear regression</h2>
  </div>
  <div class="card-footer text-muted">
    Reference: 
  </div>
</div>

<div class="card mb-4" id="machine-learning-">
  <div class="card-body">
    <h2 class="card-title">Implement logistic regression</h2>
  </div>
  <div class="card-footer text-muted">
    Reference: 
  </div>
</div>

<div class="card mb-4" id="machine-learning-">
  <div class="card-body">
    <h2 class="card-title">Implement a shallow neural network </h2>
  </div>
  <div class="card-footer text-muted">
    Reference: 
  </div>
</div>

<div class="card mb-4" id="machine-learning-">
  <div class="card-body">
    <h2 class="card-title"></h2>
  </div>
  <div class="card-footer text-muted">
    Reference: 
  </div>
</div>

<div class="card mb-4" id="machine-learning-">
  <div class="card-body">
    <h2 class="card-title"></h2>
  </div>
  <div class="card-footer text-muted">
    Reference: 
  </div>
</div>

<div class="card mb-4" id="machine-learning-">
  <div class="card-body">
    <h2 class="card-title"></h2>
  </div>
  <div class="card-footer text-muted">
    Reference: 
  </div>
</div>

<div class="card mb-4" id="machine-learning-">
  <div class="card-body">
    <h2 class="card-title"></h2>
  </div>
  <div class="card-footer text-muted">
    Reference: 
  </div>
</div>

<!-- <div class="card mb-4" id="machine-learning-">
  <div class="card-body">
    <h2 class="card-title">Bias and variance</h2>
    <ul>
      <li>Need to find right balance without overfitting or underfitting the data</li>
    </ul>

    <h3 class="card-title">Bias</h3>
    <ul>
      <li>Train set error</li>
      <li>How far off model prediction is from correct value</li>
      <li>Error from approximately true underlying function</li>
      <li>Difference between predicted and actual value</li>
    </ul>

    <h3 class="card-title">Variance</h3>
    <ul>
      <li>Dev set error</li>
      <li>Variability of model prediction for given data point</li>
      <li>Sensitivity to changes in training data</li>
      <li>Overfitting - model works well on training data, but doesn't generalize well on unseen data</li>
    </ul>

    <h3 class="card-title">Ex. election survey</h3>
    <ul>
      <li>Surveying from a phonebook is source of bias</li>
      <li>Small sample size is source of variance</li>
    </ul>

    <table>
      <tr>
        <td></td>
        <td>variance</td>
        <td>bias</td>
        <td>both bias and variance</td>
        <td>neither</td>
      </tr>
      <tr>
        <td>train set error</td>
        <td>1%</td>
        <td>15%</td>
        <td>15%</td>
        <td>0.5%</td>
      </tr>
      <tr>
        <td>test set error</td>
        <td>11%</td>
        <td>16%</td>
        <td>30%</td>
        <td>1%</td>
      </tr>
    </table>

    <h3 class="card-title">K-fold cross validation</h3>
    <ul>
      <li>Randomly shuffle dataset</li>
      <li>Divide dataset into k bins</li>
      <li>Iterate on k bins</li>
      <ul>
        <li>Use one bin as test set and all other bins as training set</li>
        <li>Fit model on training set and evaluate on test set</li>
        <li>Remember the score from each iteration</li>
      </ul>
      <li>Average accuracies from iterations to compute the accuracy score</li>
      <li>k in practice should be 4-5</li>
    </ul>

    <h3 class="card-title">Why human-level performance</h3>
    <ul>
      <li>While ML is worse than human, you can</li>
      <ul>
        <li>Get labeled data from human.</li>
        <li>Gain insight from manual error analysis. (why did a person get this right?)</li>
        <li>Better analysis of bias/variance.</li>
      </ul>
    </ul>

    <h3 class="card-title">Avoidable bias</h3>
    <ul>
      <li>Human error as a proxy for bays error.</li>
      <li>Gap between human and training error: avoidable bias.</li>
      <li>Gap between training and dev error: variance.</li>
    </ul>

    <h3 class="card-title">Two fundamental assumptions of supervised learning</h3>
    <ul>
      <li>You can fit the training set pretty well ~ avoidable bias.</li>
      <li>Training set performance generalizes pretty well to dev/test set ~ variance.</li>
      <li>Increasing lambda decrease variance. Decreasing lambda decrease bias.</li>
      <li>More features decrease bias but increases variance. Less features decreases variance but increases bias.</li>
    </ul>

    <h3 class="card-title">Approaches</h3>
    <ul>
      <li>Linear model</li>
      <ul>
        <li>Regularization is used to decrease variance at the cost of increasing bias.</li>
      </ul>
      <li>Neural network</li>
      <ul>
        <li>Variance increases and bias decreases with number of hidden units. Regularization is used.</li>
      </ul>
      <li>K-nearest neighbor</li>
      <ul>
        <li>High K leads to high bias and low variance.</li>
      </ul>
      <li>Decision tree</li>
      <ul>
        <li>Depth of trees increases variance. Trees are pruned to control variance.</li>
      </ul>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://www.coursera.org/specializations/deep-learning?">Deep Learning Specialization</a> | <a href="https://www.shiksha.com/online-courses/articles/k-fold-cross-validation/#:~:text=In%20K%2Dfold%20cross%2Dvalidation%2C%20the%20data%20set%20is,5%2Dfold%20cross%2Dvalidation">K-fold Cross-validation</a>
  </div>
</div> -->

<!-- <div class="card mb-4" id="machine-learning-">
  <div class="card-body">
    <h2 class="card-title">Classification</h2>
    <ul>
      <li>Finite number of outputs</li>
      <li>Ex. logistic regression, decision tree, random forests</li>
    </ul>

    <h3 class="card-title">Imbalanced data in classification</h3>
    <ul>
      <li>Collect more data</li>
      <li>Undersample from over-represented class</li>
      <li>Change performance metric</li>
      <ul>
        <li>Accuracy is not the right metric to use when data is imbalanced</li>
        <li>Look at precision, recall, F1 score</li>
      </ul>
      <li>Data augmentation</li>
      <ul>
        <li>For example, crop/rotate images</li>
      </ul>
    </ul>

    <h3 class="card-title">Evaluate classification model</h3>
    <ul>
      <li>True Negative - ground truth was negative and prediction was negative</li>
      <li>True Positive - ground truth was positive and prediction was positive</li>
      <li>False Negative - ground truth was positive but prediction was negative</li>
      <li>False Positive - ground truth was negative but prediction was positive</li>
      <li>Confusion table shows TP, FP, TN, FN</li>
      <li>In perfectly separable data, both precision and recall can be 1</li>
      <li>But in real world, shifting decision boundary increase one but decrease the other</li>
      <li>Precision</li>
      <ul>
        <li>Correctness on predicted positive</li>
        <li>What percentage of positive predictions were correct?</li>
        <ul>
          <li>Ex. of examples recognized as cat, what % actually are cats?</li>
        </ul>
        <li>\( \dfrac{\text{True Positive}}{\text{True Positive} + \text{False Positive}} \)</li>
      </ul>
      <li>Recall</li>
      <ul>
        <li>Correctness of actual positive</li>
        <li>What percentage of positive cases did you catch?</li>
        <ul>
          <li>Ex. what % of actual cats are correctly recognized</li>
        </ul>
        <li>\( \dfrac{\text{True Positive}}{\text{True Positive} + \text{False Negative}} \)</li>
      </ul>
      <li>F1 score</li>
      <ul>
        <li>Average of precision and recall</li>
        <li>\( \dfrac{2}{\frac{1}{P} + \frac{1}{R}} \)</li>
      </ul>
      <li>Accuracy</li>
      <ul>
        <li>What percentage of predictions were correct?</li>
        <li>\( \dfrac{\text{True Positive} + \text{True Negative}}{\text{True Negative} + \text{True Positive} + \text{False Negative} + \text{False Positive}} \)</li>
      </ul>
      <li>False Positive Vs. False Negative</li>
      <ul>
        <li>In medical exam, False Negative is threatening to patients. Thus, False Positive is preferred</li>
        <li>In spam filtering, False Positive is annoying to users. Thus, False Negative is preferred</li>
      </ul>
      <li>ROC-AUC</li>
      <ul>
        <li>Trade-off between TP and FP</li>
      </ul>
      <li>PR-AUC</li>
      <ul>
        <li>Trade-off between precision and recall</li>
      </ul>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href=""></a>
  </div>
</div> -->

<!-- <div class="card mb-4" id="machine-learning-">
  <div class="card-body">
    <h2 class="card-title">Hyperparameter tuning</h2>
    <ul>
      <li>Should use random sampling to choose the number of layers, number of features, etc</li>
      <li>Scale parameters accordingly such as log scale</li>
      <ul>
        <li>For example, \( \alpha = 0.0001 \dots 1 \)</li>
        <ul>
          <li>Use log scale such that \( 0.0001, 0.001, 0.01, 0.1, 1 \)</li>
          <pre><code class="python">r = -4 * np.random.rand()
alpha = 10 ** r</code></pre>
        </ul>
        <li>For example, \( \beta = 0.9 \dots 0.999 \)</li>
        <ul>
          <li>Use \( 1-\beta \) such that \( 0.1, 0.01, 0.001 \)</li>
        </ul>
      </ul>
      <li>Panda - babysit one model</li>
      <li>Caviar - train many models in parallel</li>
      <li>Tuning methods</li>
      <ul>
        <li>Grid search</li>
        <li>Random search</li>
        <li>Bayesian Optimization (heaviliy outperforms above two)</li>
      </ul>
    </ul>

    <h3 class="card-title">Hyperparameter</h3>
    <ul>
      <li>\( \alpha \) the learning rate is most important</li>
      <li>Mini-batch size, the number of hidden units are second important</li>
      <li>The number of layers and learning rate decay are third important</li>
      <li>If using Adam, never tune \( \beta_{1}, \beta_{2}, \epsilon \)</li>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://www.deeplearning.ai/deep-learning-specialization">Deep Learning Specialization</a>
  </div>
</div>

<div class="card mb-4" id="machine-learning-">
  <div class="card-body">
    <h2 class="card-title">Orthogonalization</h2>
    <ul>
      <li>Models are trained on train set (Tackle underfitting)</li>
      <ul>
        <li>Better neural network architecture</li>
        <li>Hyperparameter search</li>
        <li>Bigger network/model</li>
        <li>Better optimization algorithm</li>
        <li>Train longer</li>
      </ul>
      <li>Hyperpameters are selected on validation set (Tackle overfitting)</li>
      <ul>
        <li>Better neural network architecture</li>
        <li>Hyperparameter search</li>
        <li>Regularization</li>
        <li>Bigger training set</li>
      </ul>
      <li>Final evaluation is done on test set</li>
      <ul>
        <li>Bigger dev set</li>
        <li>Dev and test set must come from the same distribution!</li>
        <li>Test set may not be needed</li>
      </ul>
      <li>Perform in real world</li>
      <ul>
        <li>Change dev set</li>
        <li>Change cost function</li>
      </ul>
    </ul>

    <h3 class="card-title">Training and dev/test on different distribution</h3>

    <h4 class="card-title">Example</h4>
    <ul>
      <li>Assume 200,000 is from web image and 10,000 is from mobile image</li>
      <li>Goal is to do image recognition on mobile device</li>
      <li>Then, use 2,500 mobile images for dev set and another 2,500 mobile images for test set</li>
      <li>Use 200,000 + 5,000 for the training set</li>
    </ul>

    <h4 class="card-title">Error analysis</h4>
    <ul>
      <li>If dev error >> training error, it could be because both variance and distribution mismatch</li>
      <li>Training-dev set - same distribution as training set, but not used in training</li>
      <li>If training-dev error >> training error, it is variance problem</li>
      <li>If dev error >> training-dev error, it is not a variance problem. It is data mismatch problem</li>
    </ul>

    <h4 class="card-title">Handle data mismatch</h4>
    <ul>
      <li>Make training data similar to dev/test set via synthesizing data</li>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://www.deeplearning.ai/deep-learning-specialization/">Deep Learning Specialization</a>
  </div>
</div> -->

<!-- <div class="card mb-4" id="machine-learning-">
  <div class="card-body">
    <h2 class="card-title">Project workflow</h2>
    <ul>
      <li>What is business objective?</li>
      <ul>
        <li>Increase revenue, win more customers?</li>
      </ul>
      <li>Define problem</li>
      <ul>
        <li>Outline the gap we are trying to solve.</li>
      </ul>
      <li>Can the problem be solved without data science?</li>
      <ul>
        <li>For example, just recommend top N items based on very simple logic.</li>
      </ul>
      <li>Review existing ML</li>
      <ul>
        <li>No need to re-invent the wheel.</li>
      </ul>
      <li>Setup metrics.</li>
      <ul>
        <li>What does it mean to be sucessful and not successful?</li>
      </ul>
      <li>Exploratory data analysis</li>
      <ul>
        <li>See what data is like via lots of plotting.</li>
      </ul>
      <li>Partition data into 3 sets.</li>
      <ul>
        <li>Train/dev/test.</li>
      </ul>
      <li>Preprocess</li>
      <ul>
        <li>Data cleaning, transformation, etc.</li>
      </ul>
      <li>Feature engineering</li>
      <ul>
        <li>Requires domain knowledge. Can be minimum if using deep learning.</li>
      </ul>
      <li>Develop model</li>
      <ul>
        <li>Choose algorithms, hypterparameters, etc.</li>
      </ul>
      <li>Ensemble</li>
      <ul>
        <li>Beware. Some ensembles are too complex to put into prodiction.</li>
      </ul>
      <li>Deploy/Monitor model</li>
      <ul>
        <li>Continue iterating afterwards.</li>
      </ul>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href=""></a>
  </div>
</div> -->

<!-- <div class="card mb-4" id="machine-learning-">
  <div class="card-body">
    <h2 class="card-title">Decision tree</h2>
    <ul>
      <li>Used for classification.</li>
      <li>Internal node: test on attribute.</li>
      <li>Branch: test outcome.</li>
      <li>Leaf: class label.</li>
      <li>Main parameters: maximum tree depth, minimum samples per tree node, impurity criterion.</li>
    </ul>

    <h3 class="card-title">Random forest</h3>
    <ul>
      <li>Used for regression and classification.</li>
      <li>Consist of many decision trees.</li>
    </ul>

    <h3 class="card-title">Gradient boosting</h3>
    <ul>
      <li>Relies on regression trees, which minimizes MSE.</li>
      <li>Greedy algorithm: tree is built starting from root. For each leaf, split selected to minimize MSE for this step.</li>
      <li>Build collection of trees one by one. Then, predictions of individual trees are summed.</li>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href=""></a>
  </div>
</div> -->

<!-- <div class="card mb-4" id="machine-learning-">
  <div class="card-body">
    <h2 class="card-title">Statistics</h2>
    <ul>
      <li>Type 1 error - rejecting the null hypothesis when it is actually true</li>
      <li>Type 2 error - failing to reject the null hypothesis when it is actually false</li>
    </ul>

    <h3 class="card-title">Sparse data</h3>
    <ul>
      <li>L1 regularization</li>
      <li>Linear regression if linear relationship</li>
      <li>One-hot encoding</li>
    </ul>

    <h3 class="card-title">Statistical power</h3>
    <ul>
      <li>Likelyhood that study will find effect when in fact there is effect</li>
      <li>Higher statistical power, less likely to make false negative</li>
    </ul>

    <h3 class="card-title">Outlier</h3>
    <ul>
      <li>Can be removed during data preparation using standard deviation</li>
    </ul>

    <h3 class="card-title">Anomaly</h3>
    <ul>
      <li>68% of data is one std away</li>
      <li>95% of data is two std away</li>
      <li>9% of data is three std away</li>
      <li>Statistical method</li>
      <ul>
        <li>Consider data point with 3 std away as outlier and likely anomaly</li>
      </ul>
      <li>Metric method</li>
      <ul>
        <li>A point is considered anomaly if removing it significantly improves the model</li>
        <li>Outlier score is a degree that a point doesn't belong to a cluster</li>
      </ul>
    </ul>

    <h3 class="card-title">Gaussian mixture model Vs K-means</h3>
    <ul>
      <li>K-mean</li>
      <ul>
        <li>Data point must belong to one cluster</li>
        <li>Computes distance</li>
      </ul>
      <li>GM</li>
      <ul>
        <li>Probability of point belonging to each cluster</li>
        <li>Computes weighted distance</li>
      </ul>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href=""></a>
  </div>
</div> -->

<!-- <div class="card mb-4" id="linear-regression-1">
  <div class="card-body">
    <h2 class="card-title">Linear regression</h2>
    <ul>
      <li>Fits line to data</li>
      <li>Dependent or target variable <strong>y</strong></li>
      <li>Independent or predictor variable <strong>x</strong></li>
      <li>Assumes errors are independently and identically normally distributed</li>
      <li>Single layer neural network with linear activation is identical with linear regression</li>
    </ul>

    <h3 class="card-title">Linear classifier</h3>
    <ul>
      <li>\( \textbf{y} = \textbf{W}\textbf{x} + \textbf{b} \)</li>
      <ul>
        <li>\( \textbf{y} \) is output of the classifier</li>
        <li>\( \textbf{x} \) is \( N \) dimensional vector</li>
        <li>\( \textbf{W} \) is \( N \times N \) matrix</li>
      </ul>
      <li>Data are provided in pairs \( (x,t) \)</li>
      <ul>
        <li>\( x \) is input to the classifer</li>
        <li>\( t \) is label</li>
      </ul>
    </ul>

    <h3 class="card-title">Linear classifier with 5 inputs and 10 outputs</h3>

<pre><code class="python">import torch
import torch.nn as nn

classifier = nn.Linear(5, 10)</code></pre>

    <h3 class="card-title">Mean squared error</h3>
    <ul>
      <li>\( \displaystyle\sum_{i=0}^n (y_{i} - t_{i})^2 \)</li>
      <li>Goal is to find \( \textbf{W} \) that minimizes the cost</li>
    </ul>

<pre><code class="python">import torch
import torch.nn as nn

model = nn.Linear(10,3)
loss = nn.MSELoss()

## Use random input x.
input_vector = torch.randn(10)

## Assume classiciation with 3 clases.
target = torch.tensor([0,0,1])

pred = model(input_vector)
output = loss(pred, target)
print("Prediction: " ,pred)
print("Output: " , output)</code></pre>

    <h3 class="card-title">Solve linear regression</h3>
    <ul>
      <li>Matrix algebra
        <ul>
          <li>Inverse of matrix may not be computable.</li>
          <li>Singular value decomposition could be used to compute the whole expression including the inverse.</li>
        </ul>
      </li>
      <li>Gradient descent</li>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://github.com/alexeygrigorev/data-science-interviews/blob/master/theory.md">Theoretical interview questions</a> | <a href="https://www.educative.io/path/become-a-deep-learning-professional">Become a Deep Learning Professional</a> | <a href="https://github.com/alexeygrigorev/data-science-interviews/blob/master/theory.md">Theoretical interview questions</a>
  </div>
</div> -->

<!-- <div class="card mb-4" id="machine-learning-">
  <div class="card-body">
    <h2 class="card-title">Dimensionality</h2>

    <h3 class="card-title">Curse of dimensionalty</h3>
    <ul>
      <li>High dimensional data is extremely sparse</li>
      <li>It's hard to do machine learning on sparse data</li>
    </ul>

    <h3 class="card-title">Sigular value decomposition</h3>
    <ul>
      <li>Refactor a matrix into three pieces - left matrix, diagonal matrix, right matrix</li>
    </ul>

    <h3 class="card-title">Priciple component analysis</h3>
    <ul>
      <li>Special type of SVD</li>
      <li>Left matrix and right matrix are eigenvectors</li>
      <li>Diagonal matrix is eigenvalues</li>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href=""></a>
  </div>
</div> -->

<!-- <div class="card mb-4" id="machine-learning-">
  <div class="card-body">
    <h2 class="card-title">Recommender system</h2>
    <ul>
      <li>Relevant and personalized information</li>
      <li>Should not be something users know well</li>
      <li>Diverse suggestions</li>
      <li>Users should explore new items</li>
    </ul>

    <h3 class="card-title">Collaborative filtering</h3>
    <ul>
      <li>Recommendation is calculated as average of other experiences</li>
      <li>Does not work well on sparse data, also has <strong>cold start</strong> problem</li>
      <ul>
        <li>Cannot make recommendation for new item</li>
        <li>Cannot find similarity with other users for new user</li>
      </ul>
      <li>Item-based - rate an item based on ratings by users similar to current user</li>
      <li>User-based - rate an item based on similar items that current user rated</li>
    </ul>

    <h3 class="card-title">Content-based filtering</h3>
    <ul>
      <li>An approach to solve cold start problem</li>
      <li>Recommend items that are similar to items that user liked already</li>
      <li>Do not take other users into consideration</li>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href=""></a>
  </div>
</div> -->

<!-- <div class="card mb-4" id="machine-learning-">
  <div class="card-body">
    <h2 class="card-title">Tranfer learning</h2>
    <ul>
      <li>When transfer learning A to B</li>
      <ul>
        <li>Task A and B have the same input</li>
        <li>Task A has a lot more data than task B</li>
      </ul>
      <li>Transfer learning is almost always used in computer vision</li>
      <li>Example</li>
      <ul>
        <li>Use image recognition to do x-ray diagnosis</li>
        <li>Delete the last layer and weights that feed to the last layer</li>
        <li>Create new randomly initialized weights for the last layer</li>
        <li>Retain the network with x-ray dataset</li>
        <li>In this case, training the network for image recognition task is called <strong>pre-training</strong></li>
        <li>In this case, training the network for image x-ray diagnosis is called <strong>fine-tuning</strong></li>
      </ul>
    </ul>

    <h3 class="card-title">Fine tuning</h3>
    <ul>
      <li>Take a pre-trained model and train it on more specific dataset</li>
      <li>Make model perform better in specific tasks that were not covered during pre-training</li>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://www.deeplearning.ai/deep-learning-specialization/">Deep Learning Specialization</a>
  </div>
</div> -->

<!-- <div class="card mb-4" id="machine-learning-">
  <div class="card-body">
    <h2 class="card-title">Multitask learning</h2>
    <ul>
      <li>Cost function </li>
      <ul>
        <li>\( J = \dfrac{1}{m}\displaystyle\sum_{i=1}^{m}\sum_{j=1}^{C} L(\hat{y}_{i}^{(j)},y_{i}^{(j)}) \)</li>
        <li>\( C \) is the number of different tasks</li>
        <li>Unlike softmax regression, each data can have multiple labels</li>
      </ul>
      <li>Used when there is shared low-level features</li>
      <li>Used when amount of data for each task is similar</li>
      <li>Not used very much in practice</li>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://www.deeplearning.ai/deep-learning-specialization/">Deep Learning Specialization</a>
  </div>
</div> -->
<!-- Machine learning END -->

</div> <!-- /.col-md-12 -->
</div> <!-- /.row -->
</div> <!-- /.container -->

<include src="/footer.html"></include>

</body>

</html>