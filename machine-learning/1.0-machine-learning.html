<!DOCTYPE html>

<html lang="en">

<head>

<!-- Metadata -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="">
<meta name="author" content="Seungmoon Rieh">
<meta name="keywords" content="">

<!-- Title and image -->
<title>Seungmoon Rieh</title>
<link href="/img/seungmoonrieh.jpg" rel="icon">

<!-- CSS -->
<link href="/css/bootstrap.min.css" rel="stylesheet">
<link href="/css/monokai-sublime.css" rel="stylesheet">
<link href="/css/site.css" rel="stylesheet">

<!-- JavaScript -->
<script src="/js/jquery.min.js" type="text/javascript"></script>
<script src="/js/bootstrap.bundle.min.js" type="text/javascript"></script>
<script src="/js/highlight.pack.js" type="text/javascript"></script>
<script src="/js/include_html.js" type="text/javascript"></script>
<script src="/js/mathjax/tex-chtml.js" type="text/javascript"></script>
<script src="/js/site.js" type="text/javascript"></script>

</head>

<body>

<include src="/header.html"></include>

<div class="container">
<div class="row">
<div class="col-md-12">
<h1 class="my-4">Machine Learning</h1>

<!-- Machine learning BEGIN -->
<div class="card mb-4" id="subject">
  <div class="card-body">
    <h2 class="card-title">Machine learning</h2>
    <!-- <ul class="list-unstyled mb-0"> -->
    <ul>
      <li><a href="#machine-learning-">MLP</a></li>
      <ul>
        <li><a href="#machine-learning-">Statistics</a></li>
        <li><a href="#machine-learning-">Machine learning</a></li>
        <li><a href="#machine-learning-">Supervised learning</a></li>
        <li><a href="#machine-learning-">Unsupervised learning</a></li>
        <li><a href="#machine-learning-">Feature engineering</a></li>
        <li><a href="#machine-learning-">Bias and variance</a></li>
        <li><a href="#machine-learning-">Classification</a></li>
        <!-- <li><a href="#machine-learning-">Reinforcement learning</a></li> -->
        <!-- <li><a href="#machine-learning-">Dimentionality reduction</a></li> -->
        <!-- <li><a href="#machine-learning-">Recommendation system</a></li> -->
      </ul>
      <li><a href="#machine-learning-">NN</a></li>
      <ul>
        <li><a href="#machine-learning-">Neural network</a></li>
        <li><a href="#machine-learning-">Activation</a></li>
        <li><a href="#machine-learning-">Normalization</a></li>
        <li><a href="#machine-learning-">Initialization</a></li>
        <li><a href="#machine-learning-">Regularization</a></li>
        <li><a href="#machine-learning-">Optimization</a></li>
        <li><a href="#machine-learning-">Learning rate</a></li>
      </ul>
      <!-- <li><a href="#machine-learning-">CNN</a></li>
      <ul>
        <li><a href="#machine-learning-">CNN</a></li>
      </ul> -->
      <li><a href="#machine-learning-">NLP</a></li>
      <ul>
        <li><a href="#machine-learning-">NLP</a></li>
        <li><a href="#machine-learning-">LLM</a></li>
      </ul>
      <li><a href="#machine-learning-">Tree</a></li>
      <ul>
        <li><a href="#machine-learning-">Decision tree</a></li>
      </ul>
      <li><a href="#machine-learning-">Engineering</a></li>
      <ul>
        <li><a href="#machine-learning-">Data</a></li>
        <li><a href="#machine-learning-">Predictive</a></li>
        <li><a href="#machine-learning-">Generative</a></li>
        <!-- <li><a href="#machine-learning-">Non-functional</a></li>
        <li><a href="#machine-learning-">Deployment</a></li>
        <li><a href="#machine-learning-">Cache</a></li>
        <li><a href="#machine-learning-">Load balancing</a></li>
        <li><a href="#machine-learning-">Distributed system</a></li> -->
      </ul>
    </ul>
  </div>
</div>

<div class="card mb-4" id="machine-learning-">
  <div class="card-body">
    <h2 class="card-title">MLP</h2>

    <h3 class="card-title">Statistics</h3>
    <ul>
      <li>Explain central limit theorem</li>
      <ul>
        <li>If take large enough sample from population, distribution will be normally distributed</li>
      </ul>
      <li>Explain Hypothesis testing and p-value</li>
      <ul>
        <li>Null hypothesis - there is no relationship</li>
        <li>Alternative hypothesis - there is relationship</li>
        <li>Low p-value - low probability that observed results are due to random chance, null hypothesis is rejected</li>
        <li>High p-value - high probability that observed results are due to random chance, null hypothesis is not rejected</li>
      </ul>
      <li>Explain difference between Type I and Type II error</li>
      <ul>
        <li>Type 1 error - rejecting the null hypothesis when it is actually true (False positive)</li>
        <li>Type 2 error - failing to reject the null hypothesis when it is actually false (False negative)</li>
      </ul>
      <li>Explain correlation and correlation coefficient</li>
      <ul>
        <li>Both strength and direction of relationship between two variables</li>
        <li>Coefficient - positive or negative depending on direction, magnitude represents strength</li>
      </ul>
      <li>Explain statistical power</li>
      <ul>
        <li>Likelyhood that study will find effect when in fact there is effect</li>
        <li>Higher statistical power, less likely to make false negative</li>
      </ul>
      <li>Explain how to deal with anomaly</li>
      <ul>
        <li>68% of data is one std away</li>
        <li>95% of data is two std away</li>
        <li>9% of data is three std away</li>
        <li>Statistical method - consider data point with 3 std away as outlier and likely anomaly</li>
        <li>Metric method - a point is considered anomaly if removing it significantly improves the model</li>
      </ul>
      <!-- <li>Explain Pearson’s correlation coefficient</li>
      <li>Explain Spearman’s correlation coefficient</li> -->
    </ul>

    <h3 class="card-title">Machine learning</h3>
    <ul>
      <li>Explain how to explain machine learning to a kid</li>
      <ul>
        <li>Computers watch what others do, learn and copy that actions</li>
        <li>Computers learn from previous mistakes</li>
      </ul>
      <li>Explain difference between supervised, unsupervised, and reinforcement learning</li>
      <ul>
        <li>Supervised - data is labeld (regression/classification, NN/tree)</li>
        <li>Unsupervised - data is not labeled, learn pattern from data (clustering/dimensionaity-reduction)</li>
        <li>Reinforcement - learn to maximize reward via trial and error</li>
      </ul>
    </ul>

    <h3 class="card-title">Supervised learning</h3>
    <ul>
      <li>Explain regression algorithm</li>
      <ul>
        <li>Predict continuous variable from a set of features</li>
        <li>Ex. linear regression</li>
        <li>Evaluation - MSE (computation is simpler), MAE (more robust to outliers)</li>
      </ul>
      <li>Explain classification algorithm</li>
      <ul>
        <li>Predict category or class from a set of features</li>
        <li>Ex. logistic regression</li>
        <li>Evaluation - precision, recall, accuracy, F1</li>
        <li>Dataset is bernoulli (binary) or multinomial (multi-class)</li>
      </ul>
      <li>Explain linear regression</li>
      <ul>
        <li>Learn coefficients from training data and inference using only independant variables</li>
        <li>Assume linear relationship between features and target</li>
        <li>Assume errors are independently and identically normally distributed</li>
        <li>Assume data are independent</li>
      </ul>
      <li>Explain logistic regression</li>
      <ul>
        <li>Assume linear relationship between input features and log-odds of outputs</li>
        <li>Uses sigmoid/softmax</li>
      </ul>
      <li>Explain neural network algorithm</li>
      <ul>
        <li>Data is fed to input layer</li>
        <li>Each neuron in hidden layer calculates based on input, weight, and activation function</li>
        <li>Output layer generates prediction</li>
        <li>Loss (or cost, which is sum of losses) is computed using prediction and label</li>
        <li>Gradient of loss w.r.t. weights and bias (which minimizes the cost) are computed</li>
        <li>Weights and bias are updated using gradients</li>
        <li>This process iterates over many epoch</li>
      </ul>
    </ul>

    <h3 class="card-title">Unsupervised learning</h3>
    <ul>
      <li>Explain KNN</li>
      <ul>
        <li>From a point, find k closest number of points</li>
        <li>No training needed</li>
        <li>High K leads to high bias and low variance</li>
        <li>Classification - return majority vote of nearest neighbors</li>
        <li>Regression - return average value of nearest neighbors</li>
      </ul>
      <li>Explain ANN</li>
      <ul>
        <li>Approximate k points that are closest to a datapoint</li>
      </ul>
      <li>Explain clustering algorithm</li>
      <ul>
        <li>Unsupervised learning</li>
        <li>Group data points into clusters based on their similarity</li>
        <li>Ex. K-means, DBSCAN</li>
      </ul>
      <li>Explain k-means clustering</li>
      <ul>
        <li>Partition data into K clusters</li>
        <li>Randomly pick centroids of each cluster</li>
        <li>Assign points to closest cluster, then update centroid</li>
        <li>Repeat until convergence</li>
      </ul>
      <li>Explain convergence in k-means clustering</li>
      <ul>
        <li>Algorithm finished grouping data points into k clusters</li>
        <li>It is when the centroid of last two clusters are very similar</li>
      </ul>
      <li>Explain how to choose optimum number of clusters</li>
      <ul>
        <li>Elbow method - plot variation (within-cluster sum of squared errors) against number of clsuters, when the curve elbows, adding more cluster does not benefit</li>
        <li>Silhouette score - measures how similar an object is to its own cluster versus other clusters (iterate over different values of k to see which k results in best average score), more computationally expensive than elbow method</li>
      </ul>
    </ul>

    <h3 class="card-title">Feature engineering</h3>
    <ul>
      <li>Explain why feature selection is needed</li>
      <ul>
        <li>Too many features can cause overfitting</li>
        <li>Become technical debt</li>
        <li>Slower inference if dynamic feature computation is needed</li>
        <li>Increase memory requirement</li>
      </ul>
      <li>Explain feature selection technique</li>
      <ul>
        <li>Filter - evaluate each feature with target one by one, but this ignores feature interactions (Ex. chi-squared test)</li>
        <li>Wrapper - try different combination of features (Ex. recursive feature elimination)</li>
        <li>Embedded - during training, model decides which features are important (Ex. LASSO regression)</li>
      </ul>
      <li>Explain SHAP (SHapley Additive exPlanations)</li>
      <ul>
        <li>Contribution of each feature to prediction</li>
        <li>Compute combination of features and their predictions</li>
      </ul>
      <li>Explain feature scaling</li>
      <ul>
        <li>Help gradient descent to find optimum faster</li>
        <li>Ex. standardization (z-score normalization) where mean is 0 and std is 1</li>
        <li>Assumes data is normallly distributed</li>
      </ul>
      <li>Explain how to encode categorical data</li>
      <ul>
        <li>Integer encoding - assign integer to each category</li>
        <li>One-hot encoding - for dense features, assign binary vector to each category</li>
        <li>Embedding - for sparse features, either learn vector for each categorical value or use pre-trained model</li>
      </ul>
      <li>Explain how to handle missing data</li>
      <ul>
        <li>Delete - data is lost</li>
        <li>Imputation - data gets noisy (supplyment with mean, median, etc)</li>
        <li>Use model that can handel missing data (Ex. XGBoost)</li>
      </ul>
      <li>Explain how to handle imbalanced dataset</li>
      <ul>
        <li>Undersample/oversample</li>
        <li>Data augmentation - crop/rotate images</li>
        <li>Loss function - higher weights to under-represented data</li>
        <li>Change performance metric - use precision, recall, F1 score rather than accuracy</li>
      </ul>
      <li>Explain how to avoid data leakage</li>
      <ul>
        <li>Split time-correlated data by time, not random (otherwise, information from future is leaked to training set)</li>
        <li>Split data first, then scaling (otherwise, statistics of test set can be leaked into training set)</li>
        <li>Do not fill missing data from statistics of test set</li>
        <li>Oversample data only after splitting data</li>
      </ul>
    </ul>

    <h3 class="card-title">Bias and variance</h3>
    <ul>
      <li>Explain bias-variance trade-off</li>
      <ul>
        <li>Challenge to fight both underfitting and overfitting the model to data</li>
        <li>High bias and low variance - linear regression</li>
        <li>Low bias and high variance - neural network</li>
      </ul>
      <li>Explain underfitting and how to tackle it</li>
      <ul>
        <li>High bias, training error, model does not work well on training data</li>
        <li>More data</li>
        <li>Better/bigger model</li>
        <li>Train longer</li>
      </ul>
      <li>Explain overfitting and how to tackle it</li>
      <ul>
        <li>High variance, validation error, model works well on training data but not well on unseen data</li>
        <li>More data</li>
        <li>Less features</li>
        <li>Reduce model complexity</li>
        <li>Regularization</li>
        <li>Make sure validation/test set come from same distribution</li>
      </ul>
      <li>Explain why we use validation and test set</li>
      <ul>
        <li>Validation set - used for hyparameters tunung, use data unseen in training set</li>
        <li>Test set - used to test model performance, use data unseen in training and validation set</li>
      </ul>
      <li>Explain cross-validation</li>
      <ul>
        <li>Split training set into different bins</li>
        <li>One bin is set aside as test set and all others as training set</li>
        <li>Train on training set using test set as label</li>
        <li>Average accuracies to compute the final accuracy score</li>
        <li>Base population must not co-exist between bins</li>
      </ul>
      <li>Explain how to debug model not performing well on test data</li>
      <ul>
        <li>Not enough data</li>
        <li>Data that is not representative</li>
        <li>Data with outlier, error, noise, missing values</li>
        <li>Irrelevant features</li>
      </ul>
      <li>Explain how to perfrom error analysis</li>
      <ul>
        <li>Training-dev set - same distribution as training set, but not used in training</li>
        <li>If dev error >> training error, it could be because both variance and distribution mismatch</li>
        <li>If training-dev error >> training error, it is variance problem</li>
        <li>If dev error >> training-dev error, it is not a variance problem. It is data mismatch problem</li>
      </ul>
    </ul>

    <h3 class="card-title">Classification</h3>
    <ul>
      <li>Explain confusion matrix</li>
      <ul>
        <li>Table showing predicted versus actual (true positive, false negative, false positive, true negative)</li>
      </ul>
      <li>Explain difference between precision and recall</li>
      <ul>
        <li>Precision - how often model is correct when predicting target class, true positive / (true positive + false positive)</li>
        <li>Recall - whether model can find target class, true positive / (true positive + false negative)</li>
      </ul>
      <li>Explain difference between PR-AUC and ROC-AUC</li>
      <ul>
        <li>ROC-AUC - true positive vs false positive, area beneath the curve (If 0.5, random guess, If 1, perfect model)</li>
        <li>PR-AUC - precision vs recall, area beneath the curve</li>
      </ul>
      <li>Explain accuracy</li>
      <ul>
        <li>What percentage of predictions were correct?</li>
        <li>(True Positive + True Negative) / (True Negative + True Positive + False Negative + False Positive)</li>
      </ul>
      <li>Explain F1 score</li>
      <ul>
        <li>Average of precision and recall</li>
      </ul>
      <li>Explain how to find threshold for classifier</li>
      <ul>
        <li>ROC-AUC</li>
        <li>PR-AUC</li>
      </ul>
      <li>Explain when to use which metric</li>
      <ul>
        <li>Precision - when missing target class is less critical than incorrectly labeling target class (Ex. spam detection)</li>
        <li>Recall - when incorrect lable is less important than missing target class (Ex. fraud detection)</li>
        <li>Both - (Ex. cancer detection)</li>
      </ul>

      <!-- <h3 class="card-title">Reinforcement learning</h3>
      <ul>
        <li>Explain Markov Decision Process for reinforcement learning</li>
        <li>Explain main component of reinforcement learning agents</li>
        <li>Explain how to define reward function</li>
        <li>Explain difference between model-based and model-free reinforcement learning</li>
        <li>Explain the role of discount factor</li>
        <li>Explain exploration-exploitation trade-off</li>
        <li>Explain difference between policy gradient and value iteration method</li>
        <li>Explain State (Q) and Action-Value (Q) functions</li>
        <li>Explain how to handle continuous action space</li>
        <li>Explain deep reinforcement learning</li>
        <li>Explain how to ensure convergence of reinforcement learning</li>
        <li>Explain how multi-agent reinforcement learning system work</li>
      </ul> -->

      <!-- <h3 class="card-title">Dimension reduction</h3>
      <ul>
        <li>Explain how to tackle curse of dimensionality</li>
        <ul>
          <li>High dimensional data is extremely sparse</li>
          <li>It's hard to do machine learning on sparse data</li>
        </ul>
        <li>Explain principal component analysis</li>
        <ul>
          <li>Special type of SVD</li>
          <li>Left matrix and right matrix are eigenvectors</li>
          <li>Diagonal matrix is eigenvalues</li>
        </ul>
        <li>Explain t-SNE</li>
        <li>Explain sigular value decomposition</li>
        <ul>
          <li>Refactor a matrix into three pieces - left matrix, diagonal matrix, right matrix</li>
        </ul>
      </ul> -->

      <!-- <li>Recommendation system</li>
      <ul>
        <li>Explain collaborative filtering</li>
        <ul>
          <li>Recommendation is based on historical user-item interaction</li>
          <li>Cold start problem - cannot make recommendation for new item, cannot find similarity with other users for new user</li>
          <li>Item-based - rate an item based on ratings by users similar to current user</li>
          <li>User-based - rate an item based on similar items that current user rated</li>
        </ul>
        <li>Explain content-based filtering</li>
        <ul>
          <li>An approach to solve cold start problem</li>
          <li>Recommend items that are similar to items that user liked already</li>
          <li>Do not take other users into consideration</li>
        </ul>
      </ul> -->
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: 
  </div>
</div>

<div class="card mb-4" id="machine-learning-">
  <div class="card-body">
    <h2 class="card-title">NN</h2>

    <h3 class="card-title">Neural network</h3>
    <ul>
      <li>Explain cost function</li>
      <ul>
        <li>Loss is a function that computes difference between predicted and label</li>
        <li>Cost is sum of losses over all training batch</li>
      </ul>
      <li>Explain difference between epoch, batch, and iteration</li>
      <ul>
        <li>Epoch - number of times to loop through all training data</li>
        <li>Batch - amount of training data to make a single gradient update</li>
        <li>Iteration - number of times to train model through all epoch under a set of hyperparameters</li>
      </ul>
      <li>Explain difference between single-layer perception and multi-layer perception</li>
      <ul>
        <li>Single-layer perception - single layer network with linear activation, which is equivalent to linear regression</li>
        <li>Multi-layer percention - has at least one hidden layer</li>
      </ul>
      <li>Explain what is meant by depth in neural network</li>
      <ul>
        <li>Number of layers (input layer plus hidden layers, not counting output layer)</li>
      </ul>
      <li>Explain significance of bias term in neural network</li>
      <ul>
        <li>Without bias term, activation function curve always passes through origin</li>
        <li>Bias allows activation function to be shifted, making it more flexible to fit data well</li>
      </ul>
      <li>Explain challenges with very deep neural network</li>
      <ul>
        <li>Exploding and vanishing gradient - use gradient clipping</li>
        <li>Overfitting - use regularization</li>
        <li>Slow convergence - use adaptive learning rate and learning rate scheduler</li>
      </ul>
      <li>Explain why we shuffle training data after each epoch</li>
      <ul>
        <li>Model may learn pattern from ordering of data</li>
      </ul>
      <li>Explain hyperparameter tuning</li>
      <ul>
        <li>Hold out validation set</li>
        <li>Try combinations of hyperparameters and test model performance on validation set</li>
        <li>Scale parameters using log scale (Ex. 0.0001 to 1, use 0.0001, 0.001, 0.01, 0.1, 1)</li>
        <li>Ex. grid search, random search, bayesian optimization</li>
        <li>Iterate and pick the best combination</li>
      </ul>
      <li>Explain which hyperparameter are important</li>
      <ul>
        <li>Learning rate is most important</li>
        <li>Mini-batch size, the number of hidden units are second important</li>
        <li>The number of layers and learning rate decay are third important</li>
      </ul>
      <li>Explain logits</li>
      <ul>
        <li>Unnormalized output of model</li>
      </ul>
    </ul>

    <h3 class="card-title">Activation</h3>
    <ul>
      <li>Explain activation function</li>
      <ul>
        <li>Function that determines outputs of neuron</li>
        <li>Add non-linearity to allow network to learn more complex pattern</li>
      </ul>
      <li>Explain why ReLU is used more than sigmoid in neural network</li>
      <ul>
        <li>Sigmoid - function flattens near zero, leading to vanishing gradient</li>
        <li>ReLU - neuron does not active for negative input, thus addresses vanishing gradient</li>
      </ul>
    </ul>

    <h3 class="card-title">Normalization</h3>
    <ul>
      <li>Explain normalization and why we use it</li>
      <ul>
        <li>If features are not scaled, contours can be assymmetric, gradient descent can take long time</li>
        <li>If features are scaled, contours can be symmetric, gradient decent can find optimum faster</li>
      </ul>
      <li>Explain difference between normalization and standardization</li>
      <ul>
        <li>Normalization - values are between 0 and 1, does not change distribution</li>
        <li>Standardization - values have mean 0 and std 1, changes to normal distribution</li>
      </ul>
      <li>Explain difference between batch normalization and layer normalization</li>
      <ul>
        <li>Batch normalization - take a batch and normalize feature by feature</li>
        <li>Layer normalization - take all features and normalize data point by data point</li>
      </ul>
    </ul>

    <h3 class="card-title">Initialization</h3>
    <ul>
      <li>Explain weight initialization methods like Xavier, He</li>
      <ul>
        <li>He - initialize weights to small random values (If values are too large or too small, it can lead to exploding/vanish gradient)</li>
      </ul>
      <li>Explain what happens if weights are initialized to same values</li>
      <ul>
        <li>All neurons in each layer output the same value</li>
      </ul>
      <li>Explain weight constraints and how it can benefit training</li>
      <ul>
        <li>During training, if weights exceed certain value, keep it as threshold</li>
        <li>This helps preventing overfitting</li>
      </ul>
    </ul>

    <h3 class="card-title">Regularization</h3>
    <ul>
      <li>Explain regularization and why we use it</li>
      <ul>
        <li>Larger lambda leads W to become smaller, reducing the impacts of hidden units</li>
        <li>Smaller W also leads Z to become smaller</li>
        <li>The activation function could more or less be linear function when Z is small</li>
        <li>Thus, network does not compute complex function</li>
      </ul>
      <li>Explain difference between L1 and L2 regularization</li>
      <ul>
        <li>L1 - add sum of absolute weights to loss function, works well on sparse data, features can be removed</li>
        <li>L2 - add sum of squares of weights to loss function, works well when features are corelated</li>
      </ul>
      <li>Explain dropout</li>
      <ul>
        <li>Neurons are ramdonly shut down with probability p</li>
        <li>Neurons become reluctant to put too much weight on one feature</li>
        <li>Weight values can be smaller and more spread out</li>
      </ul>
      <li>Explain how early stopping prevents overfitting</li>
      <ul>
        <li>Stop training when validation error increases</li>
        <li>Tackle bias and variance at the same time (rather than one at a time)</li>
      </ul>
    </ul>

    <h3 class="card-title">Optimization</h3>
    <ul>
      <li>Explain batch size and how it is related to convergence</li>
      <ul>
        <li>Small batch - frequent and noisy gradient updates, can escape local optima well but too much osciallation and slow down convergence</li>
        <li>Large batch - stable gradient updates, may get stuck in local optima</li>
        <li>Balanced batch is needed</li>
      </ul>
      <li>Explain vanishing and exploding gradient</li>
      <ul>
        <li>Vanishing - gradient becomes very small</li>
        <li>Exploding - gradient becomes very large</li>
      </ul>
      <li>Explain how to deal with vanishing gradient</li>
      <ul>
        <li>ReLu</li>
        <li>Weight initialization</li>
        <li>Small learning rate</li>
        <li>Residual connection</li>
        <li>Batch/layer normalization</li>
      </ul>
      <li>Explain how to deal with exploding gradient</li>
      <ul>
        <li>Gradient clipping</li>
        <li>Weight initialization</li>
        <li>Small learning rate</li>
        <li>Residual connection</li>
        <li>Batch/layer normalization</li>
      </ul>
      <li>Explain momentum</li>
      <ul>
        <li>On vertical axis (b), we want slower learning to reduce oscilation</li>
        <li>On horizontal axis (W), we want faster learning</li>
        <li>Apply additional computation when updating W and b with velocity of W and b</li>
      </ul>
      <li>Explain RMSProp</li>
      <ul>
        <li>Similar idea from momentum, but different calculation</li>
      </ul>
      <li>Explain ADAM optimizer</li>
      <ul>
        <li>Combine calculations from momentum and RMSProp</li>
      </ul>
      <li>Explain difference between batch, min-batch, and stochastic gradient descent</li>
      <ul>
        <li>Batch - use all training data to make a single gradient update, stable but slow</li>
        <li>Min-batch - use subset of training data</li>
        <li>Stachastic - use a single data, fast but final parameters may not be optimal</li>
      </ul>
      <li>Explain difference between global optima and local optima</li>
      <ul>
        <li>Global - optimum value of function in its entire span</li>
        <li>Local - optimum value of function in its subrange</li>
      </ul>
      <li>Explain gradient clipping and how it help training</li>
      <ul>
        <li>If gradient exceed certain value, set it to threshold</li>
        <li>Prevents exploding gradient</li>
      </ul>
    </ul>

    <h3 class="card-title">Learning rate</h3>
    <ul>
      <li>Explain learning rate</li>
      <ul>
        <li>In gradient descent, size of step you want to take in particular direction</li>
      </ul>
      <li>Explain adaptive learning rate</li>
      <ul>
        <li>Bigger learning rate in the beginning and smaller near the optimum</li>
      </ul>
      <li>Explain learning rate scheduler</li>
      <ul>
        <li>Similar to adaptive learning rate, but uses pre-defined schedule</li>
        <li>Normally consists of warm up, constant hold, and exponential phases</li>
      </ul>
      <li>Explain why learning rate is considered as most important hyperparameter</li>
      <ul>
        <li>It directly controls how quickly model adjusts its weights during training</li>
      </ul>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: 
  </div>
</div>

<!-- <div class="card mb-4" id="machine-learning-">
  <div class="card-body">
    <h2 class="card-title">CNN</h2>

    <h3 class="card-title">CNN</h3>
    <ul>
      <li>Explain how convolution works, when inputs are gray-scale or RGB</li>
      <li>Explain why we use convolution layer rather than just FC layers</li>
      <li>Explain why we use many small kernels like 3 by 3 rather than few large kernels</li>
      <li>Explain why we use max-pooling and how it is different from average pooling</li>
      <li>Explain ResNet and skip connection</li>
      <li>Explain non-max suppression</li>
      <li>Explain data augmentation</li>
      <li>Explain autoencoder</li>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: 
  </div>
</div> -->

<div class="card mb-4" id="machine-learning-">
  <div class="card-body">
    <h2 class="card-title">NLP</h2>

    <h3 class="card-title">NLP</h3>
    <ul>
      <li>Explain RNN</li>
      <ul>
        <li>Neural network that process words timestep by timestep</li>
        <li>Uses previous words to predict the next word</li>
      </ul>
      <li>Explain difference between feed forward network and recurrent network</li>
      <ul>
        <li>Feed forward - suitable for problems having single output, neurons have different weights</li>
        <li>RNN - suitable for sequential input data, all neurons in a layer have the same weights</li>
      </ul>
      <li>Explain LSTM</li>
      <ul>
        <li>RNN that captures long-term dependencies in sequential data</li>
        <li>Uses memory cells and gates to remember or forget information</li>
      </ul>
      <li>Explain how LSTM handle vanishing gradient</li>
      <ul>
        <li>Memory cells keep gradient consistant over long sequence</li>
      </ul>
      <li>Explain word embedding and how it is different from one-hot encoding</li>
      <ul>
        <li>Representing word by numerical vector such that words with similar meaning has close distance</li>
        <li>One-hot encoding can be high dimensional but embedding can be low dimensional</li>
      </ul>
      <li>Explain sentence embedding</li>
      <ul>
        <li>Tokenize sentence into words</li>
        <li>Compute word embeddings and aggregate</li>
      </ul>
      <li>Explain tokenization</li>
      <ul>
        <li>Split paragraphs or sentences into words (removing punctation, lower casing, etc)</li>
        <li>Rare words can be broken down into subwords while common words can be kept as they are</li>
        <li>High quality data (No abbrevation, spelling mistakes) is needed</li>
      </ul>
      <li>Explain Word2Vec</li>
      <ul>
        <li>NLP technique to learn embeddings of words</li>
        <li>For example, CBOW and skip-gram</li>
      </ul>
      <li>Explain CBOW</li>
      <ul>
        <li>Predict word by surrounding words, fill-in-the-blank task</li>
      </ul>
      <li>Explain Skip-gram</li>
      <ul>
        <li>Predict surrounding words by target word</li>
      </ul>
    </ul>

    <h3 class="card-title">LLM</h3>
    <ul>
      <li>Explain challenges in LLM</li>
      <ul>
        <li>Knowledge is limited to what was available at the time of training</li>
        <li>LLM tends to make things up</li>
        <li>Input and output length is limited</li>
        <li>Does not work well with structured (tabular) data</li>
        <li>Can reflect bias</li>
        <li>Not good at precise math</li>
      </ul>
      <li>Explain transfer learning and its benefits</li>
      <ul>
        <li>Task A and B have the same input</li>
        <li>Task A has a lot more data than task B</li>
        <li>Delete the last layer and weights that feed to the last layer</li>
        <li>Create new randomly initialized weights for the last layer</li>
        <li>Retain the network with dataset for task B</li>
      </ul>
      <li>Explain pre-training and fine-tuning</li>
      <ul>
        <li>In this case, training the network for task A is called <strong>pre-training</strong></li>
        <li>In this case, training the network for task B is called <strong>fine-tuning</strong></li>
      </ul>
      <li>Explain self-attention</li>
      <ul>
        <li>Every word (token) is related to every other word (token)</li>
        <li>Input sentence is tokenized and represented by Q,K,V whose dimension is batch_size, seq_len, d_model</li>
        <li>Q - words that we are focusing on, K - words that we are comparing against, V - multipled to softmax output</li>
      </ul>
      <li>Explain attention mask</li>
      <ul>
        <li>Separate important and unimportant tokens in input</li>
      </ul>
      <li>Explain transformer</li>
      <ul>
        <li>Model architecture used to process sequence data, particularly relating input and output sequences</li>
        <li>Ex. machine translation</li>
        <li>Encoder - input embedding, positional encoding, multi-head attention, feedforward, residual connection</li>
        <li>Decoder - output embedding, positional encoding, masked multi-head attention, multi-head attention, feedforward, residual connection</li>
      </ul>
      <li>Explain drawbacks of transformer</li>
      <ul>
        <li>Self attention runs in \( O(N^{2}D )\) where \( N \) is sequence length and \( D \) is dimension of words embedding</li>
        <li>Capturing word/token interaction requires large number of parameters, thus large memory</li>
      </ul>
      <li>Explain regularization used in transformer</li>
      <ul>
        <li>Dropout applied to fully connected layer</li>
      </ul>
      <li>Explain normalization used in transformer</li>
      <ul>
        <li>Layer normalization is typically applied just before self-attention layer</li>
        <li>Batch normalization is not normally used due to varying sequence length</li>
      </ul>
      <li>Explain how to achieve fast inference on transformer model</li>
      <ul>
        <li>Sparse attention or local attention</li>
        <li>Data parallelism or model parallelism</li>
        <li>Lower precision floating format</li>
        <li>Dynamic padding sequence</li>
      </ul>
      <li>Explain BERT</li>
      <ul>
        <li>Bi-drectional encoder representation from transformer</li>
        <li>Model architecture used to understand contexts of words</li>
        <li>Ex. question answering, sentiment analysis, text classification</li>
        <li>Encoder only</li>
      </ul>
      <li>Explain GPT-3</li>
      <ul>
        <li>Generative pre-trained transformers</li>
        <li>Model architecture used to generate texts</li>
        <li>Ex. text generation</li>
        <li>Decoder only</li>
      </ul>
      <li>Explain RAG</li>
      <ul>
        <li>Retrieval augmented generation</li>
        <li>Populate DB with information not available in LLM</li>
        <li>Using query embedding, search relevant information by embedding similairty</li>
        <li>Provide LLM with query and context retrieved from DB</li>
      </ul>
      <li>Explain LoRA</li>
      <ul>
        <li>Low ranking adaption</li>
        <li>Fine-tuning LLM with lower number of parameters</li>
        <li>Freeze weights of pre-trained model and train low rank matrix (adapter)</li>
      </ul>
      <li>Explain PPO</li>
      <ul>
        <li>Proximal policy optimization</li>
        <li>Policy gradient method that trains agent via rules</li>
        <li>Can be used in fune-tuning LLM to generate better dialogue</li>
      </ul>
      <li>Explain RLHF</li>
      <ul>
        <li>Reinforment learning from human feedback</li>
        <li>Human evaluates LLM outputs (rating, text summary, etc)</li>
        <li>Train reward model to predict how human would evaluate LLM response</li>
        <li>Fine-tune LLM via reinforment learning with reward model</li>
      </ul>
      <li>Explain multi-modal models</li>
      <ul>
        <li>Model that supports multiple types of inputs (text, image, audio, video)</li>
      </ul>
      <li>Explain how stability diffusion model use LLM to understand complex text prompts</li>
      <ul>
        <li>Generate image based on text description</li>
        <li>Combine response from LLM and stability diffusion model to provide final response with both text and image</li>
      </ul>
      <li>Explain how to train LLM with billions of parameters</li>
      <ul>
        <li>Text cleaning and tokenization</li>
        <li>Distribute workload among GPU nodes</li>
        <li>Optimize learning rate</li>
      </ul>
      <li>Explain how to train LLM to prevent hallucinations</li>
      <ul>
        <li>Improve data quality and diversity</li>
        <li>Alternatively, RAG/RLHF can be used</li>
      </ul>
      <li>Explain how to prevent bias and harmful prompt</li>
      <ul>
        <li>Use diverse and unharmful data</li>
        <li>Alternatively, RLHF can be used</li>
      </ul>
      <li>Explain how knowledge distillation benefits LLM</li>
      <ul>
        <li>Create smaller model from large model without losing much knowledge</li>
      </ul>
      <li>Explain few shot learning in LLM</li>
      <ul>
        <li>Provide few examples (explaining how LLM should respond to the prompt) in the prompt</li>
      </ul>
      <li>Explain how to evaluate LLM performance</li>
      <ul>
        <li>BLEU (bilingual evaluation understudy) to measure embedding similarity between LLM output and human-generated text</li>
        <li>Hallucination rate - human decides whether LLM hallucinates or not</li>
        <li>Factual accuracy - human decides whether each claim LLM makes in reponse is factual or not</li>
      </ul>
      <li>Explain how to improve factual accuracy of LLM</li>
      <ul>
        <li>Improve data quality</li>
        <li>Fine-tune on domain specific data</li>
        <li>Clear and step-by-step prompt</li>
      </ul>
      <li>Explain how to detect drift in LLM performance over time</li>
      <ul>
        <li>Monitor distribution of prompts and generated response</li>
        <li>Statistical tests (Kolmogorov–Smirnov, Mann–Whitney U-test)</li>
      </ul>
      <li>Explain how to curate dataset for training LLM</li>
      <ul>
        <li>Text cleanup - lower case, remove punctuation</li>
        <li>Tokenize - split sentences into words</li>
        <li>Token to ID - convert each word into ID</li>
      </ul>
      <li>Explain how to fine-tune LLM for domain specific applications like finance</li>
      <ul>
        <li>Prepare dataset for fune-tuning</li>
        <li>Tokenize prepared dataset</li>
        <li>Load pre-trained model</li>
        <li>Freeze weights in pre-trained model and initialize weights for fine-tuning</li>
        <li>Determine hyperparameters</li>
      </ul>
    </ul>        
  </div>
  <div class="card-footer text-muted">
    Reference: 
  </div>
</div>

<div class="card mb-4" id="machine-learning-">
  <div class="card-body">
    <h2 class="card-title">Tree</h2>

    <h3 class="card-title">Decision tree</h3>
    <ul>
      <li>Explain decision tree</li>
      <ul>
        <li>Depth of trees increases variance, trees are pruned to control variance</li>
        <li>Main parameters - maximum tree depth, minimum samples per tree node, impurity criterion</li>
        <li>Classification - return majority vote of trees (branch - test outcome, leaf - class label)</li>
        <li>Regression - return average value of trees</li>
      </ul>
      <li>Explain information gain and entropy</li>
      <ul>
        <li>Metrics used to determine best way to split the tree</li>
        <li>Entropy is measure of uncertainty, information gain calculates reduction in entropy</li>
        <li>Select variable that maximizes the information gain and split dataset into groups based on that variable</li>
      </ul>
      <li>Explain why emsembles have higher scores than individual models</li>
      <ul>
        <li>Because they gather strengths from many models</li>
      </ul>
      <li>Explain bagging</li>
      <ul>
        <li>Train models independently in parallel and combine the result (Ex. random forest)</li>
        <li>Less prone to overfitting, faster training, less hyperparameter tuning</li>
      </ul>
      <li>Explain boosting</li>
      <ul>
        <li>Train models in sequence where later models learn from mistakes from previous models (Ex. gradient boosting)</li>
        <li>Tree is built starting from root, for each leaf, split selected to minimize MSE</li>
        <li>Predictions of individual trees are summed</li>
        <li>More accurate, works well on unbalanced datatset</li>
      </ul>
      <li>Explain why XGBoost is not suitable for continual learning</li>
      <ul>
        <li>Any new data requires the model to be re-trained</li>
        <li>Incremental learning, where updating model without re-training, is not possible</li>
        <li>Model training involves assessing entire dataset, which is slow</li>
      </ul>
      <li>Explain why XGBoost is good at dealing with sparse data</li>
      <ul>
        <li>It can handle one-hot encoded categorical features with any transformation</li>
        <li>Trees can split based on missing values or can treat missing values as distinct feature</li>
        <li>It does not focus on features that don't contribute much to target</li>
      </ul>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: 
  </div>
</div>

<div class="card mb-4" id="machine-learning-">
  <div class="card-body">
    <h2 class="card-title">Engineering</h2>

    <h3 class="card-title">Data</h3>
    <ul>
      <li>Explain how to ingest large amount of data with minimal latency</li>
      <ul>
        <li>Use streaming data processing</li>
        <li>Edge compute at data source location, compress data, then load/transfer</li>
        <li>Parallel process using distributed compute nodes</li>
        <li>Convert data format to protobuf (rather than JSON, XML) for faster processing</li>
      </ul>
      <li>Explain how to handle schema evolution in data lake environment</li>
      <ul>
        <li>Use parquet, which supports adding columns, re-ordering columns without data re-writes</li>
      </ul>
      <li>Explain when to prefer streaming or batch processing</li>
      <ul>
        <li>Streaming - low latency decision-making is needed (Ex. fraud detection), data is written continuously</li>
        <li>Batch - cost effective</li>
      </ul>
      <li>Explain how to design data model for fast lookup and analytical queries</li>
      <ul>
        <li>For fast lookup, prepare normalized and indexed dataset. For queries, prepare de-normalized dataset</li>
      </ul>
      <li>Explain index in database</li>
      <ul>
        <li>Sorting columns to execute binary search</li>
        <li>Need to store sorted data in memory</li>
        <li>Data write is slower</li>
        <li>Not performant when many duplicate data exists</li>
      </ul>
      <li>Explain perfomance bottlenecks in Spark</li>
      <ul>
        <li>Data shuffles - moving data between partitions is expensive</li>
        <li>Data skews - partition with more data, thus slower compute, determine speed of overall execution</li>
      </ul>
      <li>Explain how to build data pipeline to handle duplicate, missing, PII data</li>
      <ul>
        <li>Enforce schema validation on arriving data</li>
        <li>Standardize data - date format, currency, etc</li>
        <li>De-dup - calculate similarity between fields (using algorithms) and merge data</li>
        <li>Missing data rule - impute/delete</li>
        <li>Implement pattern matching rule to detect PII data, then mask them in non-prod and encrypt them in prod</li>
      </ul>
      <li>Explain how to prepare data for feature store</li>
      <ul>
        <li>Data cleaning - de-dup, missing data, PII</li>
        <li>If numbers, scale and normalize</li>
        <li>If categorical - encode (one-hot, embedding)</li>
        <li>If text - tokenize and convert to ID</li>
        <li>If image - convert to 1D vector, standardize pixel value, pick color mode</li>
      </ul>
    </ul>

    <h3 class="card-title">Predictive</h3>
    <ul>
      <li>Explain difference between batch serving and online serving</li>
      <li>Explain strategies for re-training the model</li>
      <li>Explain how to use model registry and model versioning</li>
      <li>Explain how to monitor the model</li>
      <li>Explain how to orchestrate workflow using Airflow/Kubeflow</li>
      <li>Explain how to track experiments using MLflow</li>
      <li>Explain how to reduce model size and optimize for deployment on smart phones</li>
    </ul>

    <h3 class="card-title">Generative</h3>
    <ul>
      <li>Explain how to achieve scaling, load-balancing, fast response for LLM application</li>
      <li>Explain how to use caching to improve LLM performance</li>
      <li>Explain trade-off between GPU and TPU</li>
      <li>Explain how to monitor LLM in production</li>
      <li>Explain how vector database work</li>
    </ul>

    <!-- <h3 class="card-title">Non-functional</h3>
    <ul>
      <li>Explain availability</li>
      <li>Explain reliability</li>
      <li>Explain scalability</li>
      <li>Explain performance</li>
      <li>Explain consistency</li>
    </ul>

    <h3 class="card-title">Deployment</h3>
    <ul>
      <li>Explain vertical and horizontal scaling as well as scaling out and scaling in</li>
      <li>Explain how to achieve zero-downtime deployment</li>
      <li>Explain how to rollback faulty deployment</li>
      <li>Explain edge deployment</li>
    </ul>

    <h3 class="card-title">Cache</h3>
    <ul>
      <li>Explain caching</li>
      <li>Explain write-through cache and write-back cache</li>
    </ul>

    <h3 class="card-title">Load balancing</h3>
    <ul>
      <li>Explain load balancing</li>
    </ul>

    <h3 class="card-title">Distributed system</h3>
    <ul>
      <li>Explain partitioning</li>
      <li>Explain database sharding strategies like consistent hashing</li>
      <li>Explain CAP theorem</li>
      <li>Explain how to handle security concerns in distributed system</li>
      <li>Explain how to handle millions of events per second</li>
      <li>Explain how to achieve high availability</li>
      <li>Explain how to perform real-time analytics</li>
      <li>Explain how to ensure data consistency in micro-service architecture</li>
      <li>Explain how to track user activities on websites</li>
      <li>Explain how to optimize read-heavy and write-heavy systems</li>
      <li>Explain how to handle versioning in micro-service architecture</li>
      <li>Explain how to handle data replication and consistency in distributed database</li>
      <li>Explain how to handle hot keys in caching system</li>
      <li>Explain how to handle system failure during transactions</li>
      <li>Explain how to support multiple languages and regions</li>
      <li>Explain how to handle database connection pooling in a large scale application</li>
    </ul> -->
  </div>
  <div class="card-footer text-muted">
    Reference: 
  </div>
</div>
<!-- Machine learning END -->

</div> <!-- /.col-md-12 -->
</div> <!-- /.row -->
</div> <!-- /.container -->

<include src="/footer.html"></include>

</body>

</html>