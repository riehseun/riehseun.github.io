<!DOCTYPE html>

<html lang="en">

<head>

<!-- Metadata -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="">
<meta name="author" content="Seungmoon Rieh">
<meta name="keywords" content="">

<!-- Title and image -->
<title>Seungmoon Rieh</title>
<link href="/img/seungmoonrieh.jpg" rel="icon">

<!-- CSS -->
<link href="/css/bootstrap.min.css" rel="stylesheet">
<link href="/css/monokai-sublime.css" rel="stylesheet">
<link href="/css/site.css" rel="stylesheet">

<!-- JavaScript -->
<script src="/js/jquery.min.js" type="text/javascript"></script>
<script src="/js/bootstrap.bundle.min.js" type="text/javascript"></script>
<script src="/js/highlight.pack.js" type="text/javascript"></script>
<script src="/js/include_html.js" type="text/javascript"></script>
<script src="/js/mathjax/tex-chtml.js" type="text/javascript"></script>
<script src="/js/site.js" type="text/javascript"></script>

</head>

<body>

<include src="/header.html"></include>

<div class="container">
<div class="row">
<div class="col-md-12">
<h1 class="my-4">Machine Learning</h1>

<!-- Neural style transfer BEGIN -->
<div class="card mb-4" id="subject">
  <div class="card-body">
    <h2 class="card-title">Neural style transfer</h2>
    <ul class="list-unstyled mb-0">
      <li><a href="#neural-style-transfer-">Neural style transfer</a></li>
      <li><a href="#neural-style-transfer-">Cost function for generated</a></li>
      <li><a href="#neural-style-transfer-">Cost function for content</a></li>
      <li><a href="#neural-style-transfer-">Cost function for style</a></li>
      <li><a href="#neural-style-transfer-">Solve optimization problem</a></li>
    </ul>
  </div>
</div>

<div class="card mb-4" id="neural-style-transfer-">
  <div class="card-body">
    <h2 class="card-title">Neural style transfer</h2>
    <ul>
      <li>Content image</li>
      <li>Style image</li>
      <li>Generated image</li>
    </ul>

    <img class="img-fluid" class="card-img-top" src="/machine-learning/image/machine-learning-1/neural-style-transfer1.png" style="width: 800px; height: 250px" alt="Card image cap">
  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://www.deeplearning.ai/deep-learning-specialization/">Deep Learning Specialization</a>
  </div>
</div>

<div class="card mb-4" id="neural-style-transfer-">
  <div class="card-body">
    <h2 class="card-title">Cost function for generated</h2>
    <ul>
      <li>\( J(G) = \alpha J_{content}(C,G) + \beta J_{style}(S,G) \)</li>
<pre><code class="python">def total_cost(j_content, j_style, alpha = 10, beta = 40):
    return alpha * j_content + beta * j_style</code></pre>
      <li>1. Initialize \( G \) randomly</li>
      <li>2. Use gradient descent to minimize \( J(G) \) randomly</li>
      <ul>
        <li>\( G = G - \dfrac{\partial}{\partial G} J(G) \)</li>
      </ul>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://www.deeplearning.ai/deep-learning-specialization/">Deep Learning Specialization</a>
  </div>
</div>

<div class="card mb-4" id="neural-style-transfer-">
  <div class="card-body">
    <h2 class="card-title">Cost function for content</h2>
    <ul>
      <li>Say you use hidden layer \( l \) to compute content cost from a pre-trained ConvNet like VGG</li>
      <ul>
        <li>Shallower layers tend to detect lower-level features such as edges and simple textures</li>
        <li>Deeper layers tend to detect higher-level features such as more complex textures and object classes</li>
        <li>Choosing a middle layer (neither too shallow or too deep) will likely yield the most visually pleasing result</li>
      </ul>
      <li>Set image C as the input to pretrained VGG and run forward prop</li>
      <li>Set image G as the input to pretrained VGG and run forward prop</li>
      <li>Let \( a^{[l](C)}, a^{[l](G)} \) be the activation of  layer \( l \)</li>
      <ul>
        <li>If \( a^{[l](C)}, a^{[l](G)} \) are similar, both images have simialr content</li>
      </ul>
      <li>\( J_{content}(C,G) = \dfrac{1}{4 \times n_{H} \times n_{W} \times n_{C}}||a^{[l](C)} - a^{[l](G)}||^{2} \)</li>

<pre><code class="python">def compute_content_cost(a_C, a_G):
    """
    Args:
    a_C -- tensor of dimension (1, n_H, n_W, n_C)
    a_G -- tensor of dimension (1, n_H, n_W, n_C)

    Returns:
    J_content
    """

    m, n_H, n_W, n_C = a_G.get_shape().as_list()

    a_C_unrolled = tf.reshape(a_C, [m, tf.multiply(n_H, n_W), n_C])
    a_G_unrolled = tf.reshape(a_G, [m, tf.multiply(n_H, n_W), n_C])

    J_content = tf.reduce_sum(tf.square(tf.subtract(a_C_unrolled, a_G_unrolled))) / (4 * n_H * n_W * n_C)

    return J_content</code></pre>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://www.deeplearning.ai/deep-learning-specialization/">Deep Learning Specialization</a>
  </div>
</div>

<div class="card mb-4" id="neural-style-transfer-">
  <div class="card-body">
    <h3 class="card-title">Cost function for style</h3>
    <ul>
      <li>Capture the style from layer \( l \)</li>
      <li>Minimize the distance between Gram matrix of "style" image S and Gram matrix of "generated" image G</li>
      <li>\( J_{style}^{[l]}(S,G) = \dfrac{1}{4 \times n_{C}^{2} \times n_{H} \times n_{W}^{2}} \displaystyle\sum _{i=1}^{n_{C}}\displaystyle\sum_{j=1}^{n_{C}}(G_{i,j}^{[l][G]}-G_{i,j}^{[l][S]})^2 \)</li>
    </ul>

<pre><code class="python">def gram_matrix(A):
    """
    Args:
    A -- matrix of shape (n_C, n_H*n_W)

    Returns:
    GA -- Gram matrix of A, of shape (n_C, n_C)
    """

    GA = tf.matmul(A, tf.transpose(A))

    return GA</code></pre>

<pre><code class="python">def compute_layer_style_cost(a_S, a_G):
    """
    Args:
    a_S -- tensor of dimension (1, n_H, n_W, n_C)
    a_G -- tensor of dimension (1, n_H, n_W, n_C)

    Returns:
    J_style_layer -- tensor representing a scalar value
    """

    m, n_H, n_W, n_C = a_G.get_shape().as_list()

    a_S = tf.transpose(tf.reshape(a_S, [tf.multiply(n_H, n_W), n_C]))
    a_G = tf.transpose(tf.reshape(a_G, [tf.multiply(n_H, n_W), n_C]))

    GS = gram_matrix(a_S)
    GG = gram_matrix(a_G)

    J_style_layer = tf.reduce_sum(tf.square(tf.subtract(GS, GG))) / (4*n_H*n_W*n_H*n_W*n_C*n_C)

    return J_style_layer</code></pre>

    <h3 class="card-title">Merge style costs from several different layers</h3>
    <ul>
      <li>Each layer will be given weights ( \( \lambda^{[l]} \) ) that reflect how much each layer will contribute to the style</li>
      <li>\( J_{style}(S,G) = \displaystyle\sum_{l} \lambda^{[l]} J_{style}^{[l]}(S,G) \)</li>
      <li>\( \displaystyle\sum_{l}^{L}\lambda^{[l]} = 1 \)</li>
    </ul>

<pre><code class="python">STYLE_LAYERS = [
    ('conv1_1', 0.2),
    ('conv2_1', 0.2),
    ('conv3_1', 0.2),
    ('conv4_1', 0.2),
    ('conv5_1', 0.2)]</code></pre>

<pre><code class="python">def compute_style_cost(model, style_layers):
    """
    Args:
    model -- our tensorflow model
    style_layers -- list of the names of the layers and a coefficient

    Returns:
    J_style -- tensor representing a scalar value
    """

    J_style = 0

    for layer_name, coeff in style_layers:

        # Select the activation (the output tensor) of the current layer
        out = model[layer_name]

        # Get the style of the style image "S" from the current layer
        a_S = sess.run(out)

        # Get the style of the generated image "G" from the current layer
        a_G = out

        # Compute the "style cost" for the current layer
        J_style_layer = compute_layer_style_cost(a_S, a_G)

        # Add the weighted style cost to the overall style cost (J_style)
        J_style += coeff * J_style_layer

    return J_style</code></pre>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://www.deeplearning.ai/deep-learning-specialization/">Deep Learning Specialization</a>
  </div>
</div>

<div class="card mb-4" id="neural-style-transfer-">
  <div class="card-body">
    <h2 class="card-title">Solve optimization problem</h2>

    <h3 class="card-title">Load content image</h3>

<pre><code class="python">def reshape_and_normalize_image(image):

    # Reshape image to match expected input of VGG16
    image = np.reshape(image, ((1,) + image.shape))

    # Substract the mean to match the expected input of VGG16
    image = image - CONFIG.MEANS

    return image</code></pre>

<pre><code class="python">content_image = scipy.misc.imread("/path/to/content-image.png")
content_image = reshape_and_normalize_image(content_image)</code></pre>

    <h3 class="card-title">Load style image</h3>

<pre><code class="python">style_image = scipy.misc.imread("/path/to/style-image.png")
style_image = reshape_and_normalize_image(style_image)</code></pre>

    <h3 class="card-title">Randomly initialize the image to be generated</h3>
    <ul>
      <li>Generated image is slightly correlated with the content image, which helps the content of the "generated" image to more rapidly match the content of the "content" image</li>
    </ul>

<pre><code class="python">def generate_noise_image(content_image, noise_ratio = CONFIG.NOISE_RATIO):

    # Generate a random noise_image
    noise_image = np.random.uniform(-20, 20, (1, CONFIG.IMAGE_HEIGHT, CONFIG.IMAGE_WIDTH, CONFIG.COLOR_CHANNELS)).astype('float32')

    # Set the input_image to be a weighted average of the content_image and a noise_image
    input_image = noise_image * noise_ratio + content_image * (1 - noise_ratio)

    return input_image</code></pre>

<pre><code class="python">generated_image = generate_noise_image(content_image)</code></pre>

    <h3 class="card-title">Load the VGG19 model</h3>

<pre><code class="python">def load_vgg_model(path):
    """
    Returns a model for the purpose of 'painting' the picture.
    Takes only the convolution layer weights and wrap using the TensorFlow
    Conv2d, Relu and AveragePooling layer. VGG actually uses maxpool but
    the paper indicates that using AveragePooling yields better results.
    The last few fully connected layers are not used.
    Here is the detailed configuration of the VGG model:
        0 is conv1_1 (3, 3, 3, 64)
        1 is relu
        2 is conv1_2 (3, 3, 64, 64)
        3 is relu
        4 is maxpool
        5 is conv2_1 (3, 3, 64, 128)
        6 is relu
        7 is conv2_2 (3, 3, 128, 128)
        8 is relu
        9 is maxpool
        10 is conv3_1 (3, 3, 128, 256)
        11 is relu
        12 is conv3_2 (3, 3, 256, 256)
        13 is relu
        14 is conv3_3 (3, 3, 256, 256)
        15 is relu
        16 is conv3_4 (3, 3, 256, 256)
        17 is relu
        18 is maxpool
        19 is conv4_1 (3, 3, 256, 512)
        20 is relu
        21 is conv4_2 (3, 3, 512, 512)
        22 is relu
        23 is conv4_3 (3, 3, 512, 512)
        24 is relu
        25 is conv4_4 (3, 3, 512, 512)
        26 is relu
        27 is maxpool
        28 is conv5_1 (3, 3, 512, 512)
        29 is relu
        30 is conv5_2 (3, 3, 512, 512)
        31 is relu
        32 is conv5_3 (3, 3, 512, 512)
        33 is relu
        34 is conv5_4 (3, 3, 512, 512)
        35 is relu
        36 is maxpool
        37 is fullyconnected (7, 7, 512, 4096)
        38 is relu
        39 is fullyconnected (1, 1, 4096, 4096)
        40 is relu
        41 is fullyconnected (1, 1, 4096, 1000)
        42 is softmax
    """

    vgg = scipy.io.loadmat(path)

    vgg_layers = vgg['layers']

    def _weights(layer, expected_layer_name):
        """
        Return the weights and bias from the VGG model for a given layer.
        """
        wb = vgg_layers[0][layer][0][0][2]
        W = wb[0][0]
        b = wb[0][1]
        layer_name = vgg_layers[0][layer][0][0][0][0]
        assert layer_name == expected_layer_name
        return W, b

        return W, b

    def _relu(conv2d_layer):
        """
        Return the RELU function wrapped over a TensorFlow layer. Expects a
        Conv2d layer input.
        """
        return tf.nn.relu(conv2d_layer)

    def _conv2d(prev_layer, layer, layer_name):
        """
        Return the Conv2D layer using the weights, biases from the VGG
        model at 'layer'.
        """
        W, b = _weights(layer, layer_name)
        W = tf.constant(W)
        b = tf.constant(np.reshape(b, (b.size)))
        return tf.nn.conv2d(prev_layer, filter=W, strides=[1, 1, 1, 1], padding='SAME') + b

    def _conv2d_relu(prev_layer, layer, layer_name):
        """
        Return the Conv2D + RELU layer using the weights, biases from the VGG
        model at 'layer'.
        """
        return _relu(_conv2d(prev_layer, layer, layer_name))

    def _avgpool(prev_layer):
        """
        Return the AveragePooling layer.
        """
        return tf.nn.avg_pool(prev_layer, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')

    # Constructs the graph model.
    graph = {}
    graph['input']   = tf.Variable(np.zeros((1, CONFIG.IMAGE_HEIGHT, CONFIG.IMAGE_WIDTH, CONFIG.COLOR_CHANNELS)), dtype = 'float32')
    graph['conv1_1']  = _conv2d_relu(graph['input'], 0, 'conv1_1')
    graph['conv1_2']  = _conv2d_relu(graph['conv1_1'], 2, 'conv1_2')
    graph['avgpool1'] = _avgpool(graph['conv1_2'])
    graph['conv2_1']  = _conv2d_relu(graph['avgpool1'], 5, 'conv2_1')
    graph['conv2_2']  = _conv2d_relu(graph['conv2_1'], 7, 'conv2_2')
    graph['avgpool2'] = _avgpool(graph['conv2_2'])
    graph['conv3_1']  = _conv2d_relu(graph['avgpool2'], 10, 'conv3_1')
    graph['conv3_2']  = _conv2d_relu(graph['conv3_1'], 12, 'conv3_2')
    graph['conv3_3']  = _conv2d_relu(graph['conv3_2'], 14, 'conv3_3')
    graph['conv3_4']  = _conv2d_relu(graph['conv3_3'], 16, 'conv3_4')
    graph['avgpool3'] = _avgpool(graph['conv3_4'])
    graph['conv4_1']  = _conv2d_relu(graph['avgpool3'], 19, 'conv4_1')
    graph['conv4_2']  = _conv2d_relu(graph['conv4_1'], 21, 'conv4_2')
    graph['conv4_3']  = _conv2d_relu(graph['conv4_2'], 23, 'conv4_3')
    graph['conv4_4']  = _conv2d_relu(graph['conv4_3'], 25, 'conv4_4')
    graph['avgpool4'] = _avgpool(graph['conv4_4'])
    graph['conv5_1']  = _conv2d_relu(graph['avgpool4'], 28, 'conv5_1')
    graph['conv5_2']  = _conv2d_relu(graph['conv5_1'], 30, 'conv5_2')
    graph['conv5_3']  = _conv2d_relu(graph['conv5_2'], 32, 'conv5_3')
    graph['conv5_4']  = _conv2d_relu(graph['conv5_3'], 34, 'conv5_4')
    graph['avgpool5'] = _avgpool(graph['conv5_4'])

    return graph</code></pre>

<pre><code class="python">model = load_vgg_model("/path/to/vgg19.mat")</code></pre>

    <h3 class="card-title">Content cost</h3>

<pre><code class="python"># Assign the content image to be the input of the VGG model
sess.run(model['input'].assign(content_image))

# Select the output tensor of layer conv4_2
out = model['conv4_2']

# Set a_C to be the hidden layer activation from the layer we have selected
a_C = sess.run(out)

# Set a_G to be the hidden layer activation from same layer
a_G = out

# Compute the content cost
J_content = compute_content_cost(a_C, a_G)</code></pre>

    <h3 class="card-title">Style cost</h3>

<pre><code class="python"># Assign the input of the model to be the "style" image
sess.run(model['input'].assign(style_image))

# Compute the style cost
J_style = compute_style_cost(model, STYLE_LAYERS)</code></pre>

    <h3 class="card-title">Total cost</h3>

<pre><code class="python">J = total_cost(J_content, J_style, 10, 40)</code></pre>

    <h3 class="card-title">Optimizer</h3>

<pre><code class="python">optimizer = tf.train.AdamOptimizer(2.0)
train_step = optimizer.minimize(J)</code></pre>

    <h3 class="card-title">Model</h3>

<pre><code class="python">def save_image(path, image):

    # Un-normalize the image so that it looks good
    image = image + CONFIG.MEANS

    # Clip and Save the image
    image = np.clip(image[0], 0, 255).astype('uint8')
    scipy.misc.imsave(path, image)</code></pre>

<pre><code class="python">def model_nn(sess, input_image, num_iterations = 200):

    # Initialize global variables
    sess.run(tf.global_variables_initializer())

    # Run the noisy input image (initial generated image) through the model
    model["input"].assign(input_image)

    for i in range(num_iterations):

        # Run the session on the train_step to minimize the total cost
        sess.run(train_step)

        # Compute the generated image by running the session on the current model['input']
        generated_image = sess.run(model["input"])

        # Print every 20 iteration
        if i%20 == 0:
            Jt, Jc, Js = sess.run([J, J_content, J_style])
            print("Iteration " + str(i) + " :")
            print("total cost = " + str(Jt))
            print("content cost = " + str(Jc))
            print("style cost = " + str(Js))

            # Save current generated image in the "/output" directory
            save_image("output/" + str(i) + ".png", generated_image)

    # Save last generated image
    save_image('output/generated_image.jpg', generated_image)

    return generated_image

model_nn(sess, generated_image)</code></pre>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://www.deeplearning.ai/deep-learning-specialization/">Deep Learning Specialization</a>
  </div>
</div>
<!-- Neural style transfer END -->

</div> <!-- /.col-md-12 -->
</div> <!-- /.row -->
</div> <!-- /.container -->

<include src="/footer.html"></include>

</body>

</html>