<!DOCTYPE html>

<html lang="en">

<head>

<!-- Metadata -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="">
<meta name="author" content="Seungmoon Rieh">
<meta name="keywords" content="">

<!-- Title and image -->
<title>Seungmoon Rieh</title>
<link href="/img/seungmoonrieh.jpg" rel="icon">

<!-- CSS -->
<link href="/css/bootstrap.min.css" rel="stylesheet">
<link href="/css/monokai-sublime.css" rel="stylesheet">
<link href="/css/site.css" rel="stylesheet">

<!-- JavaScript -->
<script src="/js/jquery.min.js" type="text/javascript"></script>
<script src="/js/bootstrap.bundle.min.js" type="text/javascript"></script>
<script src="/js/highlight.pack.js" type="text/javascript"></script>
<script src="/js/include_html.js" type="text/javascript"></script>
<script src="/js/mathjax/tex-chtml.js" type="text/javascript"></script>
<script src="/js/site.js" type="text/javascript"></script>

</head>

<body>

<include src="/header.html"></include>

<div class="container">
<div class="row">
<div class="col-md-12">
<h1 class="my-4">Machine Learning</h1>

<!-- Transformer BEGIN -->
<div class="card mb-4" id="subject">
  <div class="card-body">
    <h2 class="card-title">Transformer</h2>
    <ul class="list-unstyled mb-0">
      <li><a href="#transformer-">Transformer</a></li>
      <li><a href="#transformer-">Input embedding</a></li>
      <li><a href="#transformer-">Positional encoding</a></li>
      <li><a href="#transformer-">Multi-head attention</a></li>
      <li><a href="#transformer-">Layer normalization</a></li>
      <li><a href="#transformer-">Feedforward network</a></li>
      <li><a href="#transformer-">Transformer evaluation</a></li>
      <li><a href="#transformer-">T5</a></li>
    </ul>
  </div>
</div>

<div class="card mb-4" id="transformer-">
  <div class="card-body">
    <h2 class="card-title">Transformer</h2>
    <ul>
      <li>Original transformer was used for machine translation</li>
      <li>Training data consisted of 4.5M English-German sentence pairs and 3.6M English-French sentence pairs</li>
      <li>Training base model took 12 hours for 100k steps on a machine with 8 NVIDIA P100 GPUs</li>
      <li>Training big model took 3.5 days for 300k steps</li>
      <li>Adam optimizer was used with varying learning rates (linear during start, then decresing going forward)</li>
    </ul>

    <img class="img-fluid" class="card-img-top" src="/machine-learning/image/machine-learning-1/transformer1.png" style="width: 500px; height: 700px" alt="Card image cap">

    <ul>
      <li>Output of layer \( l \) is becomes the input of layer \( l+1 \) until prediction is reached</li>
      <li>There is no RNN or LSTM</li>
      <li>There are 6 layers of encoder stack and 6 layers of decoder stack</li>
      <li>Multi-head attention runs \( 8 \) attentions in parallel</li>
    </ul>

    <h3 class="card-title">Encoder stack</h3>

    <img class="img-fluid" class="card-img-top" src="/machine-learning/image/machine-learning-1/transformer2.png" style="width: 300px; height: 500px" alt="Card image cap">

    <ul>
      <li>Structure</li>
      <ul>
        <li>Two sublayers</li>
        <ul>
          <li>Multi-head attention</li>
          <li>Fully connected position-wise feed forward</li>
        </ul>
        <li>Each sublayer has residual connection</li>
        <ul>
          <li>Transmit unprocessed input X of sublayer to layer normalization function</li>
          <li>Ensure key information like positional encoding is not lost on the way</li>
          <li>\( \text{LayerNormalization}(x+\text{sublayer}(x)) \)</li>
        </ul>
        <li>Output of every sublayer \( d_{\text{model}} \) remain constant</li>
        <ul>
          <li>All key operations in transformer is dot product</li>
          <li>Reduces computation</li>
        </ul>
      </ul>
    </ul>

    <h3 class="card-title">Decoder stack</h3>

    <img class="img-fluid" class="card-img-top" src="/machine-learning/image/machine-learning-1/transformer3.png" style="width: 300px; height: 700px" alt="Card image cap">

    <ul>
      <li>Masked multi-head attention</li>
      <ul>
        <li>Model must not see future words</li>
        <li>In the output, at a given position, all subsequent words are masked (values are replaced with \( -\infty \))</li>
        <li>Only lets attention apply to positions up to and including current position</li>
      </ul>
      <li>Multi-head attention</li>
      <ul>
        <li>Only attends to positions up to current position</li>
        <li>This avoid seeing the sequence it must predict</li>
        <li>Draws \( K, V \) from encoder and \( Q \) from decoder (masted multi-head attention) to compute attention</li>
      </ul>
    </ul>

    <h3 class="card-title">Use case</h3>
    <ul>
      <li>Translation</li>
      <ul>
        <li>Input is English sentence, which goes through encoder layer</li>
        <li>Output is French sentence, which goes through decoder layer</li>
      </ul>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: Transformers for Natural Language Processing, Denis Rothman
  </div>
</div>

<div class="card mb-4" id="transformer-">
  <div class="card-body">
    <h2 class="card-title">Input embedding</h2>
    <ul>
      <li>Converts input tokens to vector of \( d_{\text{model}} = 512 \) dimension using learned embeddings</li>
      <li>Tokenizer transforms a sentence into tokens (input_ids)</li>
      <li>Same word is represented by the same token (input_id)</li>
      <ul>
        <li>Ex. text = "The cat slept on the couch.It was too tired to get up."</li>
        <li>Ex. input_ids = [1996, 4937, 7771, 2006, 1996, 6411, 1012, 2009, 2001, 2205, 5458, 2000, 2131, 2039, 1012]</li>
      </ul>
      <li>An embedding method (Ex. skip gram) is used to convert each word to a vector of \( 512 \) numbers</li>
      <ul>
        <li>Ex. black = [[-0.01206071 ... -0.04273562]]</li>
        <li>Ex. brown = [[0.00135794589 ... -0.0490022525]]</li>
      </ul>
    </ul>
  <div class="card-footer text-muted">
    Reference: Transformers for Natural Language Processing, Denis Rothman
  </div>
</div>

<div class="card mb-4" id="transformer-">
  <div class="card-body">
    <h2 class="card-title">Positional encoding</h2>
    <ul>
      <li>Considers position of a word in a sentence</li>
      <li>For each word, create a vector of \( d_{\text{model}} = 512 \) dimension</li>
      <ul>
        <li>\( PE_{pos 2i} = \sin\left(\dfrac{pos}{10000^{\frac{2i}{d_{\text{model}}}}}\right) \)</li>
        <ul>
          <li>Applied to even position in the vector of \( d_{\text{model}} = 512 \)</li>
        </ul>
        <li>\( PE_{pos 2i+1} = \cos\left(\dfrac{pos}{10000^{\frac{2i}{d_{\text{model}}}}}\right) \)</li>
        <ul>
          <li>Applied to odd position in the vector of \( d_{\text{model}} = 512 \)</li>
        </ul>
      </ul>
      <li>Original authors added positional encoding to input embedding to produce final encoding</li>
      <li>To minimize loss of information of word embedding, we can multiply word embedding by \( \sqrt{d_{\text{model}}} \)</li>
      <li>Positional encoding is computed once and re-used throughout the training</li>
<pre><code class="python">def encoding(pos, positional_vector)
  for i in range(0, 512,2):
      val =  pos / (10000**((2*i)/d_model))
      positional_vector[0][i] = math.sin(val)
      positional_vector[0][i+1] = math.cos(val)
      positional_encoding[0][i] = (embedding_vector[0][i]*math.sqrt(d_model)) + positional_vector[0][i]
      positional_encoding[0][i+1] = (embedding_vector[0][i+1]*math.sqrt(d_model)) + positional_vector[0][i+1]</code></pre>
    </ul>
  <div class="card-footer text-muted">
    Reference: Transformers for Natural Language Processing, Denis Rothman
  </div>
</div>

<div class="card mb-4" id="transformer-">
  <div class="card-body">
    <h2 class="card-title">Attention</h2>
    <ul>
      <li>Unlike RNN where each token goes to network one by one, attention <strong>relates every token to every other tokens</strong></li>
      <li>Input sentence is represented as \( Q, K, V \) where these matrices are multipled to find relationship between all words to all words</li>
      <ul>
        <li>Value of diagonal element should be the highest since a word would stongly be related to itself</li>
        <li>Dimension of \( Q, K, V \) is \( n \times  d_{\text{model}} \)</li>
        <li>A word is represented by \( d_{\text{model}} \) vector</li>
        <li>Number of words in vocabulary is represented by \( n \)</li>
      </ul>
      <li>\( \text{Attention}(Q,K,V) = \text{softmax} \left( \dfrac{QK^{T}}{\sqrt{d_{k}}} \right) V \)</li>
      <ul>
        <li>For large values of \( d_{k} \), dot product becomes big and softmax function can have very small gradient, so scale by \( \frac{1}{\sqrt{d_{k}}} \)</li>
      </ul>
    </ul>

    <h3 class="card-title">Multi-head attention</h3>
    <ul>
      <li>Input \( (\text{max_seq_len} \times d_{\text{model}}) \)</li>
      <li>\( Q \) \( (\text{max_seq_len} \times d_{\text{model}}) \)</li>
      <li>\( K \) \( (\text{max_seq_len} \times d_{\text{model}}) \)</li>
      <li>\( V \) \( (\text{max_seq_len} \times d_{\text{model}}) \)</li>
      <li>\( W_{O} \) \( (d_{\text{model}} \times d_{\text{model}}) \)</li>
      <li>\( W_{Q} \) \( (d_{\text{model}} \times d_{\text{model}}) \)</li>
      <li>\( W_{K} \) \( (d_{\text{model}} \times d_{\text{model}}) \)</li>
      <li>\( W_{V} \) \( (d_{\text{model}} \times d_{\text{model}}) \)</li>
      <li>\( Q^{\prime} \) - \( (Q \times W_{Q}) \), matrix where each query that has dimension \( (\text{max_seq_len} \times d_{\text{model}}) \)</li>
      <li>\( K^{\prime} \) - \( (K \times W_{K}) \), matrix where each key that has dimension \( (\text{max_seq_len} \times d_{\text{model}}) \)</li>
      <li>\( V^{\prime} \) - \( (V \times W_{V}) \), matrix where each value that has dimension \( (\text{max_seq_len} \times d_{\text{model}}) \)</li>
      <li>\( Q^{\prime} \) is split into \( Q_{1} \dots Q_{8} \) \( (d_{k} \times \text{max_seq_len}) \) where each head only sees smaller part of embedding of words</li>
      <li>\( K^{\prime} \) is split into \( K_{1} \dots K_{8} \)</li>
      <li>\( V^{\prime} \) is split into \( V_{1} \dots V_{8} \)</li>
      <li>\( \text{head}_{i} \) - \( (d_{k} \times \text{max_seq_len}) \) is computed from \( Q_{i}, K_{i}, V_{i} \)</li>
      <li>\( \text{Multi-head attention}(Q,K,V) = \text{Concat}(\text{head}_{1} \dots \text{head}_{h}) W_{O} \) \( (\text{max_seq_len} \times d_{\text{model}}) \)</li>
      <li>Each head watches different aspect of word</li>
      <ul>
        <li>A head may learn a word as noun</li>
        <li>Another head may learn the same word as verb</li>
        <li>Another head may learn the same word as adjective</li>
      </ul>
    </ul>

    <h3 class="card-title">Multi-head attention</h3>
    <ul>
      <li>Each word is represented by vector of \( 512 \) dimension</li>
      <li>Ex. The cat sat on the rug and it was dry-cleaned</li>
      <li>Model is trained to find if "it" is related to "cat" or "rug"</li>
      <li>There are \( 8 \) heads whose dimension is \( 64 \)</li>
      <li>In each head, weight matrices whose dimension is \( 512 \times 64 \) must be trained</li>
      <ul>
        <li>\( W_{Q} \) - train the queries</li>
        <li>\( W_{K} \) - train the keys</li>
        <li>\( W_{V} \) - train the values</li>
      </ul>
      <li>Example</li>
      <ul>
        <li>Represent the input</li>
        <ul>
          <li>Assume \( 3 \) input tokens (or words)</li>
          <li>Assume \( d_{\text{model}} = 4 \)</li>
<pre><code class="python">X = np.array([[1.0, 0.0, 1.0, 0.0],   # Input token 1
            [0.0, 2.0, 0.0, 2.0],   # Input token 2
            [1.0, 1.0, 1.0, 1.0]])  # Input token 3</code></pre>
        </ul>
        <li>Initialize the weight matrices</li>
        <ul>
          <li>Assume \( d_{k} = 3 \)</li>
<pre><code class="python">W_query = np.array([[1, 0, 1],
                  [1, 0, 0],
                  [0, 0, 1],
                  [0, 1, 1]])
W_key = np.array([[0, 0, 1],
                [1, 1, 0],
                [0, 1, 0],
                [1, 1, 0]])
W_value = np.array([[0, 2, 0],
                  [0, 3, 0],
                  [1, 0, 3],
                  [1, 1, 0]])</code></pre>
        </ul>
        <li>Compute \( Q, K, V \)</li>
        <ul>
          <li>Assume there is one set of \( W_{Q}, W_{K}, W_{V} \) for all inputs</li>
<pre><code class="python">Q = np.matmul(X, W_query)
K = np.matmul(X, W_key)
V = np.matmul(X, W_value)</code></pre>
        </ul>
        <li>Compute attention score</li>
        <ul>
          <li>\( \text{Attention}(Q,K,V) = \text{softmax} \left( \dfrac{QK^{T}}{\sqrt{d_{k}}} \right) V \)</li>
<pre><code class="python">d_k = 1   # Square root of k_d simplified to 1 for this example
attention_scores = (Q@K.transpose()) / d_k</code></pre>
          <li>Softmax score for each input token</li>
<pre><code class="python">attention_scores[0] = softmax(attention_scores[0])
attention_scores[1] = softmax(attention_scores[1])
attention_scores[2] = softmax(attention_scores[2])</code></pre>
          <li>Attention for each input token</li>
<pre><code class="python">attention1 = attention_scores[0].reshape(-1,1)
attention1 = attention_scores[0][0] * V[0]
attention2 = attention_scores[0][1] * V[1]
attention3 = attention_scores[0][2] * V[2]</code></pre>
          <li>Sum up the result</li>
<pre><code class="python">attention_input1 = attention1 + attention2 + attention3</code></pre>
        </ul>
      </ul>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: Attention Is All You Need, Ashish Vaswani et al | Transformers for Natural Language Processing, Denis Rothman
  </div>
</div>

<div class="card mb-4" id="transformer-">
  <div class="card-body">
    <h2 class="card-title">Post-layer normalization</h2>
    <ul>
      <li>Add</li>
      <ul>
        <li>Processes residual connections, which transports unprocessed input \( X \) to normalization</li>
        <li>This prevents key information, such as positional encoding, from being lost on the way</li>
      </ul>
      <li>Layer normalization</li>
      <ul>
        <li>For each sentence, compute mean and variance of all words in the sentence</li>
        <li>Then, standardized each value using the mean and variance</li>
        <li>\( \gamma \) and \( \beta \) is multiplied and added to allow model to amplify the values</li>
        <li>\( \text{LayerNormalization}(v) = \gamma \dfrac{v-\mu}{\sqrt{\sigma^{2}+\epsilon}} + \beta \)</li>
        <li>\( \epsilon \) is added so that the values don't become too big or too small</li>
        <ul>
          <li>\( \mu \) is the mean of \( v \) of dimension \( d \)</li>
          <ul>
            <li>\( \mu = \dfrac{1}{d} \displaystyle\sum_{k=1}^{d} v_{k} \)</li>
          </ul>
          <li>\( \sigma \) is the standard deviation of \( v \) dimension \( d \)</li>
          <ul>
            <li>\( \sigma^{2} = \dfrac{1}{d} \displaystyle\sum_{k=1}^{d} (v_{k}-\mu) \)</li>
          </ul>
          <li>\( \gamma \) is a scaling vector</li>
          <li>\( \beta \) is a bias vector</li>
        </ul>
      </ul>
    </ul>
  <div class="card-footer text-muted">
    Reference: Transformers for Natural Language Processing, Denis Rothman
  </div>
</div>

<div class="card mb-4" id="transformer-">
  <div class="card-body">
    <h2 class="card-title">Feedforward network</h2>
    <ul>
      <li>Contains two layers and applies ReLU</li>
      <li>Input and output has \( d_{\text{model}} = 512 \) but inner layer has \( d_{\text{ff}} = 2048 \)</li>
      <li>\( \text{FFN}(x) = max(0, xW_{1} + b_{1})W_{2} + b_{2} \)</li>
      <li>Can be considered as two convolutions with size \( 1 \) kernels</li>
      <li>Output goes to layer normalization, then to next layer of encoder and multi-head attention layer of decoder</li>
    </ul>
  <div class="card-footer text-muted">
    Reference: Transformers for Natural Language Processing, Denis Rothman
  </div>
</div>

<div class="card mb-4" id="transformer-">
  <div class="card-body">
    <h2 class="card-title">Transformer evaluation</h2>

    <h3 class="card-title">Human</h3>
    <ul>
      <li>Input - perception of raw events for layer 0</li>
      <li>Output - language</li>
      <li>Transduction - trial-and-error</li>
      <ul>
        <li>Take structure we perceive and represent them with patterns</li>
      </ul>
      <li>We learn that many events are related (Ex. sunrise-light, sunset-dark, etc)</li>
      <li>New borns are find-tuned for many tasks by previous generations (Ex. we are taught that fire burns us)</li>
    </ul>

    <h3 class="card-title">Transformer</h3>
    <ul>
      <li>Perform transductions connecting all the tokens (subwords) that occur together in language sequence</li>
      <li>Build inductions from these transductions</li>
      <li>Train those inductions based on tokens to produce patterns of tokens</li>
    </ul>

    <h3 class="card-title">Attention</h3>
    <ul>
      <li>Attention sublayers can process millions of example for their inductive thinking operations</li>
      <li>Attention sublayers find patterns in sequences through transduction and induction</li>
      <li>Attention sublayers memorize these patterns using parameters that are stored with their model</li>
    </ul>

    <h3 class="card-title">SuperGLUE (GLUE - General language understanding evaluation)</h3>
    <ul>
      <li>Choice of plausible answers (COPA)</li>
      <ul>
        <li>Transformer chooses most plausible answer to a question</li>
      </ul>
      <li>BoolQ</li>
      <ul>
        <li>Yes-or-no answer task</li>
      </ul>
      <li>Commitment bank (CB)</li>
      <ul>
        <li>Transformer reads a premise, then examine a hypothesis</li>
        <li>Hypothesis confirms the premise or contradict it</li>
        <li>Transformer must label the hypothesis as neutral, entailment, or contradiction of the premise</li>
      </ul>
      <li>Multi-sentence reading comprehension (MultiRC)</li>
      <ul>
        <li>Transformer reads a text and choose from several possible choices</li>
      </ul>
      <li>Reading comprehension with commonsense reasoning dataset (ReCoRD)</li>
      <ul>
        <li>Dataset contains over 120,000 queries for more than 70,000 news articles</li>
        <li>Transformer uses common-sense to solve the problems</li>
      </ul>
      <li>Recognizing textual entailment (RTE)</li>
      <ul>
        <li>Transformer must read the premise, examine a hypothesis, and predict the label of entailment hypothesis status</li>
      </ul>
      <li>Words in context (WiC)</li>
      <ul>
        <li>Transformer analyzes two sentences and determine whether the target word has the same meaning in both sentences</li>
      </ul>
      <li>The Winograd schemae challence (WSC)</li>
      <ul>
        <li>Each sentence contains an occupation, a participant, and a pronoun</li>
        <li>Transformer determines whether the pronoun is coreferent with the occupation or the participant</li>
      </ul>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: Transformers for Natural Language Processing, Denis Rothman
  </div>
</div>

<div class="card mb-4" id="transformer-">
  <div class="card-body">
    <h2 class="card-title">T5</h2>
    <ul>
      <li>Stands for text-to-text transfer transformer</li>
      <li>One input format for every task submitted to the transformer</li>
      <li>Same model, hyperparameter, optimizer for a wide range of tasks</li>
      <ul>
        <li>Add a prefix to input sequence</li>
        <li>Ex. translate English to German + [sequence] for machine translation problem</li>
        <li>Ex. cola sentence + [sequence] for corpus of linguistic acceptability</li>
        <li>Ex. stsb sentence + [sequence] for semantic textual similarity benchmarks</li>
        <li>Ex. summarize + [sequence] for text summarization problem</li>
      </ul>
      <li>Same architecture as original transformer except</li>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: Transformers for Natural Language Processing, Denis Rothman
  </div>
</div>

<div class="card mb-4" id="transformer-">
  <div class="card-body">
    <h2 class="card-title">Transformer implementation</h2>
<pre><code class="python">import torch
import torch.nn as nn

class InputEmbedding(nn.Module):

    def __init__(self, vocab_size:int, d_model:int):
        self.vocal_size = vocab_size
        self.d_model = d_model
        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=d_model)

    def forward(self, x):
        return self.embedding(x) * math.sqrt(self.d_model)

class PositionalEncoding(nn.Module):

    def __init__(self, seq_len:int, d_model:int, dropout: float):
        self.seq_len = seq_len
        self.d_model = d_model
        self.dropout = nn.Dropout(p=dropout)

        # Matrix of shape (seq_len, d_model)
        pe = torch.zeros(seq_len, d_model)
        # Vector of shape (seq_len, 1) This represents position of word in sentence
        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float()*-(math.log(10000.0)/d_model))

        # Apply sin to even indices
        pos_enc[:, 0::2] = torch.sin(position*div_term)
        # Apply cos to odd indices
        pos_enc[:, 1::2] = torch.cos(position*div_term)

        # (1, max_seq_len, d_model) We will have batch of sentences
        pos_enc = pos_enc.unsqueeze(0)

        # Save tensor as file since positional encoding is not learnable parameter
        self.register_buffer('pos_enc', pos_enc)

    def forward(self, x):
        # Add positional encoding to every word in sentence
        # No need to store grad in tensor
        x = x + self.pe[:, :x.shape(1), :].requires_grad(False)
        return self.dropout(x)

class LayerNormalization(nn.Module):

    def __init__(self, epsilon: float = 10**-6):
        self.epsilon = epsilon
        self.gamma = nn.Parameter(torch.ones(1))
        self.beta = nn.Parameter(torch.zeros(1))

    def forward(self, x):
        mean = x.mean(dim=-1, keepdim=True)
        std = x.std(dim=-1, keepdim=True)
        return self.gamma * (x-mean) / (std+epsilon) + self.beta

class FeedForwardNetwork(nn.Module):

    def __init__(self, d_model: int, d_ff: float, dropout: float):
        self.linear_1 = nn.Linear(d_model, d_ff)  # W1 and B1
        self.dropout = nn.Dropout(dropout)
        self.linear_2 = nn.Linear(d_ff, d_model)  # W2 and B2

    def forward(self, x):
        # (batch, max_seq_len, d_model) -> (batch, max_seq_len, d_ff) -> (batch, max_seq_len, d_model)
        return self.linear2(self.dropout(torch.relu(self.linear_1(x))))

class MultiHeadAttention(nn.Module):

    def __init__(self, d_model: int, num_heads: int, dropout: float):
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads

        self.w_q = nn.Linear(d_model, d_model)
        self.w_k = nn.Linear(d_model, d_model)
        self.w_v = nn.Linear(d_model, d_model)
        self.w_o = nn.Linear(d_model, d_model)

        self.dropout = nn.Dropout(dropout)

    @staticmethod
    def attention(self, q, k, v, mask):
        d_k = query.shape[-1]

        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)
        if mask:
            attention_scores.masked_fill_(mask==0, -1e9)  # -1e9 represents negative infinity
        attention_scores = attention_scores.softmax(dim-1)  # (batch, num_heads, seq_len, seq_len)

        if dropout:
            attention_scores = self.dropout(atten_scores)

        return (attention_scores @ value), attention_scores

    def forward(self, q, k, v, mask):
        query = self.w_q(q)  # (batch, seq_len, d_model) -> (batch, seq_len, d_model)
        key = self.w_q(k)  # (batch, seq_len, d_model) -> (batch, seq_len, d_model)
        value = self.w_q(v)  # (batch, seq_len, d_model) -> (batch, seq_len, d_model)

        # (batch, seq_len, d_model) -> (batch, seq_len, num_heads, d_k) -> (batch, num_heads, seq_len, d_k)
        query = query.view(query.shape[0], query.shape[1], self.num_heads, self.d_k).transpose(1, 2)
        key = key.view(key.shape[0], key.shape[1], self.num_heads, self.d_k).transpose(1, 2)
        value = value.view(value.shape[0], value.shape[1], self.num_heads, self.d_k).transpose(1, 2)

        x, self.attention_scores = MultiHeadAttention.attention(query, key, value, mask, self.dropout)

class ResidualConnection(nn.Module):

    def __init__(self, dropout: float):
        self.dropout = nn.Dropout(dropout)
        self.norm = LayerNormalization()

    def forward(self, x, sublayer):
        return x + self.dropout(sublayer(self.norm))</code></pre>

<pre><code class="python">class EncoderBlock(nn.Module):

    def __init__(self, multihead_attention: MultiHeadAttention, feed_forward: FeedForwardNetwork, dropout: float):
        self.multihead_attention = multihead_attention
        self.feed_forward = feed_forward
        self.residual_connection = nn.ModuleList([ResidualConnection(dropout) for _ in range(2)])

    # src_mask is needed in encoder because we don't want padded words to interact with other words
    def forward(self, x, src_mask):
        x = self.residual_connection[0](x, lambda x: self.multihead_attention(x, x, x, src_mask))
        x = self.residual_connection[1](x, self.feed_forward)
        return x

class Encoder(nn.Module):

    def __init__(self, layers: nn.ModuleList):
        self.layers = layers
        self.norm = LayerNormalization()

    def forward(self, x):
        for layer in self.layers:
            x = layer(x, mask)
        return self.norm(x)

class DecoderBlock(nn.Module):

    def __init__(self, multihead_attention: MultiHeadAttention, cross_attention: MultiHeadAttention, feed_forward: FeedForwardNetwork, dropout: float):
        self.multihead_attention = multihead_attention
        self.cross_attention = cross_attention
        self.feed_forward = feed_forward
        self.residual_connection = nn.ModuleList([ResidualConnection(dropout\) for _ in range(3)])

    # src_mask is needed in encoder because we don't want padded words to interact with other words
    def forward(self, x, encoder_output, src_mask, tgt_mast):
        x = self.residual_connection[0](x, lambda x: self.multihead_attention(x, x, x, tgt_mask))
        x = self.residual_connection[0](x, lambda x: self.cross_attention(x, encoder_output, encoder_output, src_mask))
        x = self.residual_connection[1](x, self.feed_forward)
        return x

class Decoder(nn.Module):

    def __init__(self, layers: nn.ModuleList):
        self.layers = layers
        self.norm = LayerNormalization()

    def forward(self, encoder_output, src_mask, tgt_mask):
        for layer in self.layers:
            x = layer(x, encoder_output, src_mask, tgt_mask)
        return self.norm(x)

class Projection(nn.Module):

    def __init__(self, vocab_size:int, d_model:int):
        self.vocal_size = vocab_size
        self.d_model = d_model
        self.projection = nn.Linear(d_model, vocab_size)

    def forward(self, x):
        # (batch, max_seq_len, d_model) -> (batch, max_seq_len, vocab_size)
        return torch.log_softmax(self.projection(x), dim=-1)</code></pre>

<pre><code class="python">class Transformer(nn.Module):

    def __init__(self, encoder: Encoder, deconder: Decoder, src_embedding: InputEmbedding, tgt_embedding:InputEmbedding, src_position: PositionalEncoding, tgt_position: PositionalEncoding, projection: Projection):
        self.encoder = encoder
        self.decoder = decoder
        self.src_embedding = src_embedding
        self.tgt_embedding = tgt_embedding
        self.src_position = src_position
        self.tgt_position = tgt_position
        self.projection = projection

    def encode(self, src, src_mask):
        src = self.src_embedding(src)
        src = self.src_position(src)
        return self.encoder(src, src_mask)

    def decode(self, encoder_output, src_mask, tgt, tgt_mask):
        tgt = self.tgt_embedding(tgt)
        tgt = self.src_position(tgt)
        return self.encoder(tgt, encoder_output, src_mask, tgt_mask)

    def project(self, x):
        return self.projection(x)

def build_transformer(src_vocab_size: int, tgt_vocab_size: int, src_seq_len: int, tgt_seq_len: int, d_model=512, num_layers=6, num_heads=8, dropout=0.1, d_ff=2048):

    # Create embedding layers
    src_embedding = InputEmbedding(src_vocab_size, d_model)
    tgt_embedding = InputEmbedding(tgt_vocab_size, d_model)

    # Positional encoding layers
    src_position = PositionalEncoding(d_model, src_seq_len, dropout)
    tgt_position = PositionalEncoding(d_model, tgt_seq_len, dropout)

    # Encoder
    encoders = []
    for _ in range(n):
        encoders.append(EncoderBlock(MultiHeadAttention(d_model, num_heads, dropout), FeedForwardNetwork(d_model, d_ff, dropout), dropout))
    encoder = Encoder(nn.ModuleList(encoders))

    # Decoder
    decoders = []
    for _ in range(n):
        decoders.append(DecoderBlock(MultiHeadAttention(d_model, num_heads, dropout), CrossAttention(d_model, h, dropout), FeedForwardNetwork(d_model, d_ff, dropout), dropout))
    decoder = Decoder(nn.ModuleList(decoders))

    transformer(encoder, decoder, src_embedding, tgt_embedding, src_positional, tgt_position, Projection(d_model, tgt_vocab_size))

    # Initialize parameter
    for p in transformer.parameters():
        if p.dim() > 1:
            nn.init.xavier_uniform_(p)

    return transformer</code></pre>

<pre><code class="python">import torch
import torch.nn as nn

from torch.utils.data import Dataset, DataLoader, random_split
from datasets import load_dataset
from tokenizers import Tokenizer
from tokenizers.model import WordLevel
from tokenizers.trainers import WordLevelTrainer
from tokenizers.pre_tokenizers import Whitespace
from pathlib import Path

def get_all_sentences(dataset, language):
    for item in dataset:
        yield item["translation"][language]

def get_or_build_tokenizer(config, dataset, language):
    tokenizer_path = Path(config["tokenizer_file"].format(language))
    if not Path.exist(tokenizer_path):
        tokenizer = Tokenizer(WordLevel(unk_token="[UNK]"))
        tokenizers.pre_tokenizers = WhiteSpace()
        trainer = WordLevelTrainer(special_tokens=["[UNK]", "[PAD]", "[SOS]", "[EOD]"], min_frequency=2)
        tokenizer.train_from_iterator(get_all_sentences(dataset, language), trainer=trainer)
        tokenizer.save(str(tokenizer_path))
    else:
        tokenizer = Tokenizer.from_file(str(tokenizer_path))
    return tokenizer

def get_dataset(config):
    dataset_raw = load_dataset("opus_books", f"{config["lang_src"]}-{config["lang_tgt"]}", split="train")

    # Build tokenizer
    tokenizer_src = get_or_build_tokenizer(config, dataset_raw, config["lang_src"])
    tokenizer_tgt = get_or_build_tokenizer(config, dataset_raw, config["lang_tgt"])

    # 90-10 split
    train_dataset_size = int(0.9*len(dataset_raw))
    validation_dataset_size = int(0.1*len(dataset_raw))
    train_dataset_raw, validation_dataset_raw = random_split(dataset_raw, [train_dataset_size, validation_dataset_size])

    train_dataset = BilingualDataset(train_dataset_raw, tokenizer_src, tokenizer_tgt, config["lang_src"], config["lang_tgt"], config["seq_len"])
    validation_dataset = BilingualDataset(validation_dataset_raw, tokenizer_src, tokenizer_tgt, config["lang_src"], config["lang_tgt"], config["seq_len"])

    max_len_src, max_len_tgt = 0, 0

    for item in dataset_raw:
        src_ids = tokenizer_src.encode(item["translation"][config["lang_src"]]).ids 
        tgt_ids = tokenizer_src.encode(item["translation"][config["lang_tgt"]]).ids 
        max_len_src = max(max_len_src, len(src_ids))
        max_len_tgt = max(max_len_tgt, len(tgt_ids))

    train_dataloader = DataLoader(trains_dataset, batch_size=config["batch_size"], shuffle=True)
    validation_dataloader = DataLoader(validation_dataset, batch_size=1, shuffle=True)

    return train_dataloader, validation_dataloader, tokenizer_src, tokenizer_tgt

def get_model(config, src_vocal_len, tgt_vocal_len):

    model = build_transformer(src_vocal_len, tgt_vocal_len, config["seq_len"], config["seq_len"], config["d_model"])
    return model</code></pre>

<pre><code class="python">import torch
import torch.nn as nn

from torch.utils.data import Dataset

class BilingualDataset(Dataset):

    def __init__(self, dataset, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len):

        self.dataset = dataset
        self.tokenizer_src = tokenizer_src
        self.tokenizer_tgt = tokenizer_tgt
        self.src_lang = src_lang
        self.tgt_lang = tgt_lang
        self.seq_len = seq_len

        self.sos_token = torch.Tensor(tokenizer_src.token_to_id(["SOS"], dtype=torch.int64))
        self.eos_token = torch.Tensor(tokenizer_src.token_to_id(["EOS"], dtype=torch.int64))
        self.pad_token = torch.Tensor(tokenizer_src.token_to_id(["PAD"], dtype=torch.int64))

    def __len__(self):

        return len(self.dataset)

    def __get_item__(self, index: any):
        src_tgt_pair = self.dataset[index]
        src_text = src_tgt_pair["translation"][src_lang]
        tgt_text = src_tgt_pair["translation"][tgt_lang]

        enc_input_tokens = self.tokenizer_src.encode(src_text).ids
        dec_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids

        enc_num_padding_tokens = self.seq_len - len(enc_input_tokens) - 2
        dec_num_padding_tokens = self.seq_len - len(dec_input_tokens) - 1

        if enc_num_padding_tokens < 0 or dec_num_padding_tokens < 0:
            raise ValueError("sentence is too long")

        # Add SOS and EOS to source text
        encoder = torch.cat(
            [
                self.sos_token,
                torch.tensor(enc_input_tokens, dtype=torch.int64),
                self.eos_token,
                torch.tensor([self.pad_token] * enc_num_padding_tokens, dtype=torch.int64)
            ]
        )

        # Add SOS to target text
        decoder = torch.cat(
            [
                self.sos_token,
                torch.tensor(dec_input_tokens, dtype=torch.int64),
                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64)
            ]
        )

        # Add EOS to label (ground truth - what we expect from decoder)
        label = torch.cat(
            [
                torch.tensor(dec_input_tokens, dtype=torch.int64),
                self.sos_token,
                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64)
            ]
        )

        return {
            "encoder_input": encoder_input,  # (seq_len)
            "decoder_input": decoder_input,  # (seq_len)
            "encoder_mask": (encoder_input != self.pad_token).unsqeeze(0).unsqeeze(0).int(),  # (1, 1, seq_len)
            "decoder_mask": (decoder_input != self.pad_token).unsqeeze(0).unsqeeze(0).int() & casual_mask(decoder_input.size(0)),  # (1, seq_len) (1, seq_len, seq_len)
            "label": label,  # (seq_len)
            "src_text": src_text,
            "tgt_text": tgt_text
        }

    def casual_mask(size):

        mask = torch.triu(torch.ones(1, size, size), diagonal=1).type(torch.int)
        return mask == 0</code></pre>
  </div>
  <div class="card-footer text-muted">
    Reference: Coding a Transformer from scratch on PyTorch, with full explanation, training and inference, Umar Jamil
  </div>
</div>
<!-- Transformer END -->

</div> <!-- /.col-md-12 -->
</div> <!-- /.row -->
</div> <!-- /.container -->

<include src="/footer.html"></include>

</body>

</html>