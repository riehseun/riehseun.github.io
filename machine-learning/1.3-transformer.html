<!DOCTYPE html>

<html lang="en">

<head>

<!-- Metadata -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="">
<meta name="author" content="Seungmoon Rieh">
<meta name="keywords" content="">

<!-- Title and image -->
<title>Seungmoon Rieh</title>
<link href="/img/seungmoonrieh.jpg" rel="icon">

<!-- CSS -->
<link href="/css/bootstrap.min.css" rel="stylesheet">
<link href="/css/monokai-sublime.css" rel="stylesheet">
<link href="/css/site.css" rel="stylesheet">

<!-- JavaScript -->
<script src="/js/jquery.min.js" type="text/javascript"></script>
<script src="/js/bootstrap.bundle.min.js" type="text/javascript"></script>
<script src="/js/highlight.pack.js" type="text/javascript"></script>
<script src="/js/include_html.js" type="text/javascript"></script>
<script src="/js/mathjax/tex-chtml.js" type="text/javascript"></script>
<script src="/js/site.js" type="text/javascript"></script>

</head>

<body>

<include src="/header.html"></include>

<div class="container">
<div class="row">
<div class="col-md-12">
<h1 class="my-4">Machine Learning</h1>

<!-- Transformer BEGIN -->
<div class="card mb-4" id="subject">
  <div class="card-body">
    <h2 class="card-title">Transformer</h2>
    <ul class="list-unstyled mb-0">
      <li><a href="#transformer-">Attention</a></li>
      <li><a href="#transformer-">Transformer</a></li>
      <li><a href="#transformer-">Transformer evaluation</a></li>
      <li><a href="#transformer-">T5</a></li>
    </ul>
  </div>
</div>

<div class="card mb-4" id="transformer-">
  <div class="card-body">
    <h2 class="card-title">Attention</h2>
    <ul>
      <li>Unlike RNN where tokens are analyzed in sequence, attention <strong>relates every token to every other tokens</strong> in a sequence</li>
      <li>Input sentence is represented as \( Q, K, V \) where these matrices are multipled to find relationship between all words to all words</li>
      <ul>
        <li>Value of diagonal element should be the highest since a word would stongly be related to itself</li>
        <li>\( Q, K, V \) - dimension is \( n \times  d_{model} \), a word is represented by \( d_{model} \) vector, number of words in vocabulary is represented by \( n \)</li>
      </ul>
      <li>\( \text{Attention}(Q,K,V) = \text{softmax} \left( \dfrac{QK^{T}}{\sqrt{d_{k}}} \right) V \)</li>
      <li>For large values of \( d_{k} \), dot product becomes big and softmax function can have very small gradient, so scale by \( \frac{1}{\sqrt{d_{k}}} \)</li>
    </ul>

    <h3 class="card-title">Multi-head attention</h3>
    <ul>
      <li>\( W_{Q} \) - parameter matrix whose dimension is \( d_{model} \times d_{model} \)</li>
      <li>\( W_{K} \) - parameter matrix whose dimension is \( d_{model} \times d_{model} \)</li>
      <li>\( W_{V} \) - parameter matrix whose dimension is \( d_{model} \times d_{model} \)</li>
      <li>\( Q^{\prime} \) - \( Q \times W_{Q} \), matrix where each query that has dimension \( n \times d_{model} \)</li>
      <li>\( K^{\prime} \) - \( K \times W_{K} \), matrix where each key that has dimension \( n \times d_{model} \)</li>
      <li>\( V^{\prime} \) - \( V \times W_{V} \), matrix where each value that has dimension \( n \times d_{model} \)</li>
      <li>\( Q^{\prime} \) is split into \( Q_{1} \dots Q_{8} \) where each head only sees smaller part of embedding of words</li>
      <li>\( K^{\prime} \) is split into \( K_{1} \dots K_{8} \)</li>
      <li>\( V^{\prime} \) is split into \( V_{1} \dots V_{8} \)</li>
      <li>\( \text{head}_{1} \) is computed from \( Q_{1}, K_{1}, V_{1} \)</li>
      <li>\( \text{Multi-head attention}(Q,K,V) = \text{Concat}(\text{head}_{1} \dots \text{head}_{h}) W_{O} \)</li>
      <li>Each head watches different aspect of word</li>
      <ul>
        <li>A head may learn a word as noun</li>
        <li>Another head may learn the same word as verb</li>
        <li>Another head may learn the same word as adjective</li>
      </ul>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: Attention Is All You Need, Ashish Vaswani et al | Transformers for Natural Language Processing, Denis Rothman
  </div>
</div>

<div class="card mb-4" id="transformer-">
  <div class="card-body">
    <h2 class="card-title">Transformer</h2>
    <ul>
      <li>Original transformer was used for machine translation</li>
      <li>Training data consisted of 4.5M English-German sentence pairs and 3.6M English-French sentence pairs</li>
      <li>Training base model took 12 hours for 100k steps on a machine with 8 NVIDIA P100 GPUs</li>
      <li>Training big model took 3.5 days for 300k steps</li>
      <li>Adam optimizer was used with varying learning rates (linear during start, then decresing going forward)</li>
    </ul>

    <img class="img-fluid" class="card-img-top" src="/machine-learning/image/machine-learning-1/transformer1.png" style="width: 500px; height: 700px" alt="Card image cap">

    <ul>
      <li>Output of layer \( l \) is becomes the input of layer \( l+1 \) until prediction is reached</li>
      <li>There is no RNN or LSTM</li>
      <li>There are 6 layers of encoder stack and 6 layers of decoder stack</li>
      <li>Multi-head attention runs \( 8 \) attentions in parallel</li>
    </ul>

    <h3 class="card-title">Encoder stack</h3>

    <img class="img-fluid" class="card-img-top" src="/machine-learning/image/machine-learning-1/transformer2.png" style="width: 300px; height: 500px" alt="Card image cap">

    <ul>
      <li>Structure</li>
      <ul>
        <li>Two sublayers</li>
        <ul>
          <li>Multi-head attention</li>
          <li>Fully connected position-wise feed forward</li>
        </ul>
        <li>Each sublayer has residual connection</li>
        <ul>
          <li>Transmit unprocessed input X of sublayer to layer normalization function</li>
          <li>Ensure key information like positional encoding is not lost on the way</li>
          <li>\( \text{LayerNormalization}(x+\text{sublayer}(x)) \)</li>
        </ul>
        <li>Output of every sublayer \( d_{model} \) remain constant</li>
        <ul>
          <li>All key operations in transformer is dot product</li>
          <li>Reduces computation</li>
        </ul>
      </ul>
      <li>Input embedding</li>
      <ul>
        <li>Converts input tokens to vector of \( d_{\text{model}} = 512 \) dimension using learned embeddings</li>
        <ul>
          <li>Tokenizer transforms a sentence into tokens (each token is a single word)</li>
          <ul>
            <li>Ex. text = "The cat slept on the couch.It was too tired to get up."</li>
            <li>Ex. input_ids = [1996, 4937, 7771, 2006, 1996, 6411, 1012, 2009, 2001, 2205, 5458, 2000, 2131, 2039, 1012]</li>
          </ul>
          <li>An embedding method (Ex. skip gram) is used to convert each word to a vector of \( 512 \) numbers</li>
          <ul>
            <li>Ex. black = [[-0.01206071 ... -0.04273562]]</li>
            <li>Ex. brown = [[0.00135794589 ... -0.0490022525]]</li>
          </ul>
        </ul>
      </ul>
      <li>Positional encoding</li>
      <ul>
        <li>Considers position of a word in a sentence</li>
        <li>\( PE_{pos 2i} = \sin\left(\dfrac{pos}{10000^{\frac{2i}{d_{model}}}}\right) \)</li>
        <ul>
          <li>Applied to even position in the vector of \( d_{model} \)</li>
        </ul>
        <li>\( PE_{pos 2i+1} = \cos\left(\dfrac{pos}{10000^{\frac{2i}{d_{model}}}}\right) \)</li>
        <ul>
          <li>Applied to odd position in the vector of \( d_{model} \)</li>
        </ul>
        <li>Original authors added positional encoding to input embedding to produce final encoding</li>
        <li>To minimize loss of information of word embedding, we can multiply word embedding by \( \sqrt{\text{d_{model}}} \)</li>
        <li>Positional encoding is computed once and re-used through the training</li>
<pre><code class="python">def encoding(pos, positional_vector)
    for i in range(0, 512,2):
        val =  pos / (10000**((2*i)/d_model))
        positional_vector[0][i] = math.sin(val)
        positional_vector[0][i+1] = math.cos(val)
        positional_encoding[0][i] = (embedding_vector[0][i]*math.sqrt(d_model)) + positional_vector[0][i]
        positional_encoding[0][i+1] = (embedding_vector[0][i+1]*math.sqrt(d_model)) + positional_vector[0][i+1]</code></pre>
      </ul>
      <li>Multi-head attention</li>
      <ul>
        <li>Each word is represented by vector of \( 512 \) dimension</li>
        <li>Ex. The cat sat on the rug and it was dry-cleaned</li>
        <li>Model is trained to find if "it" is related to "cat" or "rug"</li>
        <li>There are \( 8 \) heads whose dimension is \( 64 \)</li>
        <li>In each head, weight matrices whose dimension is \( 512 \times 64 \) must be trained</li>
        <ul>
          <li>\( W_{Q} \) - train the queries</li>
          <li>\( W_{K} \) - train the keys</li>
          <li>\( W_{V} \) - train the values</li>
        </ul>
        <li>Example</li>
        <ul>
          <li>Represent the input</li>
          <ul>
            <li>Assume \( 3 \) input tokens (or words)</li>
            <li>Assume \( d_{\text{model}} = 4 \)</li>
<pre><code class="python">X = np.array([[1.0, 0.0, 1.0, 0.0],   # Input token 1
              [0.0, 2.0, 0.0, 2.0],   # Input token 2
              [1.0, 1.0, 1.0, 1.0]])  # Input token 3</code></pre>
          </ul>
          <li>Initialize the weight matrices</li>
          <ul>
            <li>Assume \( d_{k} = 3 \)</li>
<pre><code class="python">W_query = np.array([[1, 0, 1],
                    [1, 0, 0],
                    [0, 0, 1],
                    [0, 1, 1]])
W_key = np.array([[0, 0, 1],
                  [1, 1, 0],
                  [0, 1, 0],
                  [1, 1, 0]])
W_value = np.array([[0, 2, 0],
                    [0, 3, 0],
                    [1, 0, 3],
                    [1, 1, 0]])</code></pre>
          </ul>
          <li>Compute \( Q, K, V \)</li>
          <ul>
            <li>Assume there is one set of \( W_{Q}, W_{K}, W_{V} \) for all inputs</li>
<pre><code class="python">Q = np.matmul(X, W_query)
K = np.matmul(X, W_key)
V = np.matmul(X, W_value)</code></pre>
          </ul>
          <li>Compute attention score</li>
          <ul>
            <li>\( \text{Attention}(Q,K,V) = \text{softmax} \left( \dfrac{QK^{T}}{\sqrt{d_{k}}} \right) V \)</li>
<pre><code class="python">d_k = 1   # Square root of k_d simplified to 1 for this example
attention_scores = (Q@K.transpose()) / d_k</code></pre>
            <li>Softmax score for each input token</li>
<pre><code class="python">attention_scores[0] = softmax(attention_scores[0])
attention_scores[1] = softmax(attention_scores[1])
attention_scores[2] = softmax(attention_scores[2])</code></pre>
            <li>Attention for each input token</li>
<pre><code class="python">attention1 = attention_scores[0].reshape(-1,1)
attention1 = attention_scores[0][0] * V[0]
attention2 = attention_scores[0][1] * V[1]
attention3 = attention_scores[0][2] * V[2]</code></pre>
            <li>Sum up the result</li>
<pre><code class="python">attention_input1 = attention1 + attention2 + attention3</code></pre>
          </ul>
        </ul>
      </ul>
      <li>Post-layer normalization</li>
      <ul>
        <li>Add</li>
        <ul>
          <li>Processes residual connections, which transports unprocessed input \( X \) to normalization</li>
          <li>This prevents key information, such as positional encoding, from being lost on the way</li>
        </ul>
        <li>Normalization</li>
        <ul>
          <li>\( \text{LayerNormalization}(v) = \gamma \dfrac{v-\mu}{\sigma} + \beta \)</li>
          <li>\( \mu \) is the mean of \( v \) of dimension \( d \)</li>
          <ul>
            <li>\( \mu = \dfrac{1}{d} \displaystyle\sum_{k=1}^{d} v_{k} \)</li>
          </ul>
          <li>\( \sigma \) is the standard deviation of \( v \) dimension \( d \)</li>
          <ul>
            <li>\( \sigma^{2} = \dfrac{1}{d} \displaystyle\sum_{k=1}^{d} (v_{k}-\mu) \)</li>
          </ul>
          <li>\( \gamma \) is a scaling vector</li>
          <li>\( \beta \) is a bias vector</li>
        </ul>
      </ul>
      <li>Feedforward network</li>
      <ul>
        <li>Contains two layers and applies ReLU</li>
        <li>Input and output has \( d_{\text{model}} = 512 \) but inner layer has \( d_{\text{ff}} = 2048 \)</li>
        <li>\( \text{FFN}(x) = max(0, xW_{1} + b_{1})W_{2} + b_{2} \)</li>
        <li>Can be considered as two convolutions with size \( 1 \) kernels</li>
        <li>Output goes to layer normalization, then to next layer of encoder and multi-head attention layer of decoder</li>
      </ul>
    </ul>

    <h3 class="card-title">Decoder stack</h3>

    <img class="img-fluid" class="card-img-top" src="/machine-learning/image/machine-learning-1/transformer3.png" style="width: 300px; height: 700px" alt="Card image cap">

    <ul>
      <li>Masked multi-head attention</li>
      <ul>
        <li>In the output, at a given position, the following words are masked</li>
        <li>Transformer bases its assumption on its inferences without seeing the rest of the sequence</li>
        <li>Only lets attention apply to positions up to and including current position</li>
      </ul>
      <li>Multi-head attention</li>
      <ul>
        <li>Only attends to positions up to current position</li>
        <li>This avoid seeing the sequence it must predict</li>
        <li>Draws \( K, V \) from encoder and \( Q \) drom decoder to compute attention</li>
      </ul>
    </ul>

    <h3 class="card-title">Use case</h3>
    <ul>
      <li>Translation</li>
      <ul>
        <li>Input is English sentence, which goes through encoder layer</li>
        <li>Output is French sentence, which goes through decoder layer</li>
      </ul>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: Transformers for Natural Language Processing, Denis Rothman
  </div>
</div>

<div class="card mb-4" id="transformer-">
  <div class="card-body">
    <h2 class="card-title">Transformer evaluation</h2>

    <h3 class="card-title">Human</h3>
    <ul>
      <li>Input - perception of raw events for layer 0</li>
      <li>Output - language</li>
      <li>Transduction - trial-and-error</li>
      <ul>
        <li>Take structure we perceive and represent them with patterns</li>
      </ul>
      <li>We learn that many events are related (Ex. sunrise-light, sunset-dark, etc)</li>
      <li>New borns are find-tuned for many tasks by previous generations (Ex. we are taught that fire burns us)</li>
    </ul>

    <h3 class="card-title">Transformer</h3>
    <ul>
      <li>Perform transductions connecting all the tokens (subwords) that occur together in language sequence</li>
      <li>Build inductions from these transductions</li>
      <li>Train those inductions based on tokens to produce patterns of tokens</li>
    </ul>

    <h3 class="card-title">Attention</h3>
    <ul>
      <li>Attention sublayers can process millions of example for their inductive thinking operations</li>
      <li>Attention sublayers find patterns in sequences through transduction and induction</li>
      <li>Attention sublayers memorize these patterns using parameters that are stored with their model</li>
    </ul>

    <h3 class="card-title">SuperGLUE (GLUE - General language understanding evaluation)</h3>
    <ul>
      <li>Choice of plausible answers (COPA)</li>
      <ul>
        <li>Transformer chooses most plausible answer to a question</li>
      </ul>
      <li>BoolQ</li>
      <ul>
        <li>Yes-or-no answer task</li>
      </ul>
      <li>Commitment bank (CB)</li>
      <ul>
        <li>Transformer reads a premise, then examine a hypothesis</li>
        <li>Hypothesis confirms the premise or contradict it</li>
        <li>Transformer must label the hypothesis as neutral, entailment, or contradiction of the premise</li>
      </ul>
      <li>Multi-sentence reading comprehension (MultiRC)</li>
      <ul>
        <li>Transformer reads a text and choose from several possible choices</li>
      </ul>
      <li>Reading comprehension with commonsense reasoning dataset (ReCoRD)</li>
      <ul>
        <li>Dataset contains over 120,000 queries for more than 70,000 news articles</li>
        <li>Transformer uses common-sense to solve the problems</li>
      </ul>
      <li>Recognizing textual entailment (RTE)</li>
      <ul>
        <li>Transformer must read the premise, examine a hypothesis, and predict the label of entailment hypothesis status</li>
      </ul>
      <li>Words in context (WiC)</li>
      <ul>
        <li>Transformer analyzes two sentences and determine whether the target word has the same meaning in both sentences</li>
      </ul>
      <li>The Winograd schemae challence (WSC)</li>
      <ul>
        <li>Each sentence contains an occupation, a participant, and a pronoun</li>
        <li>Transformer determines whether the pronoun is coreferent with the occupation or the participant</li>
      </ul>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: Transformers for Natural Language Processing, Denis Rothman
  </div>
</div>

<div class="card mb-4" id="transformer-">
  <div class="card-body">
    <h2 class="card-title">T5</h2>
    <ul>
      <li>Stands for text-to-text transfer transformer</li>
      <li>One input format for every task submitted to the transformer</li>
      <li>Same model, hyperparameter, optimizer for a wide range of tasks</li>
      <ul>
        <li>Add a prefix to input sequence</li>
        <li>Ex. translate English to German + [sequence] for machine translation problem</li>
        <li>Ex. cola sentence + [sequence] for corpus of linguistic acceptability</li>
        <li>Ex. stsb sentence + [sequence] for semantic textual similarity benchmarks</li>
        <li>Ex. summarize + [sequence] for text summarization problem</li>
      </ul>
      <li>Same architecture as original transformer except</li>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: Transformers for Natural Language Processing, Denis Rothman
  </div>
</div>
<!-- Transformer END -->

</div> <!-- /.col-md-12 -->
</div> <!-- /.row -->
</div> <!-- /.container -->

<include src="/footer.html"></include>

</body>

</html>