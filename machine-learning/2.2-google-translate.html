<!DOCTYPE html>

<html lang="en">

<head>

<!-- Metadata -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="">
<meta name="author" content="Seungmoon Rieh">
<meta name="keywords" content="">

<!-- Title and image -->
<title>Seungmoon Rieh</title>
<link href="/img/seungmoonrieh.jpg" rel="icon">

<!-- CSS -->
<link href="/css/bootstrap.min.css" rel="stylesheet">
<link href="/css/monokai-sublime.css" rel="stylesheet">
<link href="/css/site.css" rel="stylesheet">

<!-- JavaScript -->
<script src="/js/jquery.min.js" type="text/javascript"></script>
<script src="/js/bootstrap.bundle.min.js" type="text/javascript"></script>
<script src="/js/highlight.pack.js" type="text/javascript"></script>
<script src="/js/include_html.js" type="text/javascript"></script>
<script src="/js/mathjax/tex-chtml.js" type="text/javascript"></script>
<script src="/js/site.js" type="text/javascript"></script>

</head>

<body>

<include src="/header.html"></include>

<div class="container">
<div class="row">
<div class="col-md-12">
<h1 class="my-4">Machine Learning</h1>

<!-- Google translate BEGIN -->
<div class="card mb-4" id="google-translate">
  <div class="card-body">
    <h2 class="card-title">Google translate</h2>
    <ul class="list-unstyled mb-0">
      <li><a href="#google-translate-1">Requirement</a></li>
      <li><a href="#google-translate-2">Problem</a></li>
      <li><a href="#google-translate-3">Data preparation</a></li>
      <li><a href="#google-translate-4">Model development</a></li>
      <li><a href="#google-translate-5">Evaluation</a></li>
      <li><a href="#google-translate-6">Serving</a></li>
      <li><a href="#google-translate-7">Follow up</a></li>
    </ul>
  </div>
</div>

<div class="card mb-4" id="google-translate-1">
  <div class="card-body">
    <h2 class="card-title">Requirement</h2>
    <ul>
      <li>Business objective</li>
      <ul>
        <li>Which languages should the system support? English and Korean</li>
        <li>What should be the input length limit? 1000 words</li>
        <li>Should the system detect input language automatically? Yes</li>
      </ul>
      <li>System features</li>
      <li>Data</li>
      <ul>
        <li>Do we have training data available? Yes</li>
        <li>How large is dataset? 300M pairs of English and Korean sentences and terrabytes of general text data</li>
      </ul>
      <li>Constraints</li>
      <ul>
        <li>Should the model be deployed to mobile devices? No</li>
      </ul>
      <li>Scale</li>
      <li>Performance</li>
      <ul>
        <li>How fast should the suggestions be? Translation can be done offline</li>
      </ul>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: Generative AI System Design Interview, Ali Aminian & Hao Sheng
  </div>
</div>

<div class="card mb-4" id="google-translate-2">
  <div class="card-body">
    <h2 class="card-title">Problem</h2>

    <h3 class="card-title">ML problem</h3>
    <ul>
      <li>Translate betwen English and Korean</li>
      <li>Input</li>
      <ul>
        <li>English</li>
      </ul>
      <li>Output</li>
      <ul>
        <li>Korean</li>
      </ul>
    </ul>

    <h3 class="card-title">ML category</h3>
    <ul>
      <li>Generative</li>
      <ul>
        <li>Transformer</li>
      </ul>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: Generative AI System Design Interview, Ali Aminian & Hao Sheng
  </div>
</div>

<div class="card mb-4" id="google-translate-3">
  <div class="card-body">
    <h2 class="card-title">Data preparation</h2>

    <h3 class="card-title">Data engineering</h3>
    <ul>
      <li>Training data</li>
      <ul>
        <li>General data</li>
        <li>Email data</li>
      </ul>
      <li>Database</li>
      <ul>
        <li>Email</li>
        <ul>
          <li>email_id</li>
          <li>sender</li>
          <li>receiver</li>
          <li>subject</li>
          <li>body</li>
        </ul>
      </ul>
    </ul>

    <h3 class="card-title">Feature engineering</h3>
    <ul>
      <li>Text cleaning</li>
      <ul>
        <li>Remove missing data</li>
        <li>Remove incorrect/noisy data</li>
        <li>Remove duplicate data</li>
        <li>Handle entity names</li>
        <ul>
          <li>Replace entities with placeholder in order not to confuse the model with uncommon terms</li>
        </ul>
      </ul>
      <li>Text cleaning that we should not perform in LLM</li>
      <ul>
        <li>Lowercasing</li>
        <li>Removing stop words </li>
        <ul>
          <li>Ex. the, and, in</li>
        </ul>
        <li>Stemming and lemmatization</li>
        <li>Removing punctuations</li>
      </ul>
      <li>Text tokenization</li>
      <ul>
        <li>Byte-Pair Encoding (BPE)</li>
        <ul>
          <li>For all words in data set, append&lt;/w&gt; to mark the end of word</li>
          <li>For all words in data set, split into chars and create frequency table for each char</li>
          <li>Starting from chars with highest frequency, iteratively merge chars to create subwords and count frequency of those subwords</li>
          <li>Iteration continues to merge subwords into bigger subwords until reaching pre-defined size</li>
        </ul>
      </ul>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: Generative AI System Design Interview, Ali Aminian & Hao Sheng
  </div>
</div>

<div class="card mb-4" id="google-translate-4">
  <div class="card-body">
    <h2 class="card-title">Model development</h2>

    <h3 class="card-title">Model selection</h3>
    <ul>
      <li>Encoder-decoder</li>
    </ul>

    <h3 class="card-title">Model training</h3>
    <ul>
      <li>Architecture</li>
      <ul>
        <li>Encoder</li>
        <ul>
          <li>Token embedding</li>
          <li>Positional encoding</li>
          <li>Transformer</li>
        </ul>
        <li>Decoder</li>
        <ul>
          <li>Token embedding</li>
          <li>Positional encoding</li>
          <li>Transformer</li>
          <ul>
            <li>Cross attention integrates information from encoder</li>
            <li>All word/token attends to only the word/token that comes before (future tokens are masked)</li>
            <li>Model should only use previous tokens to generate next tokens</li>
          </ul>
          <li>Prediction head</li>
          <ul>
            <li>Prediction head contains linear layer followed by softmax layer</li>
          </ul>
        </ul>
      </ul>
      <li>Loss function</li>
      <ul>
        <li>Pre-training (unsupervised)</li>
        <ul>
          <li>Train on general data that has English and Korean</li>
          <li>Objective is to predict masked tokens (Masked Language Modeling)</li>
          <ul>
            <li>Randomly mask subset of input tokens by replacing them with [MASK]</li>
            <li>Feed masked sequence to encoder</li>
            <li>Encoder outputs a sequence of new embeddings for each token</li>
            <li>Feed the same masked sequence to decoder but without masking and shifting one position to right by inserting start token &gt;BOS&lt;</li>
            <li>Decoder predicts next token for each position in sequence</li>
            <li>Calculate cross-entropy loss over predicted probabilities and ground truth for masked tokens only</li>
          </ul>
          <li>Existing models (T5, BART) can be used instead</li>
        </ul>
        <li>Fine-tuning (supervised)</li>  
        <ul>
          <li>Train on pairs of English-Korean sentences</li>
          <ul>
            <li>Option 1. bilingual</li>
            <ul>
              <li>Higher accuracy</li>
              <li>Hard to handle multiple languages in terms of trainning, deployment, monitoring</li>
            </ul>
            <li>Option 2. multilingual</li>
            <ul>
              <li>Lower accuracy</li>
              <li>Simple</li>
            </ul>
          </ul>
          <li>Objective is to predict next token</li>
          <li>Cross-entropy loss to measure difference between predicted vs actual next token</li>
        </ul>
      </ul>
      <li>Hyperparameters</li>
      <li>Sampling</li>
      <ul>
        <li>Beam search for accuracy and consistency of translation</li>
      </ul>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: Generative AI System Design Interview, Ali Aminian & Hao Sheng
  </div>
</div>

<div class="card mb-4" id="google-translate-5">
  <div class="card-body">
    <h2 class="card-title">Evaluation</h2>

    <h3 class="card-title">Offline</h3>
    <ul>
      <li>Option 1. BiLingual Evaludation Understudy (BLUE)</li>
      <ul>
        <li>Precision based which compares n-gram of candidate translation against n-gram of reference translations</li>
        <li>\( \text{BLEU} = \text{BP}\cdot\text{exp} \left( \displaystyle\sum_{n=1}^{N}w_{n}\text{log}p_{n}\right) \)</li>
        <ul>
          <li>\( N \) - maximum n-gram length considered for evaluation</li>
          <li>\( \text{BP}  =  1 \) if \( c \gt r \), \( \text{BP}  =  0\)  if \( c \le r \) </li>
          <ul>
            <li>\( c \) - candidate translation length</li>
            <li>\( r \) - reference translation length<</li>
          </ul>
          <li>\( p_{n} \) - precision that measures how many n-grams in candidate translation are present in reference translation</li>
          <li>\( w_{n} \) - weights corresponding to precision of each n-gram size</li>
        </ul>
        <li>May unfairly penalize good translations that has just different ordering of words</li>
      </ul>
      <li>Option 2. Recall Oriented Understudy for Gisting Evaluation (ROUGE)</li>
      <ul>
        <li>Recall based which computes ratio of matching n-gram in candidate translation over total number of n-grams in reference translations</li>
        <li>Similar to BLUE (focus on recall rather than precision) and has same drawbacks</li>
      </ul>
      <li>Option 3. Metric for Evaluation of Translation with Explicit ORdering (METEOR)</li>
      <ul>
        <li>Matches synonyms rather than exact words</li>
        <li>Relies on linguistic resources such as synonym dictionaries and stemming algorithms</li>
      </ul>
    </ul>

    <h3 class="card-title">Online</h3>
    <ul>
      <li>User feedback</li>
      <li>User engagement</li>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: Generative AI System Design Interview, Ali Aminian & Hao Sheng
  </div>
</div>

<div class="card mb-4" id="google-translate-6">
  <div class="card-body">
    <h2 class="card-title">Serving</h2>

    <img class="img-fluid" class="card-img-top" src="/machine-learning/image/machine-learning-system-design-1/google-translate-1.png" alt="Card image cap">

    <ul>
      <li>Language detector service</li>
      <ul>
        <li>Use encoder-only model to detect language given input sentence</li>
        <ul>
          <li>Option 1. average pooling - pass transformer output to average pooling layer, then to prediction head</li>
          <li>Option 2. last token representation - pass last token representation from transformer output directly to prediction head</li>
        </ul>
      </ul>
      <li>Translation service</li>
      <ul>
        <li>Calls the model to receive translation of detected sentence</li>
      </ul>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: Generative AI System Design Interview, Ali Aminian & Hao Sheng
  </div>
</div>

<div class="card mb-4" id="google-translate-7">
  <div class="card-body">
    <h2 class="card-title">Follow up</h2>
    <ul>
      <li>Explain how to deal with limited data via transfer learning</li>
      <li>Explain how to build translation with decoder-only model</li>
      <li>Explain how to improve translation using user feedback</li>
      <li>Explain how to achieve efficient inference for models deployed on device</li>
      <li>Explain how to develop a single multilingual model</li>
      <li>Explain WER</li>
      <li>Explain how to build a language detection model</li>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: Generative AI System Design Interview, Ali Aminian & Hao Sheng
  </div>
</div>
<!-- Google translate END -->

</div> <!-- /.col-md-12 -->
</div> <!-- /.row -->
</div> <!-- /.container -->

<include src="/footer.html"></include>

</body>

</html>