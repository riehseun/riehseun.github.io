<!DOCTYPE html>

<html lang="en">

<head>

<!-- Metadata -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="">
<meta name="author" content="Seungmoon Rieh">
<meta name="keywords" content="">

<!-- Title and image -->
<title>Seungmoon Rieh</title>
<link href="/img/seungmoonrieh.jpg" rel="icon">

<!-- CSS -->
<link href="/css/bootstrap.min.css" rel="stylesheet">
<link href="/css/monokai-sublime.css" rel="stylesheet">
<link href="/css/site.css" rel="stylesheet">

<!-- JavaScript -->
<script src="/js/jquery.min.js"></script>
<script src="/js/bootstrap.bundle.min.js"></script>
<script src="/js/highlight.pack.js"></script>
<script src="/js/include_html.js" type="text/javascript"></script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script src="/js/site.js" type="text/javascript"></script>

</head>

<body>

<include src="/header.html"></include>

<div class="container">
<div class="row">
<div class="col-md-12">
<h1 class="my-4">Machine Learning</h1>

<!-- logistic regression BEGIN -->
<div class="card mb-4" id="subject">
  <div class="card-body">
    <h2 class="card-title">Logistic regression</h2>
    <p class="card-text"></p>
    <ul class="list-unstyled mb-0">
      <li><a href="#logistic-regression-">Logistic regression</a></li>
      <li><a href="#logistic-regression-">Set up</a></li>
      <li><a href="#logistic-regression-">Implement</a></li>
      <li><a href="#logistic-regression-">Softmax regression</a></li>
    </ul>
  </div>
</div>

<div class="card mb-4" id="logistic-regression-">
  <div class="card-body">
    <h2 class="card-title">Logistic regression</h2>
    <ul>
      <li>Used for binary classification</li>
      <li>Fits curve to data</li>
      <ul>
        <li>Still is linear model because there is linear relationship between input and output</li>
      </ul>
      <li>There is no layer (Or hidden units)</li>
    </ul>

    <h3 class="card-title">Steps</h3>
    <ul>
      <li>Load data</li>
      <li>If necessary</li>
      <ul>
        <li>Flatten matrix inputs into vectors</li>
        <li>Normalize the data</li>
      </ul>
      <li>Initialize parameters (Weights can be initialized to zero)</li>
      <li>Iterate</li>
      <ul>
        <li>Forward prop</li>
        <li>Compute cost</li>
        <li>Backward prop</li>
        <li>Retrive gradient</li>
        <li>Update weights</li>
      </ul>
      <li>Use the final weights and training data to do compute prediction</li>
      <li>Apply sigmoid to the prediction, which gives value either 0 or 1</li>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://github.com/iamtodor/data-science-interview-questions-and-answers">Data science interview questions with answers</a>
  </div>
</div>

<div class="card mb-4" id="logistic-regression-">
  <div class="card-body">
    <h2 class="card-title">Set up</h2>
    <ul>
      <li>\( x \in R, y \in \{0,1\} \)</li>
      <li>\( m \) training examples such that \( \{ (x^{(1)},y^{(1)}) \dots (x^{(m)},y^{(m)}) \} \)</li>
    </ul>

    <h3 class="card-title">Parameters</h3>
    <ul>
      <li>Number of features \( n_{x} = n^{[0]} \)</li>
      <li>Number of output units \( n^{[1]} = 1 \)</li>
    </ul>

    <h3 class="card-title">Dimensions</h3>
    <ul>
      <li>\( X \).shape = \( (n_{x},m) \)</li>
      <li>\( Y \).shape = \( (1,m) \)</li>
    </ul>

    <h3 class="card-title">Forward prop</h3>
    <ul>
      <li>\( X = \begin{bmatrix} \vdots & \vdots & \vdots \\ \vdots & \vdots & \vdots \\ x^{(1)} & x^{(2)} \ldots & x^{(m)} \\ \vdots & \vdots & \vdots \\ \vdots & \vdots & \vdots \end{bmatrix} \)</li>
      <li>\( Z = \begin{bmatrix} z^{(1)} & z^{(2)} \ldots & z^{(m)} \end{bmatrix} = \begin{bmatrix} w^{T}x^{(1)}+b & w^{T}x^{(2)}+b \ldots & w^{T}x^{(m)}+b \end{bmatrix} = w^{T}X + \begin{bmatrix} b & b \ldots & b \end{bmatrix} \)</li>
      <li>\( \hat{y} = A = \begin{bmatrix} a^{(1)} & a^{(2)} \ldots & a^{(m)} \end{bmatrix} = \sigma(Z) = \sigma (w^{T}X+b) \)</li>
      <ul>
        <li>Without \( \sigma \), it is just linear regression</li>
      </ul>
      <li>\( Y = \begin{bmatrix} y^{(1)} & y^{(2)} \ldots & y^{(m)} \end{bmatrix} \)</li>
    </ul>

    <h3 class="card-title">Cost function</h3>
    <ul>
      <li>Loss function: \( L(\hat{y},y) = -(ylog\hat{y} + (1-y)log(1-\hat{y})) \)</li>
      <ul>
        <li>This loss function is convex, thus gradient descent can find the global optimum</li>
        <ul>
          <li>For example, \( L(\hat{y},y) = \frac{1}{2}(\hat{y}-y)^{2} \) something like this is not convex</li>
        </ul>
        <li>If \( y = 1 \), \( L(\hat{y}, y) = -log\hat{y} \)</li>
        <ul>
          <li>We want \( \hat{y} \) large as possible ( \( y \approx 1 \) )</li>
        </ul>
        <li>If \( y = 0 \), \( L(\hat{y}, y) = -log(1-\hat{y}) \)</li>
        <ul>
          <li>We want \( \hat{y} \) small as possible ( \( y \approx 0 \) )</li>
        </ul>
      </ul>
      <li>Cost function: \( J(w,b) = \dfrac{1}{m}\displaystyle\sum_{i=1}^{m} L(\hat{y}^{(i)},y^{(i)}) \)</li>
    </ul>

    <h3 class="card-title">Backward prop</h3>
    <ul>
      <li>\( dZ = \begin{bmatrix} dz^{(1)} & dz^{(2)} \ldots & dz^{(m)} \end{bmatrix} = \begin{bmatrix} a^{(1)}-y^{(1)} & a^{(2)}-y^{(2)} \ldots & a^{(m)}-y^{(m)} \end{bmatrix} = A - Y \)</li>
      <li>\( dw = \dfrac{1}{m}XdZ^{T} \)</li>
      <li>\( db = \dfrac{1}{m}np.sum(dZ) \)</li>
      <li>\( w := w - \alpha dw \)</li>
      <li>\( b := b - \alpha db \)</li>
    </ul>

    <ul>
      <li>Let \( \textbf{a} = \sigma({\textbf{z}}) \) , \( \textbf{z} = \textbf{W}^{T}\textbf{x} + \textbf{b} \)</li>
      <li>\( \textbf{dz} = \textbf{a} - \textbf{y} \)</li>
      <li>Then, \( \textbf{dw} = \frac{1}{m}\textbf{x}\textbf{dz}^{T} \), \( \textbf{db} = \dfrac{1}{m}\displaystyle\sum_{1}^{m}dz^{(i)} \)</li>
      <li>Then, \( \textbf{w} = \textbf{w} - \alpha \textbf{dw} \), \( \textbf{b} = \textbf{b} - \alpha \textbf{db} \)</li>
    </ul>

    <!-- <h4 class="card-title">Consider single training example</h4>
    <ul>
      <li>Let \( \hat{y} = a \), \( z = w_{1}x_{1} + w_{2}x_{2} + b \) </li>
      <li>\( da = \frac{dL}{da} = -\frac{y}{a} + \frac{1-y}{1-a} \)</li>
      <li>\( \textbf{dz} = \frac{dL}{dz} = \frac{dL}{da}\frac{da}{dz} =  (-\frac{y}{a} + \frac{1-y}{1-a}) a(1-a) = \textbf{a-y} \)</li>
      <li>Then, \( dw_{1} = x_{1}dz \), \( dw_{2} = x_{2}dz \), \( db = dz \)</li>
      <li>Then, \( w_{1} = w_{1} - \alpha dw_{1} \), \( w_{2} = w_{2} - \alpha dw_{2} \), \( b = b - \alpha db \)</li>
    </ul> -->

<pre><code class="python">def propagate(w, b, X, Y):

    # Forward prop
    m = X.shape[1]
    Z = np.dot(w.T, X) + b
    A = sigmoid(Z)

    # Compute cost
    cost = -(np.dot(Y, np.log(A.T)) + np.dot((1-Y), np.log(1-A.T))) / m
    cost = np.squeeze(cost)

    # Backward prop
    dZ = A - Y
    dw = np.dot(X, (dZ).T) / m
    db = np.sum(dZ) / m

    grads = {"dw": dw, "db": db}

    return grads, cost</code></pre>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://www.coursera.org/specializations/deep-learning?">Deep Learning Specialization</a>
  </div>
</div>


<div class="card mb-4" id="logistic-regression-">
  <div class="card-body">
    <h2 class="card-title">Implement</h2>

<pre><code class="python">def load_dataset(train_set_location, test_set_location):

    train_dataset = h5py.File(train_set_location, "r")
    # These data sets are to be pre-preprocessed (Thus, appended "_orig")
    train_set_x_orig = np.array(train_dataset["train_set_x"][:])  # Train set features
    train_set_y_orig = np.array(train_dataset["train_set_y"][:])  # Train set labels

    test_dataset = h5py.File(test_set_location, "r")
    # These data sets are to be pre-preprocessed (Thus, appended "_orig")
    test_set_x_orig = np.array(test_dataset["test_set_x"][:])  # Test set features
    test_set_y_orig = np.array(test_dataset["test_set_y"][:])  # Test set labels

    classes = np.array(test_dataset["list_classes"][:])  # List of classes

    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))
    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))

    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes

# Load the data (cat/non-cat)
train_set_location = 'data/train_catvnoncat.h5'
test_set_location = 'data/test_catvnoncat.h5'
train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset(train_set_location, test_set_location)</code></pre>

<pre><code class="python"># Reshape the training and test examples such that matrices are flattened into vectors
train_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[0], -1).T
test_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[0], -1).T

# Center and standardize dataset dividing by the maximum value of a pixel channel
train_set_x = train_set_x_flatten / 255
test_set_x = test_set_x_flatten / 255</code></pre>

<pre><code class="python">def initialize_with_zeros(dim):

    w = np.zeros((dim, 1))
    b = 0

    return w, b</code></pre>

<pre><code class="python">def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):

    for i in range(num_iterations):

        # Cost and gradient calculation
        grads, cost = propagate(w, b, X, Y)

        # Retrieve derivatives from grads
        dw = grads["dw"]
        db = grads["db"]

        # Update rule
        w = w - learning_rate * dw
        b = b - learning_rate * db

    params = {"w": w, "b": b}
    grads = {"dw": dw, "db": db}

    return params, grads, costs</code></pre>

<pre><code class="python">def predict(w, b, X):

    m = X.shape[1]
    Y_prediction = np.zeros((1,m))
    w = w.reshape(X.shape[0], 1)

    # Compute vector "A" predicting the probabilities of a cat being present in the picture
    A = sigmoid(np.dot(w.T, X) + b)

    for i in range(A.shape[1]):

        # Convert probabilities A[0,i] to actual predictions p[0,i]
        if A[0, i] <= 0.5:
            Y_prediction[0, i] = 0
        else:
            Y_prediction[0, i] = 1

    return Y_prediction</code></pre>

<pre><code class="python">def sigmoid(z):

    s = 1 / (1 + np.exp(-z))

    return s</code></pre>

<pre><code class="python">def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):

    # Initialize parameters with zeros
    w, b = initialize_with_zeros(X_train.shape[0])

    # Gradient descent.
    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost = False)

    # Retrieve parameters w and b from dictionary "parameters"
    w = parameters["w"]
    b = parameters["b"]

    # Predict test/train set examples
    Y_prediction_test = predict(w, b, X_test)
    Y_prediction_train = predict(w, b, X_train)

    d = {"costs": costs,
         "Y_prediction_test": Y_prediction_test,
         "Y_prediction_train" : Y_prediction_train,
         "w" : w,
         "b" : b,
         "learning_rate" : learning_rate,
         "num_iterations": num_iterations}

    return d</code></pre>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://www.coursera.org/specializations/deep-learning?">Deep Learning Specialization</a>
  </div>
</div>

<div class="card mb-4" id="logistic-regression-">
  <div class="card-body">
    <h2 class="card-title">Softmax regression</h2>
    <ul>
      <li>Generalization of logistic regression</li>
      <li>Number of units in the output layer \( n^{L} = C \) the number of classes</li>
      <li>If \( C = 2 \), applying softmax reduces to logistic regression</li>
    </ul>

    <h3 class="card-title">Activation in the output layer</h3>
    <ul>
      <li>In the output layer \( z^{[L]} = w^{[L]}a^{[L-1]} + b^{[l]} \)</li>
      <li>The activation in the last layer for softmax is \( a^{[L]} = \dfrac{e^{z^{[L]}}}{\sum_{i=1}^{C}t_{i}} \) where \( t = e^{z^{[L]}} \)</li>
    </ul>

    <h3 class="card-title">Loss function</h3>
    <ul>
      <li>\( L(\hat{y},y) = -\displaystyle\sum_{j=1}^{C} y_{j}log\hat{y}_{j} \)</li>
    </ul>

    <h3 class="card-title">Gradient descent</h3>
    <ul>
      <li>\( \textbf{dz} = \hat{\textbf{y}} - \textbf{y} \)</li>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://www.coursera.org/specializations/deep-learning?">Deep Learning Specialization</a>
  </div>
</div>
<!-- Logistic regression END -->

</div> <!-- /.col-md-12 -->
</div> <!-- /.row -->
</div> <!-- /.container -->

<include src="/footer.html"></include>

</body>

</html>