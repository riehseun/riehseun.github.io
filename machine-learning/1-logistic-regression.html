<!DOCTYPE html>

<html lang="en">

<head>

<!-- Metadata -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="">
<meta name="author" content="Seungmoon Rieh">
<meta name="keywords" content="">

<!-- Title and image -->
<title>Seungmoon Rieh</title>
<link href="/img/seungmoonrieh.jpg" rel="icon">

<!-- CSS -->
<link href="/css/bootstrap.min.css" rel="stylesheet">
<link href="/css/monokai-sublime.css" rel="stylesheet">
<link href="/css/site.css" rel="stylesheet">

<!-- JavaScript -->
<script src="/js/jquery.min.js" type="text/javascript"></script>
<script src="/js/bootstrap.bundle.min.js" type="text/javascript"></script>
<script src="/js/highlight.pack.js" type="text/javascript"></script>
<script src="/js/include_html.js" type="text/javascript"></script>
<script src="/js/mathjax/tex-chtml.js" type="text/javascript"></script>
<script src="/js/site.js" type="text/javascript"></script>

</head>

<body>

<include src="/header.html"></include>

<div class="container">
<div class="row">
<div class="col-md-12">
<h1 class="my-4">Machine Learning</h1>

<!-- Logistic regression BEGIN -->
<div class="card mb-4" id="subject">
  <div class="card-body">
    <h2 class="card-title">Logistic regression</h2>
    <ul class="list-unstyled mb-0">
      <li><a href="#logistic-regression-">Binary classification</a></li>
      <li><a href="#logistic-regression-">Multi-class classification (softmax regression)</a></li>
    </ul>
  </div>
</div>

<div class="card mb-4" id="logistic-regression-">
  <div class="card-body">
    <h2 class="card-title">Binary classification</h2>

<pre><code class="python"># Load data
# Standardize data
# Intialize parameters
# Train
#   Compute Z, A (forward porp)
#   Compute cost
#   Compute gradient (backward prop)
#   Update weights
# Inference

from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from torch.utils.data import Dataset, DataLoader

import torch
import torch.nn as nn
import torch.nn.functional as F

# Data

class CustomDataset(Dataset):

    def __init__(self, X_train, y_train):
        self.x = F.normalize(torch.from_numpy(X_train))  # (m, n)
        self.y = torch.from_numpy(y_train) # (m)

    def __len__(self):
        return len(self.x)

    def __getitem__(self, index):
        return self.x[index].float(), self.y[index].float()

dataset = load_breast_cancer()
n = dataset.data.shape[1]
print(f"num_features: {n}")
m = len(dataset.data)
print(f"num_data: {m}")
X_train, X_test, y_train, y_test = train_test_split(dataset.data, dataset.target, test_size=0.2)
train_data = CustomDataset(X_train, y_train)
test_data = CustomDataset(X_test, y_test)
train_dataloader = DataLoader(train_data, batch_size=64, shuffle=True)
test_dataloader = DataLoader(test_data, batch_size=1, shuffle=True)

# Model
class LogisticRegression(nn.Module):

    def __init__(self, num_features):
        super().__init__()
        # X (m,n) => Y (m, 1)
        self.linear = nn.Linear(num_features, 1)

    def forward(self, x):
        x = self.linear(x)
        x = torch.sigmoid(x)
        x = torch.squeeze(x)
        return x

# Train
model = LogisticRegression(n)
learning_rate = 0.01
criterion = nn.BCELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

epoch = 1000

print(model.state_dict())

for e in range(epoch):
    for x_batch, y_batch in train_dataloader:

        optimizer.zero_grad()  # Reset grads

        y_hat = model(x_batch)  # (m, 1)

        # print(f"x_batch: {x_batch.shape}")
        # print(f"y_batch: {y_batch.shape}")
        # print(f"y_hat: {y_hat.shape}")
        # print(y_batch)
        # print(y_hat)

        loss = criterion(y_hat, y_batch)  # Compute loss

        loss.backward()  # Compute gradient

        optimizer.step()  # Adjust learning rate

        if (e+1) % 100 == 0:
            print(f'epoch: {e+1}, loss = {loss.item()}')

print(model.state_dict())

# Inference
correct = 0
incorrect = 0
with torch.no_grad():
    for x_batch, y_batch in test_dataloader:
        y_pred = model(x_batch)  # no need to call model.forward()
        if y_pred > 0.5:
            y_pred = 1
        else:
            y_pred = 0
        if y_pred == y_batch[0]:
            correct += 1
        else:
            incorrect += 1
    accuracy = correct/(correct+incorrect)
    print(f"accurach: {accuracy}")</code></pre>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href=""></a>
  </div>
</div>

<div class="card mb-4" id="logistic-regression-">
  <div class="card-body">
    <h2 class="card-title">Multi-class classification (softmax regression)</h2>
    <ul>
      <li>Generalization of logistic regression</li>
      <li>Number of units in the output layer is \( n^{L} = C \) the number of classes</li>
      <li>If \( C = 2 \), applying softmax reduces to logistic regression</li>
    </ul>

    <h3 class="card-title">Activation in the output layer</h3>
    <ul>
      <li>In the output layer \( z^{[L]} = w^{[L]}a^{[L-1]} + b^{[l]} \)</li>
      <li>The activation in the last layer for softmax is \( a^{[L]} = \dfrac{e^{z^{[L]}}}{\sum_{i=1}^{C}t_{i}} \) where \( t = e^{z^{[L]}} \)</li>
    </ul>

    <h3 class="card-title">Loss function</h3>
    <ul>
      <li>\( L(\hat{y},y) = -\displaystyle\sum_{j=1}^{C} y_{j}log\hat{y}_{j} \)</li>
    </ul>

    <h3 class="card-title">Gradient descent</h3>
    <ul>
      <li>\( \textbf{dZ} = \textbf{A} - \textbf{y} \)</li>
    </ul>

<pre><code class="python"># Load data
# Standardize data
# Intialize parameters
# Train
#   Compute Z, A (forward porp)
#   Compute cost
#   Compute gradient (backward prop)
#   Update weights
# Inference

from sklearn.datasets import load_wine
from sklearn.model_selection import train_test_split
from torch.utils.data import Dataset, DataLoader

import torch
import torch.nn as nn
import torch.nn.functional as F

# Data

class CustomDataset(Dataset):

    def __init__(self, X_train, y_train, c):
        self.x = F.normalize(torch.from_numpy(X_train))  # (m, n)
        self.y = torch.from_numpy(y_train) # (m)

    def __len__(self):
        return len(self.x)

    def __getitem__(self, index):
        return self.x[index].float(), self.y[index]

dataset = load_wine()
n = dataset.data.shape[1]
print(f"num_features: {n}")
c = 3
print(f"num_classes: {c}")
m = len(dataset.data)
print(f"num_data: {m}")
X_train, X_test, y_train, y_test = train_test_split(dataset.data, dataset.target, test_size=0.2)
train_data = CustomDataset(X_train, y_train, c)
test_data = CustomDataset(X_test, y_test, c)
train_dataloader = DataLoader(train_data, batch_size=64, shuffle=True)
test_dataloader = DataLoader(test_data, batch_size=1, shuffle=True)

# Model
class LogisticRegression(nn.Module):

    def __init__(self, num_features, num_classes):
        super().__init__()
        # X (m,n) => Y (m, num_classes)
        self.linear = nn.Linear(num_features, num_classes)

    def forward(self, x):
        x = self.linear(x)
        # x = torch.softmax(x, dim=1)
        return x

# Train
model = LogisticRegression(n, c)
learning_rate = 0.01
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

epoch = 1000

print(model.state_dict())

for e in range(epoch):
    for x_batch, y_batch in train_dataloader:

        optimizer.zero_grad()  # Reset grads

        y_hat = model(x_batch)  # (m, 3)

        # print(f"x_batch: {x_batch.shape}")
        # print(f"y_batch: {y_batch.shape}")
        # print(f"y_hat: {y_hat.shape}")
        # print(y_hat)
        # print(y_batch)

        loss = criterion(y_hat, y_batch)  # Compute loss

        loss.backward()  # Compute gradient

        optimizer.step()  # Adjust learning rate

        if (e+1) % 100 == 0:
            print(f'epoch: {e+1}, loss = {loss.item()}')

print(model.state_dict())

# Inference
correct = 0
incorrect = 0
with torch.no_grad():
    for x_batch, y_batch in test_dataloader:
        y_pred = model(x_batch)  # no need to call model.forward()
        # print(f"{torch.tensor(torch.argmax(y_pred))}:{y_batch}")
        if torch.argmax(y_pred) == y_batch[0]:
            correct += 1
        else:
            incorrect += 1
    accuracy = correct/(correct+incorrect)
    print(f"accurach: {accuracy}")</code></pre>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href=""></a>
  </div>
</div>
<!-- Logistic regression END -->

</div> <!-- /.col-md-12 -->
</div> <!-- /.row -->
</div> <!-- /.container -->

<include src="/footer.html"></include>

</body>

</html>