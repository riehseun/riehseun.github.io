<!DOCTYPE html>

<html lang="en">

<head>

<!-- Metadata -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="">
<meta name="author" content="Seungmoon Rieh">
<meta name="keywords" content="">

<!-- Title and image -->
<title>Seungmoon Rieh</title>
<link href="/img/seungmoonrieh.jpg" rel="icon">

<!-- CSS -->
<link href="/css/bootstrap.min.css" rel="stylesheet">
<link href="/css/monokai-sublime.css" rel="stylesheet">
<link href="/css/site.css" rel="stylesheet">

<!-- JavaScript -->
<script src="/js/jquery.min.js" type="text/javascript"></script>
<script src="/js/bootstrap.bundle.min.js" type="text/javascript"></script>
<script src="/js/highlight.pack.js" type="text/javascript"></script>
<script src="/js/include_html.js" type="text/javascript"></script>
<script src="/js/mathjax/tex-chtml.js" type="text/javascript"></script>
<script src="/js/site.js" type="text/javascript"></script>

</head>

<body>

<include src="/header.html"></include>

<div class="container">
<div class="row">
<div class="col-md-12">
<h1 class="my-4">Machine Learning</h1>

<!-- logistic regression BEGIN -->
<div class="card mb-4" id="subject">
  <div class="card-body">
    <h2 class="card-title">Logistic regression</h2>
    <ul class="list-unstyled mb-0">
      <li><a href="#logistic-regression-">Logistic regression</a></li>
      <li><a href="#logistic-regression-">Single layer network</a></li>
      <li><a href="#logistic-regression-">Derivatives</a></li>
      <li><a href="#logistic-regression-">Softmax regression</a></li>
      <li><a href="#logistic-regression-">Logistic regression problems</a></li>
    </ul>
  </div>
</div>

<div class="card mb-4" id="logistic-regression-">
  <div class="card-body">
    <h2 class="card-title">Logistic regression</h2>
    <ul>
      <li>Used for binary classification</li>
      <li>Fits curve to data</li>
      <ul>
        <li>Still is linear model because there is linear relationship between input and output</li>
      </ul>
      <li>There is no layer (Thus no hidden units)</li>
    </ul>

    <h3 class="card-title">Steps</h3>
    <ul>
      <li>Load data</li>
      <li>If necessary</li>
      <ul>
        <li>Flatten matrix inputs into vectors</li>
        <li>Normalize the data</li>
      </ul>
      <li>Initialize parameters (Weights can be initialized to zero)</li>
      <li>Iterate</li>
      <ul>
        <li>Forward prop</li>
        <li>Compute cost</li>
        <li>Backward prop</li>
        <li>Retrive gradient</li>
        <li>Update weights</li>
      </ul>
      <li>Use the final weights and training data to do compute prediction</li>
      <li>Apply sigmoid to the prediction, which gives value either 0 or 1</li>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://www.coursera.org/specializations/deep-learning">Deep Learning Specialization</a>
  </div>
</div>

<div class="card mb-4" id="logistic-regression-">
  <div class="card-body">
    <h2 class="card-title">Single layer network</h2>
    <ul>
      <li>\( m \) training examples such that \( \{ (x^{(1)},y^{(1)}) \dots (x^{(m)},y^{(m)}) \} \)</li>
      <li>\( X = \begin{bmatrix} \vdots & \vdots & \vdots \\ \vdots & \vdots & \vdots \\ x^{(1)} & x^{(2)} \ldots & x^{(m)} \\ \vdots & \vdots & \vdots \\ \vdots & \vdots & \vdots \end{bmatrix} \)</li>
      <li>\( Y = \begin{bmatrix} y^{(1)} & y^{(2)} \ldots & y^{(m)} \end{bmatrix} \)</li>
    </ul>

    <h3 class="card-title">Parameters</h3>
    <ul>
      <li>Number of features \( n_{x} = n^{[0]} \)</li>
      <li>Number of output units \( n^{[1]} = 1 \)</li>
    </ul>

    <h3 class="card-title">Dimensions</h3>
    <ul>
      <li>\( X \).shape = \( (n_{x},m) \)</li>
      <li>\( Y \).shape = \( (1,m) \)</li>
    </ul>

    <h3 class="card-title">Forward prop</h3>
    <ul>
      <li>\( Z = \begin{bmatrix} z^{(1)} & z^{(2)} \ldots & z^{(m)} \end{bmatrix} = \begin{bmatrix} w^{T}x^{(1)}+b & w^{T}x^{(2)}+b \ldots & w^{T}x^{(m)}+b \end{bmatrix} = W^{T}X + \begin{bmatrix} b & b \ldots & b \end{bmatrix} \)</li>
      <li>\( \hat{y} = A = \begin{bmatrix} a^{(1)} & a^{(2)} \ldots & a^{(m)} \end{bmatrix} = \sigma(Z) = \sigma (W^{T}X+b) \)</li>
      <ul>
        <li>Without \( \sigma \), it is just linear regression</li>
      </ul>
    </ul>

<pre><code class="python">Z = np.dot(w.T, X) + b
A = sigmoid(Z)</code></pre>

    <h3 class="card-title">Cost function</h3>
    <ul>
      <li>Loss function \( L(\hat{y},y) = -\left( ylog(\hat{y}) + (1-y)log(1-\hat{y}) \right) \)</li>
      <ul>
        <li>This loss function is convex, thus gradient descent can find the global optimum</li>
        <ul>
          <li>For example, \( L(\hat{y},y) = \frac{1}{2}(\hat{y}-y)^{2} \) something like this is not convex</li>
        </ul>
        <li>If \( y = 1 \), \( L(\hat{y}, y) = -log\hat{y} \)</li>
        <ul>
          <li>We want \( \hat{y} \) large as possible ( \( y \approx 1 \) )</li>
        </ul>
        <li>If \( y = 0 \), \( L(\hat{y}, y) = -log(1-\hat{y}) \)</li>
        <ul>
          <li>We want \( \hat{y} \) small as possible ( \( y \approx 0 \) )</li>
        </ul>
      </ul>
      <li>Cost function \( J(w,b) = \dfrac{1}{m}\displaystyle\sum_{i=1}^{m} L(\hat{y}^{(i)},y^{(i)}) \)</li>
    </ul>

<pre><code class="python">m = X.shape[1]
cost = -(np.dot(Y, np.log(A.T)) + np.dot((1-Y), np.log(1-A.T))) / m
cost = np.squeeze(cost)</code></pre>

    <h3 class="card-title">Backward prop</h3>
    <ul>
      <li>\( dZ = \begin{bmatrix} dz^{(1)} & dz^{(2)} \ldots & dz^{(m)} \end{bmatrix} = \begin{bmatrix} a^{(1)}-y^{(1)} & a^{(2)}-y^{(2)} \ldots & a^{(m)}-y^{(m)} \end{bmatrix} = A - Y \)</li>
      <li>\( dw = \dfrac{1}{m}XdZ^{T} \)</li>
      <li>\( db = \dfrac{1}{m}np.sum(dZ) \)</li>
      <li>\( w := w - \alpha dw \)</li>
      <li>\( b := b - \alpha db \)</li>
    </ul>

<pre><code class="python">m = X.shape[1]
dZ = A - Y
dw = np.dot(X, (dZ).T) / m
db = np.sum(dZ) / m</code></pre>

    <h3 class="card-title">Full implementation</h3>

<pre><code class="python">def propagate(w, b, X, Y):

    # Forward prop
    m = X.shape[1]
    Z = np.dot(w.T, X) + b
    A = sigmoid(Z)

    # Compute cost
    cost = -(np.dot(Y, np.log(A.T)) + np.dot((1-Y), np.log(1-A.T))) / m
    cost = np.squeeze(cost)

    # Backward prop
    dZ = A - Y
    dw = np.dot(X, (dZ).T) / m
    db = np.sum(dZ) / m

    grads = {"dw": dw, "db": db}

    return grads, cost</code></pre>

<pre><code class="python">def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):

    for i in range(num_iterations):

        grads, cost = propagate(w, b, X, Y)

        dw = grads["dw"]
        db = grads["db"]

        w = w - learning_rate * dw
        b = b - learning_rate * db

    params = {"w": w, "b": b}
    grads = {"dw": dw, "db": db}

    return params, grads, costs</code></pre>

<pre><code class="python">def predict(w, b, X):

    m = X.shape[1]
    Y_prediction = np.zeros((1,m))
    w = w.reshape(X.shape[0], 1)

    A = sigmoid(np.dot(w.T, X) + b)

    for i in range(A.shape[1]):

        # Convert probabilities A[0,i] to actual predictions p[0,i]
        if A[0, i] <= 0.5:
            Y_prediction[0, i] = 0
        else:
            Y_prediction[0, i] = 1

    return Y_prediction</code></pre>

<pre><code class="python">def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):

    w, b = initialize_with_zeros(X_train.shape[0])
    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost = False)

    w = parameters["w"]
    b = parameters["b"]

    Y_prediction_test = predict(w, b, X_test)
    Y_prediction_train = predict(w, b, X_train)

    d = {"costs": costs,
         "Y_prediction_test": Y_prediction_test,
         "Y_prediction_train" : Y_prediction_train,
         "w" : w,
         "b" : b,
         "learning_rate" : learning_rate,
         "num_iterations": num_iterations}

    return d</code></pre>

<pre><code class="python"># Load the data (cat/non-cat)
train_set_location = 'data/train_catvnoncat.h5'
test_set_location = 'data/test_catvnoncat.h5'
train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset(train_set_location, test_set_location)

# Reshape the training and test examples such that matrices are flattened into vectors
train_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[0], -1).T
test_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[0], -1).T

# Center and standardize dataset dividing by the maximum value of a pixel channel
train_set_x = train_set_x_flatten / 255
test_set_x = test_set_x_flatten / 255

d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost = True)</code></pre>

    <h3 class="card-title">Helper functions</h3>

<pre><code class="python">def load_dataset(train_set_location, test_set_location):

    train_dataset = h5py.File(train_set_location, "r")
    # These data sets are to be pre-preprocessed (Thus, appended "_orig")
    train_set_x_orig = np.array(train_dataset["train_set_x"][:])  # Train set features
    train_set_y_orig = np.array(train_dataset["train_set_y"][:])  # Train set labels

    test_dataset = h5py.File(test_set_location, "r")
    # These data sets are to be pre-preprocessed (Thus, appended "_orig")
    test_set_x_orig = np.array(test_dataset["test_set_x"][:])  # Test set features
    test_set_y_orig = np.array(test_dataset["test_set_y"][:])  # Test set labels

    classes = np.array(test_dataset["list_classes"][:])  # List of classes

    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))
    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))

    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes</code></pre>

<pre><code class="python">def initialize_with_zeros(dim):

    w = np.zeros((dim, 1))
    b = 0

    return w, b</code></pre>

<pre><code class="python">def sigmoid(z):

    s = 1 / (1 + np.exp(-z))

    return s</code></pre>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://www.coursera.org/specializations/deep-learning">Deep Learning Specialization</a>
  </div>
</div>

<div class="card mb-4" id="logistic-regression-">
  <div class="card-body">
    <h2 class="card-title">Derivatives</h2>

    <h3 class="card-title">Consider m=1 and n=2</h3>
    <ul>
      <li>Let</li>
      <ul>
        <li>\( \hat{y} = a, L(a,y) = -\left( ylog(a) + (1-y)log(1-a) \right) \)</li>
        <li>\( z = w_{1}x_{1} + w_{2}x_{2} + b \)</li>
      </ul>
      <li>\( da = \dfrac{dL}{da} = -\dfrac{y}{a} + \dfrac{1-y}{1-a} \)</li>
      <li>\( dz = \dfrac{dL}{dz} = \dfrac{dL}{da}\dfrac{da}{dz} =  \left(-\dfrac{y}{a} + \dfrac{1-y}{1-a}\right) a(1-a) = a - y \)</li>
      <li>Then</li>
      <ul>
        <li>\( dw_{1} = x_{1}dz, dw_{2} = x_{2}dz \)</li>
        <li>\( db = dz \)</li>
      </ul>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://www.coursera.org/specializations/deep-learning">Deep Learning Specialization</a>
  </div>
</div>

<div class="card mb-4" id="logistic-regression-">
  <div class="card-body">
    <h2 class="card-title">Softmax regression</h2>
    <ul>
      <li>Generalization of logistic regression</li>
      <li>Number of units in the output layer is \( n^{L} = C \) the number of classes</li>
      <li>If \( C = 2 \), applying softmax reduces to logistic regression</li>
    </ul>

    <h3 class="card-title">Activation in the output layer</h3>
    <ul>
      <li>In the output layer \( z^{[L]} = w^{[L]}a^{[L-1]} + b^{[l]} \)</li>
      <li>The activation in the last layer for softmax is \( a^{[L]} = \dfrac{e^{z^{[L]}}}{\sum_{i=1}^{C}t_{i}} \) where \( t = e^{z^{[L]}} \)</li>
    </ul>

    <h3 class="card-title">Loss function</h3>
    <ul>
      <li>\( L(\hat{y},y) = -\displaystyle\sum_{j=1}^{C} y_{j}log\hat{y}_{j} \)</li>
    </ul>

    <h3 class="card-title">Gradient descent</h3>
    <ul>
      <li>\( \textbf{dz} = \hat{\textbf{y}} - \textbf{y} \)</li>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://www.coursera.org/specializations/deep-learning">Deep Learning Specialization</a>
  </div>
</div>

<div class="card mb-4" id="logistic-regression-">
  <div class="card-body">
    <h2 class="card-title">Logistic regression problems</h2>

    <p class="card-text">What is a drawback of introducing more variables to better fit the model?</p>
    <ul>
      <li>Overfitting</li>
    </ul>

    <p class="card-text">What is meant by "odd of success"?</p>
    <ul>
      <li>Ratio betwen probability of success and probability of failure</li>
      <li>\( \dfrac{p}{1-p} \)</li>
    </ul>

    <p class="card-text">What is meant by "interaction"?</p>
    <ul>
      <li>Product of two predictor variables</li>
      <li>Ex. the last term of \( \beta_{0} + \beta_{1}X + \beta_{2}Z + \beta_{3}XZ \)</li>
    </ul>

    <p class="card-text">What is response variable?</p>
    <ul>
      <li>It is the log of the odds of being classified in one of two classes</li>
    </ul>

    <p class="card-text">How is response variable transformed to produce probability distribution?</p>
    <ul>
      <li>Sigmoid function is applied to \( Z \)</li>
    </ul>

    <p class="card-text">What is the relationship between logit and sigmoid function?</p>
    <ul>
      <li>They are inverse of each other</li>
      <li>\( logit(p) = log\left(\dfrac{p}{1-p}\right) \)</li>
      <li>\( p = \dfrac{exp(logit(p))}{1-exp(logit(p))} \)</li>
    </ul>

    <p class="card-text">Consider the following table</p>
    <table>
      <th>
        <td>Tumor eradicated</td>
        <td>Tumor not eradicated</td>
      </th>
      <tr>
        <td>Breast</td>
        <td>560</td>
        <td>260</td>
      </tr>
      <tr>
        <td>Lung</td>
        <td>69</td>
        <td>36</td>
      </tr>
    </table>

    <p class="card-text">What is the explanatory and response variable?</p>
    <ul>
      <li>Explanatory variable - cancer type</li>
      <li>Response variable - tumor eradication</li>
    </ul>

    <p class="card-text">What is the explanatory and response variable?</p>
    <ul>
      <li>Explanatory variable - cancer type</li>
      <li>Response variable - tumor eradication</li>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: Deep Learning Interviews, Shlomo Kashani
  </div>
</div>
<!-- Logistic regression END -->

</div> <!-- /.col-md-12 -->
</div> <!-- /.row -->
</div> <!-- /.container -->

<include src="/footer.html"></include>

</body>

</html>