<!DOCTYPE html>

<html lang="en">

<head>

<!-- Metadata -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="">
<meta name="author" content="Seungmoon Rieh">
<meta name="keywords" content="">

<!-- Title and image -->
<title>Seungmoon Rieh</title>
<link href="/img/seungmoonrieh.jpg" rel="icon">

<!-- CSS -->
<link href="/css/bootstrap.min.css" rel="stylesheet">
<link href="/css/monokai-sublime.css" rel="stylesheet">
<link href="/css/site.css" rel="stylesheet">

<!-- JavaScript -->
<script src="/js/jquery.min.js" type="text/javascript"></script>
<script src="/js/bootstrap.bundle.min.js" type="text/javascript"></script>
<script src="/js/highlight.pack.js" type="text/javascript"></script>
<script src="/js/include_html.js" type="text/javascript"></script>
<script src="/js/mathjax/tex-chtml.js" type="text/javascript"></script>
<script src="/js/site.js" type="text/javascript"></script>

</head>

<body>

<include src="/header.html"></include>

<div class="container">
<div class="row">
<div class="col-md-12">
<h1 class="my-4">Machine Learning</h1>

<!-- Designing machine learning systems BEGIN -->
<div class="card mb-4" id="designing-machine-learning-systems">
  <div class="card-body">
    <h2 class="card-title">Designing machine learning systems</h2>
    <ul class="list-unstyled mb-0">
      <li><a href="#designing-machine-learning-systems-">Data engineering</a></li>
      <li><a href="#designing-machine-learning-systems-">Training data</a></li>
      <li><a href="#designing-machine-learning-systems-">Feature engineering</a></li>
      <li><a href="#designing-machine-learning-systems-">Model development and offline evaluation</a></li>
      <li><a href="#designing-machine-learning-systems-">Model deployment and prediction service</a></li>
      <li><a href="#designing-machine-learning-systems-">Data distribution shifts and monitoring</a></li>
      <li><a href="#designing-machine-learning-systems-">Continual learning and test in production</a></li>
    </ul>
  </div>
</div>

<div class="card mb-4" id="designing-machine-learning-systems-">
  <div class="card-body">
    <h2 class="card-title">Data engineering</h2>
    <ul>
      <li>Data source</li>
      <ul>
        <li>User-generated</li>
        <li>System-generated</li>
        <!-- <li>How data is collected?</li>
        <li>How clean is data?</li>
        <li>Can data source be trusted?</li>
        <li>Is data user-generated or system-generated?</li>
        <li>How often new data comes in?</li>
        <li>Can data be stored in servers or data cannot leave user device?</li>
        <li>Does data need to be tokenized?</li> -->
      </ul>
      <li>Data formats</li>
      <ul>
        <li>JSON (Javascript object notation)</li>
        <ul>
          <li>Human readable, thus takes a lot of space</li>
        </ul>
        <li>CSV (comma separated values)</li>
        <ul>
          <li>Human readable, thus takes a lot of space</li>
          <li>Row major, elements in a row are stored next to each other in memory</li>
          <li>Reading examples is fast</li>
          <li>Writing data is fast</li>
        </ul>
        <li>Parquet</li>
        <ul>
          <li>Binary, thus not human readable, but compact in size</li>
          <li>Column major, elements in a column are stored next to each other in memory</li>
          <li>Reading features is fast</li>
        </ul>
      </ul>
      <li>Data models</li>
      <ul>
        <li>SQL</li>
        <ul>
          <li>Data is stord in data warehouse</li>
          <li>Structure is enforced during write time</li>
          <li>Schema changes require updating all existing data</li>
        </ul>
        <li>NoSQL</li>
        <ul>
          <li>Data is stord in data lake</li>
          <li>Document</li>
          <ul>
            <li>Collection of documents are equivalent to table, documents are equivalent to row</li>
            <li>Read operation still needs to know some structure of the document</li>
            <li>Joins are harder and inefficient</li>
          </ul>
          <li>Graph</li>
        </ul>
      </ul>
      <li>ETL</li>
      <ul>
        <li>Extract</li>
        <ul>
          <li>Extract data from different data sources</li>
          <li>Validate data</li>
          <li>Reject data if applicable</li>
        </ul>
        <li>Transform</li>
        <ul>
          <li>Data is cleansed and transformed into specific format</li>
          <li>De-duplication</li>
          <li>Standardize</li>
        </ul>
        <li>Load</li>
        <ul>
          <li>Transformed data is loaded into target destination</li>
        </ul>
      </ul>
      <li>Data flow</li>
      <ul>
        <li>Data passing through databases</li>
        <ul>
          <li>A writes to DB and B reads from DB</li>
          <li>Both A and B need access to DB</li>
          <li>DB cannot be fast for both read and write</li>
        </ul>
        <li>Data passing through services</li>
        <ul>
          <li>A requests data from B and B responds with data</li>
          <li>Ex. REST, RPC</li>
        </ul>
        <li>Data passing through real-time transport (event bus)</li>
        <ul>
          <li>Pubsub</li>
          <ul>
            <li>Don't care about consumers</li>
            <li>Ex. Kafka</li>
          </ul>
          <li>Message queue</li>
          <ul>
            <li>Has intended consumers</li>
            <li>Ex. RocketMQ, RabbitMQ</li>
          </ul>
        </ul>
      </ul>
      <li>Data processing</li>
      <ul>
        <li>Batch processing</li>
        <ul>
          <li>Compute features that do not change often (static features)</li>
          <li>Ex. Spark</li>
        </ul>
        <li>Streaming processing</li>
        <ul>
          <li>Compute features that change frequently (dynamic features)</li>
          <li>Ex. Flink</li>
        </ul>
      </ul>
      <!-- <li>Data types</li>
      <ul>
        <li>Structured</li>
        <ul>
          <li>Numerical</li>
          <ul>
            <li>Discrete</li>
            <li>Continuous</li>
          </ul>
          <li>Categorical</li>
          <ul>
            <li>Ordinal - data with sequential order (Ex. movie rating)</li>
            <li>Nominal - no numerical relationship between categories (Ex. male and female)</li>
          </ul>
        </ul>
        <li>Unstructured</li>
        <ul>
          <li>Audio</li>
          <li>Video</li>
          <li>Image</li>
          <li>Text</li>
        </ul>
      </ul> -->
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: Designing Machine Learning Systems, Chip Huyen
  </div>
</div>

<div class="card mb-4" id="designing-machine-learning-systems-">
  <div class="card-body">
    <h2 class="card-title">Training data</h2>
    <ul>
      <li>Sampling</li>
      <ul>
        <li>Non-probability sampling</li>
        <ul>
          <li>Convenience sampling - samples are selected based on their availability</li>
          <li>Snowball sampling - samples are selected based on existing samples</li>
        </ul>
        <li>Stratified sampling</li>
        <ul>
          <li>Divide population into groups and sample from each group separately</li>
          <li>Each group is called stratum</li>
          <li>Problem is when one sample belongs to multiple groups (Ex. multilabel tasks)</li>
        </ul>
        <li>Weighted sampling</li>
        <ul>
          <li>If there are three samples and want them to be selected with probabilties 50%, 30%, 20%, give them weights \( 0.5, 0.3, 0.2 \)</li>
        </ul>
        <li>Reservior sampling</li>
        <ul>
          <li>Useful for streaming data</li>
          <li>Put the first k elements into the reservior</li>
          <li>For each incoming \( n^{\text{th}} \) element, generate a random number \( i \) such that \( 1 \le i \le n \)</li>
          <ul>
            <li>If \( 1 \le i \le k \), replace \( i^{\text{th}} \) element in the reservoir with \( n^{\text{th}} \) element</li>
            <li>Else, do nothing</li>
          </ul>
          <li>Then, each incoming \( n^{\text{th}} \) element has \( \frac{k}{n} \) probability of being in the reservoir</li>
        </ul>
        <li>Importance sampling</li>
        <ul>
          <li>Assume we want to sample from \( P(x) \) but \( P(x) \) is really expensive to sample from</li>
          <li>The, sample from \( Q(x) \) instead and weigh this sample by \( \frac{P(x)}{Q(x)} \)</li>
        </ul>
      </ul>
      <li>Labeling</li>
      <ul>
        <li>Hand labeling</li>
        <ul>
          <li>Expensive, slow, data privary issue, introduce bias, require domain knowledge</li>
        </ul>
        <li>Natural labeling</li>
        <ul>
          <li>Ground truth labels are inferred</li>
        </ul>
        <li>Handling insufficient lables</li>
        <ul>
          <li>Weak supervision</li>
          <ul>
            <li>Use heuristics to label data</li>
            <li>Labeled data is noisy</li>
          </ul>
          <li>Semi-supervision</li>
          <ul>
            <li>Use structural assumptions to generate new labels based on initial labels</li>
            <li>Purturbation-based method</li>
            <ul>
              <li>Assumption is that small purturbations to a sample should not change its label</li>
              <li>Purturbation can be directly applied to the samples</li>
              <ul>
                <li>Ex. adding white noise to images</li>
              </ul>
              <li>Purturbation can be applied to the representation of samples</li>
              <ul>
                <li>Ex. adding small random values to embeddings of words</li>
              </ul>
            </ul>
          </ul>
          <li>Transfer learning</li>
          <ul>
            <li>Ex. language model</li>
            <ul>
              <li>Does not require labeled data and can be trained on any text</li>
              <li>Given a sequence of tokens, predict the next token</li>
              <ul>
                <li>Ex. "I bought NVIDIA shares because I believe in the importance of"</li>
                <li>Language model might output "hardware" or "GPU" as the next token</li>
              </ul>
              <li>The trained model can be used for downstream tasks</li>
              <ul>
                <li>Ex. sentiment analysis, intent detection, question answering</li>
              </ul>
            </ul>
          </ul>
          <li>Active learning</li>
          <ul>
            <li>Model chooses which data samples to learn from</li>
          </ul>
        </ul>
      </ul>
      <li>Class imbalance</li>
      <ul>
        <li>Evaluation metric</li>
        <ul>
          <li>Accuracy treat all class equally, thus performance of majority class will dominate the metric</li>
          <li>F1, precision, recall measure model performance w.r.t. positive class</li>
        </ul>
        <li>Resampling</li>
        <ul>
          <li>Oversampling - overfit on training data</li>
          <li>Undersampling - lose important data</li>
          <li>Never evaluate model on resampled data</li>
        </ul>
        <li>Algorithm</li>
        <ul>
          <li>Cost-sensitive learning</li>
          <li>Class-balanced loss</li>
          <li>Focal loss</li>
        </ul>
      </ul>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: Designing Machine Learning Systems, Chip Huyen
  </div>
</div>

<div class="card mb-4" id="designing-machine-learning-systems-">
  <div class="card-body">
    <h2 class="card-title">Feature engineering</h2>
    <ul>
      <li>Missing values</li>
      <ul>
        <li>Types</li>
        <ul>
          <li>Missing not at random</li>
          <ul>
            <li>Missing due to true value itself</li>
            <li>Ex. respondants not disclosing their income and it turns out that those who don't disclose tend to have higher income</li>
          </ul>
          <li>Missing at random</li>
          <ul>
            <li>Missing due to another observed variable</li>
            <li>Ex. age value of centain gender is missing because that gender tend not to disclose their age</li>
          </ul>
          <li>Missing completely at random</li>
          <ul>
            <li>There is no pattern in which the value is missing</li>
          </ul>
        </ul>
        <li>Handle missing values</li>
        <ul>
          <li>Delete - data quantity is reduced</li>
          <ul>
            <li>Delete row to remove a data point - only small portion, for example less than 0.1%, should be removed</li>
            <li>Delete column to remove a feature - important information may be removed, reducing model accuracy</li>
          </ul>
          <li>Imputation - dataset gets noisy</li>
          <ul>
            <li>Fill with default value, mean, median, mode</li>
          </ul>
        </ul>
      </ul>
      <li>Feature scaling</li>
      <ul>
        <li>Models struggle with features that has skewed distribution</li>
        <li>Log transformation - apply log function to features</li>
        <!-- <li>Feature scaling is not needed if using XGBoost</li>
        <li>Helps gradient descent to find the optimum faster</li>
        <li>Scale inputs to [-1,1]</li>
        <ul>
          <li>Make error function more spherical, thus gradient descent converges faster</li>
          <li>Not scaling inputs impacts regularization</li>
          <li>Outliers are also valid inputs. Do not throw them away</li>
        </ul>
        <li>Normalization (min-max scaling)</li>
        <ul>
          <li>Does not change distribution</li>
          <li>All values are \( [0,1] \)</li>
          <li>\( z = \dfrac{x-x_{min}}{x_{max}-x_{min}} \)</li>
          <li>Outliers can make real data shrunk in very narrow range</li>
        </ul>
        <li>Clipping</li>
        <ul>
          <li>Treat outliers as -1 or 1</li>
          <li>Numerical values are linearly scaled</li>
          <li>Works for uniformly distributed data</li>
        </ul>
        <li>Standardization (z-score normalization)</li>
        <ul>
          <li>Mean is \( 0 \) and standard deviation is \( 1 \)</li>
          <li>\( z = \dfrac{x-\mu}{\sigma} \)</li>
          <li>Works for normally distributed data</li>
        </ul>
        <li>Log scaling</li>
        <ul>
          <li>Mitigate skewness of a feature, so that gradient descent converges faster</li>
          <li>\( z = log(x) \)</li>
          <li>Used when data is neither uniformly or normally distributed</li>
        </ul> -->
      </ul>
      <li>Bucketing</li>
      <ul>
        <li>Convert numerical feature to categorical feature</li>
        <li>Rarely helps in production</li>
      </ul>
      <li>Encoding</li>
      <ul>
        <li>Use hash function to generate hashed value of each category</li>
        <li>Locality-sensitive hashing - similar categories are hashed into values close to each other</li>
        <!-- <li>Convert categorical features to numerical feature</li>
        <li>Ex. integer encoding</li>
        <ul>
          <li>Integer value is assigned to each category</li>
          <li>Cannot be used for nominal features</li>
        </ul>
        <li>Ex. one-hot encoding</li>
        <ul>
          <li>Binary value is assigned to each category</li>
          <li>Not suitable for features with high cardinality</li>
          <li>Not suitable when features values are not independent</li>
        </ul>
        <li>Ex. embedding</li>
        <ul>
          <li>Learn N-D vector for each categorial value</li>
          <li>Just another hidden layer in neural network</li>
          <li>When determining embedding dimension, hyperparameter tune between these two variables</li>
          <ul>
            <li>Fourth root of the total number of unique categorical elements</li>
            <li>1.6 times the square root of the number of unique elements in the category, no less than 600</li>
          </ul>
        </ul> -->
      </ul>
      <li>Watch for data leakage</li>
      <ul>
        <li>Splitting time-correlated data randomly instead of by time</li>
        <ul>
          <li>Should always train on data from \( 0 \) to time \( t \) and evaluate it on \( t+1 \)</li>
          <li>If random split, information from future is leaked into training process</li>
        </ul>
        <li>Scaling before splitting</li>
        <ul>
          <li>Do not use entire training data to generate global statistics before splitting into bins</li>
          <li>If not, it leaks the mean and variance of test set into training process</li>
          <li>Always split data first, then apply scaling</li>
        </ul>
        <li>Filling in missing data with statistics from the test split</li>
        <ul>
          <li>Leaking occurs when mean or median is calculated using entire data instead of just the train split</li>
        </ul>
        <li>Poor handling of data duplication before splitting</li>
        <ul>
          <li>Same samples might appear in both train and validation/test set</li>
          <li>If oversampling data, do it after splitting</li>
        </ul>
        <li>Group leakage</li>
        <ul>
          <li>Ex. CT scans that are a week apart with the same lables, one in train set and the other in test set</li>
        </ul>
      </ul>
      <li>Detect data leakage</li>
      <ul>
        <li>Measure the predictive power of each feature (or a set of features) on target variable</li>
        <li>If a feature has high correlation, investigate</li>
      </ul>
      <li>Too many features</li>
      <ul>
        <li>Increased risk of data leakage</li>
        <li>Can cause overfitting</li>
        <li>Increases memory requirement</li>
        <li>Increases inference latency, especially if prediction requires extracting features</li>
        <li>Useless features become techinical debt</li>
        <ul>
          <li>When data pipeline changes, all affected features need to adjust</li>
          <li>In theory, regularization should reduce weight of useless features to 0. However, model learns faster without useless features</li>
        </ul>
      </ul>
      <li>Feature importance</li>
      <ul>
        <li>SHAP</li>
      </ul>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: Designing Machine Learning Systems, Chip Huyen
  </div>
</div>

<div class="card mb-4" id="designing-machine-learning-systems-">
  <div class="card-body">
    <h2 class="card-title">Model development and offline evaluation</h2>
    <ul>
      <li>Ensemble</li>
      <ul>
        <li>Bagging</li>
        <ul>
          <li>Create datasets (via sample with replacement)</li>
          <li>Train model on each of these datasets</li>
          <li>If classification, final prediction is majority vote of all models</li>
          <li>If regression, final prediction is average of all model predictions</li>
          <li>Ex. random forest</li>
        </ul>
        <li>Boosting</li>
        <ul>
          <li>Train first classifier on original dataset</li>
          <li>Misclassified samples are given higher weights</li>
          <li>Train second classifier, and repeat</li>
          <li>Final classifer is weighted combo of all classifiers (better classifer gets higher weight)</li>
          <li>Ex. gradient boosting machine (GBM), XGBoost, LightGBM</li>
        </ul>
      </ul>
      <li>Versioning</li>
      <li>Distributed training</li>
      <ul>
        <li>Data parallelism</li>
        <li>Model parallelism</li>
      </ul>
      <li>AutoML</li>
      <ul>
        <li>Hyperparameter tuning</li>
        <li>Architecture search and learned optimizer</li>
      </ul>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: Designing Machine Learning Systems, Chip Huyen
  </div>
</div>

<div class="card mb-4" id="designing-machine-learning-systems-">
  <div class="card-body">
    <h2 class="card-title">Model deployment and prediction service</h2>
    <ul>
      <li>Prediction pipeline</li>
      <ul>
        <li>Batch prediction</li>
        <ul>
          <li>Only uses batch features</li>
          <li>Less responsive to change in user preference</li>
          <li>Need to know beforehand what needs to be pre-computed</li>
        </ul>
        <li>Online prediction</li>
        <ul>
          <li>May use batch features only or the combination of batch and streaming features</li>
          <li>Model may take long to generate prediction</li>
          <li>Stateless serving function</li>
          <ul>
            <li>Should support millions of requests per second</li>
            <li>Model should be packaged and deployed as stateless function</li>
          </ul>
        </ul>
      </ul>
      <li>Model compression</li>
      <ul>
        <li>Low-rank factorization</li>
        <li>Knowledge distillation</li>
        <li>Pruning</li>
        <li>Quantization</li>
      </ul>
      <li>Edge computing</li>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: Designing Machine Learning Systems, Chip Huyen
  </div>
</div>

<div class="card mb-4" id="designing-machine-learning-systems-">
  <div class="card-body">
    <h2 class="card-title">Data distribution shifts and monitoring</h2>
    <ul>
      <li>Data distribution shift</li>
      <ul>
        <li>Types</li>
        <ul>
          <li>Covariance shift</li>
          <ul>
            <li></li>
          </ul>
          <li>Label shift</li>
          <ul>
            <li></li>
          </ul>
          <li>Concept drift</li>
          <ul>
            <li></li>
          </ul>
        </ul>
        <li>Detecting</li>
        <li>Solution</li>
        <ul>
          <li>Train on large dataset</li>
          <li>Retrain regularly</li>
        </ul>
      </ul>
      <li>Monitoring</li>
      <ul>
        <li>Accuracy</li>
        <ul>
          <li>Model performance</li>
          <li>Ex. accuracy, precision, recall, F1 score</li>
        </ul>
        <li>Predictions</li>
        <ul>
          <li>Distribution drift of prediction</li>
          <li>Ex. prediction PSI</li>
        </ul>
        <li>Features</li>
        <ul>
          <li>Distribution drift of features</li>
          <li>Ex. feature PSI</li>
        </ul>
        <li>Raw inputs</li>
      </ul>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: Designing Machine Learning Systems, Chip Huyen
  </div>
</div>

<div class="card mb-4" id="designing-machine-learning-systems-">
  <div class="card-body">
    <h2 class="card-title">Continual learning and test in production</h2>
    <ul>
      <li>Continual learning</li>
      <ul>
        <li>Stage 1 - manual, training from scratch</li>
        <li>Stage 2 - automated, training from scratch</li>
        <ul>
          <li>Scheduler - Airflow, Argo</li>
          <li>Model store - SageMaker, MLflow</li>
        </ul>
        <li>Stage 3 - automated, incremental learning</li>
        <li>Stage 4 - continual learning</li>
        <ul>
          <li>Data distribution shift</li>
          <li>Model performance degradation</li>
        </ul>
      </ul>
      <li>Deployment</li>
      <ul>
        <li>Shadow deployment</li>
        <li>A/B testing</li>
        <li>Canary release</li>
      </ul>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: Designing Machine Learning Systems, Chip Huyen
  </div>
</div>
<!-- Designing machine learning systems END -->

</div> <!-- /.col-md-12 -->
</div> <!-- /.row -->
</div> <!-- /.container -->

<include src="/footer.html"></include>

</body>

</html>