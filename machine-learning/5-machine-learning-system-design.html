<!DOCTYPE html>

<html lang="en">

<head>

<!-- Metadata -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="">
<meta name="author" content="Seungmoon Rieh">
<meta name="keywords" content="">

<!-- Title and image -->
<title>Seungmoon Rieh</title>
<link href="/img/seungmoonrieh.jpg" rel="icon">

<!-- CSS -->
<link href="/css/bootstrap.min.css" rel="stylesheet">
<link href="/css/monokai-sublime.css" rel="stylesheet">
<link href="/css/site.css" rel="stylesheet">

<!-- JavaScript -->
<script src="/js/jquery.min.js" type="text/javascript"></script>
<script src="/js/bootstrap.bundle.min.js" type="text/javascript"></script>
<script src="/js/highlight.pack.js" type="text/javascript"></script>
<script src="/js/include_html.js" type="text/javascript"></script>
<script src="/js/mathjax.js" type="text/javascript"></script>
<script src="/js/site.js" type="text/javascript"></script>

</head>

<body>

<include src="/header.html"></include>

<div class="container">
<div class="row">
<div class="col-md-12">
<h1 class="my-4">Other</h1>

<!-- Machine learning system design BEGIN -->
<div class="card mb-4" id="machine-learning-system-design">
  <div class="card-body">
    <h2 class="card-title">Machine learning system design</h2>
    <ul class="list-unstyled mb-0">
      <li><a href="#machine-learning-system-design-">Machine learning system design</a></li>
    </ul>
  </div>
</div>

<div class="card mb-4" id="machine-learning-system-design-">
  <div class="card-body">
    <h2 class="card-title">Machine learning system design</h2>

    <h3 class="card-title">Clarify requirements</h3>
    <ul>
      <li>Business objective</li>
      <ul>
        <li>Ex. increase number of bookings</li>
        <li>Ex. increase revenue</li>
      </ul>
      <li>Features that the system supports</li>
      <ul>
        <li>Ex. whether users can like/dislike videos</li>
      </ul>
      <li>Data</li>
      <ul>
        <li>What are data source?</li>
        <li>How large is dataset?</li>
        <li>Is data labeled?</li>
      </ul>
      <li>Constraints</li>
      <ul>
        <li>Available computing power</li>
        <li>Mobile device vs cloud</li>
      </ul>
      <li>Scale</li>
      <ul>
        <li>How many users?</li>
      </ul>
      <li>Performance</li>
      <ul>
        <li>How fast should prediction be?</li>
        <li>Batch vs real-time</li>
        <li>Accuracy vs latency</li>
      </ul>
    </ul>

    <h3 class="card-title">Framing problem as ML task</h3>
    <ul>
      <li>Define ML objective</li>
      <ul>
        <li>Ex. increase ticket sales -> maximize number of event registration</li>
        <li>Ex. increase user engagement -> maximize time users spend watching videos</li>
        <li>Ex. increase user clicks -> maximize click-through rate</li>
        <li>Ex. improve platform's safety -> predict if content is harmful</li>
        <li>Ex. increase user network growth -> maximize number of formed connections</li>
      </ul>
      <li>Specify input and output</li>
      <ul>
        <li>Ex. input - post, output - whether post is harmful or not</li>
      </ul>
      <li>Choose right ML category</li>
      <ul>
        <li>Supervised learning - learn from training dataset</li>
        <ul>
          <li>Classification - predict discrete class label</li>
          <ul>
            <li>Binary</li>
            <li>Multiclass</li>
          </ul>
          <li>Regression - predict continuous numeric value</li>
        </ul>
        <li>Unsupervised learning - learn pattern among data</li>
        <ul>
          <li>Clustering</li>
          <li>Association</li>
          <li>Dimension reduction</li>
        </ul>
        <li>Reinforcement learning - learn from repeated trial-and-error</li>
      </ul>
    </ul>

    <h3 class="card-title">Data preparation</h3>
    <ul>
      <li>Data engineering</li>
      <ul>
        <li>Data source</li>
        <ul>
          <li>Who collceted data?</li>
          <li>How clean is data?</li>
          <li>Can data source be trusted?</li>
          <li>Is data user-generated or system-generated?</li>
        </ul>
        <li>Data storage (DB)</li>
        <ul>
          <li>SQL</li>
          <ul>
            <li>Relational</li>
            <ul>
              <li>MySQL</li>
              <li>PostgreSQL</li>
            </ul>
          </ul>
          <li>NoSQL</li>
          <ul>
            <li>Key-value</li>
            <ul>
              <li>Redis</li>
              <li>DynamoDB</li>
            </ul>
            <li>Column-based</li>
            <ul>
              <li>Cassandra</li>
              <li>HBase</li>
            </ul>
            <li>Graph</li>
            <ul>
              <li>Neo4J</li>
            </ul>
            <li>Document</li>
            <ul>
              <li>MongoDB</li>
              <li>CouchDB</li>
            </ul>
          </ul>
        </ul>
        <li>ETL</li>
        <ul>
          <li>Extract - extract data from different data sources</li>
          <li>Transform - data is cleansed and transformed into specific format</li>
          <li>Load - transformed data is loaded into target destination</li>
        </ul>
        <li>Data types</li>
        <ul>
          <li>Structured</li>
          <ul>
            <li>Numerical</li>
            <ul>
              <li>Discrete</li>
              <li>Continuous</li>
            </ul>
            <li>Categorical</li>
            <ul>
              <li>Ordinal - data with sequential order (Ex. movie rating)</li>
              <li>Nominal - no numerical relationship between categories (Ex. male and female)</li>
            </ul>
          </ul>
          <li>Unstructured</li>
          <ul>
            <li>Audio</li>
            <li>Video</li>
            <li>Image</li>
            <li>Text</li>
          </ul>
        </ul>
      </ul>
      <li>Feature engineering</li>
      <ul>
        <li>Handle missing values</li>
        <ul>
          <li>Delete - data quantity is reduced</li>
          <ul>
            <li>Delete row to remove a data point</li>
            <li>Delete column to remove a feature</li>
          </ul>
          <li>Imputation - dataset gets noisy</li>
          <ul>
            <li>Fill with default value</li>
            <li>Fill with mean, media, mode</li>
          </ul>
        </ul>
        <li>Feature scaling</li>
        <ul>
          <li>Normalization (min-max scaling)</li>
          <ul>
            <li>Does not change distribution</li>
            <li>All values are \( [0,1] \)</li>
            <li>\( z = \dfrac{x-x_{min}}{x_{max}-x_{min}} \)</li>
          </ul>
          <li>Standardization (z-score normalization)</li>
          <ul>
            <li>Mean is \( 0 \) and standard deviation is \( 1 \)</li>
            <li>\( z = \dfrac{x-\mu}{\sigma} \)</li>
          </ul>
          <li>Log scaling</li>
          <ul>
            <li>Mitigate skewness of a feature, so that gradient descent converges faster</li>
            <li>\( z = log(x) \)</li>
          </ul>
        </ul>
        <li>Bucketing</li>
        <ul>
          <li>Convert numerical feature to categorical feature</li>
        </ul>
        <li>Encodeing - categorical features converted to numerical</li>
        <ul>
          <li>Integer encoding</li>
          <ul>
            <li>Integer value is assigned to each category</li>
            <li>Cannot be used for nominal features</li>
          </ul>
          <li>One-hot encoding</li>
          <ul>
            <li>Binary value is assigned to each category</li>
          </ul>
          <li>Embedding</li>
          <ul>
            <li>Learn N-D vector for each categorial value</li>
          </ul>
        </ul>
      </ul>
    </ul>

    <h3 class="card-title">Model development</h3>
    <ul>
      <li>Model selection</li>
      <ul>
        <li>Establish baseline</li>
        <ul>
          <li>Recommend most popular video</li>
        </ul>
        <li>Experiment with simple models</li>
        <ul>
          <li>Logistic regression</li>
        </ul>
        <li>Try complex models</li>
        <ul>
          <li>Deep neural network</li>
        </ul>
        <li>Ensemble if needed</li>
        <ul>
          <li>Bagging, boosting, stacking</li>
        </ul>
        <li>Model option</li>
        <ul>
          <li>Logistic regression</li>
          <li>Linear regression</li>
          <li>Decision trees</li>
          <li>Gradient boosted decision trees and random forests</li>
          <li>Support vector machine</li>
          <li>Naive bayes</li>
          <li>Factorization machine (FM)</li>
        </ul>
        <li>Model consideration</li>
        <ul>
          <li>Amount of data</li>
          <li>Training speed</li>
          <li>Number of parameters and memory requirement</li>
          <li>Hyperparameters and how to tune them</li>
          <li>Continual learning requirement</li>
          <li>Compute requirement</li>
          <li>Latecy during inference</li>
          <li>Model interpretability</li>
        </ul>
      </ul>
      <li>Model training</li>
      <ul>
        <li>Construct dataset</li>
        <ul>
          <li>Collect raw data</li>
          <li>Identify features and labels</li>
          <ul>
            <li>Hand labeling - expensive, slow, data privary issue, introduce bias, require domain knowledge</li>
            <li>Natural labeling - ground truth labels are inferred</li>
          </ul>
          <li>Select sampling strategy</li>
          <ul>
            <li>Convenience sampling</li>
            <li>Snowball sampling</li>
            <li>Stratified sampling</li>
            <li>Reservior sampling</li>
            <li>Importance sampling</li>
          </ul>
          <li>Split data</li>
          <li>Address imbalance</li>
          <ul>
            <li>Resample training data - oversample under-represented class or undersample over-represented class</li>
            <li>Alter loss function - give more weights to data points from minority class</li>
            <ul>
              <li>Class-balanced loss</li>
              <li>Focal loss</li>
            </ul>
          </ul>
        </ul>
        <li>Choose loss function</li>
        <ul>
          <li>Cross-entropy</li>
          <li>MSE</li>
          <li>MAE</li>
          <li>Huber loss</li>
        </ul>
        <li>Regularization</li>
        <ul>
          <li>L1</li>
          <li>L2</li>
          <li>Entropy regularization</li>
          <li>K-fold CV</li>
          <li>Dropout</li>
        </ul>
        <li>Optimization</li>
        <ul>
          <li>SGD</li>
          <li>AdaGrad</li>
          <li>Momentum</li>
          <li>RMSProp</li>
        </ul>
        <li>Activation</li>
        <ul>
          <li>ELU</li>
          <li>ReLU</li>
          <li>Tanh</li>
          <li>Sigmoid</li>
        </ul>
        <li>Train from scratch vs fine-tuning</li>
        <li>Distributed training</li>
        <ul>
          <li>Data parallelism</li>
          <li>Model parallelism</li>
        </ul>
      </ul>
    </ul>

    <h3 class="card-title">Evaluation</h3>
    <ul>
      <li>Offline - during development</li>
      <ul>
        <li>Classification - precision, recall, F1 score, accuracy, ROC-AUC, PR-AUC, confusion matrix</li>
        <li>Regression - MSE, MAE, RMSE</li>
        <li>Ranking - precision@k, recall@k, MRR, mAP, nDCG</li>
        <li>Image generation - FID, inception score</li>
        <li>Natural language processing - BLUE, METEOR, ROUGE, CIDEr, SPICE</li>
      </ul>
      <li>Online - in production</li>
      <ul>
        <li>Ad click prediction - click-through rate, revenue lift</li>
        <li>Harmful content detection - prevalence, valid appeals</li>
        <li>Video recommendation - click-through rate, total watch time, number of completed videos</li>
        <li>Friend recommendation - number of requests sent per day, number of requests accepted per day</li>
      </ul>
    </ul>

    <h3 class="card-title">Deployment and serving</h3>
    <ul>
      <li>Cloud vs on-device</li>
      <ul>
        <li>Cloud - simple to deploy, faster inference, fewer constraints</li>
        <li>On-device - no cloud cost, no network latency, more privacy, no internet required</li>
      </ul>
      <li>Model compression</li>
      <ul>
        <li>Knowledge distillation - train small model to mimic larger model</li>
        <li>Pruning - find least useful parameters and set them to zero</li>
        <li>Quantization - use fewer bits to represent parameters</li>
      </ul>
      <li>Test in production</li>
      <ul>
        <li>Shadow deployment</li>
        <ul>
          <li>Deploy new model in parallel with existing model</li>
          <li>Inference from existing model is served to users</li>
          <li>Double number of prediction is needed</li>
        </ul>
        <li>A/B testing</li>
        <ul>
          <li>Deploy new model in parallel with existing model</li>
          <li>Portion of traffic is routed to new model</li>
        </ul>
      </ul>
      <li>Prediction pipeline</li>
      <ul>
        <li>Batch prediction</li>
        <ul>
          <li>Less responsive to change in user preference</li>
          <li>Need to know beforehand what needs to be pre-computed</li>
        </ul>
        <li>Online prediction</li>
        <ul>
          <li>Model may take long to generate prediction</li>
        </ul>
      </ul>
    </ul>

    <h3 class="card-title">Monitoring and infrastructure</h3>
    <ul>
      <li>Data distribution shift</li>
      <ul>
        <li>Train on large dataset</li>
        <li>Retrain regularly</li>
      </ul>
    </ul>

    <!-- <h3 class="card-title">High level design</h3>
    <img class="img-fluid" class="card-img-top" src="/img/system-design/ml-highlevel.png" alt="Card image cap">

    <h3 class="card-title">Component design</h3>
    <img class="img-fluid" class="card-img-top" src="/img/system-design/ml-development-v1.png" alt="Card image cap">
    <img class="img-fluid" class="card-img-top" src="/img/system-design/ml-training-v1.png" alt="Card image cap">
    <img class="img-fluid" class="card-img-top" src="/img/system-design/ml-inference-batch-v1.png" alt="Card image cap">
    <ul>
      <li>Performce for out-of-sample and out-of-time.</li>
      <li>Features should be mostly unchanged when input data is slighly perturbed. (Could be challgenging when data contains similar/duplicate features)</li>
      <li>Interpretation should not be depending on training sample choices.</li>
    </ul>

    <h3 class="card-title">Problem definition</h3>
    <ul>
      <li>Construct ground truth.</li>
      <li>Metrics.</li>
    </ul>

    <h3 class="card-title">Data prep</h3>
    <ul>
      <li>Proportion of null/zero values across columns.</li>
      <li>Columns that should be rejected due to zero or null values.</li>
      <li>Categorical columns with high cardinality.</li>
      <li>Columns with high sparcity.</li>
      <li>Duplicate columns.</li>
      <li>Compute per-month count of rows and selected attribute to see if they make sense.</li>
      <li>Verify that join keys are consistent across tables.</li>
      <li>Verify that labels/values to apply exclusions are available in the tables.</li>
      <li>Verify that labels/values to apply ground truth are available in the tables.</li>
      <li>Rectify missing values in columns.</li>
      <ul>
      <li>Random missing value - value is missing for unexplained reason.</li>
      <li>Structural missing value - value is missing for a reason. (For example, the fact that it is missing means something)</li>
      <li>Engineered missing value - feature engineering introduced the missing values. (For example, divide by zero)</li>
      <li>Table join mismatch - IDs in one table were missing in another table, so missing values are created after join.</li>
    </ul>
    </ul>

    <h3 class="card-title">Data profiling</h3>
    <ul>
      <li>General statistics.</li>
      <li>Columns with large portion of missing values. (For example, > 10%)</li>
      <ul>
        <li>These columns should generally be retained for they may turn out to be quite predictive.</li>
        <li>XGBoost has build-in capability to handle missing values.</li>
      </ul>
      <li>Similar/duplicate columns. (For example, prefer one feature over another? Can use univariate analysis)</li>
      <li>Columns with consistant values should be removed because they provide no signal.</li>
      <li>Remove sensitive features from model inputs. (For example, age / gender)</li>
      <li>Data imputation (For example, NULL to 0) should rarely happens when feature is critical and will undergo feature engineering and treatment is known.</li>
    </ul>

    <h3 class="card-title">Exclusion</h3>
    <ul>
      <li>Exclude rows based on certain attributes. (This is driven by business reasons)</li>
    </ul>

    <h3 class="card-title">Ground truth construction</h3>

    <h3 class="card-title">Data split</h3>
    <ul>
      <li>Training: In-Time and In-Sample. Split into 5-folds for cross-validation.</li>
      <li>Testing: Out-of-Time test and In-Time & Out-of-Sample test.</li>
      <li>If a data about person is seen at many different dates, all data about that person must be assigned to the same split. (either Out-of-Sample or one of the folds) Otherwise, there is information leak bwetween training and validation.</li>
      <li>Final model is retrained with all 5 folds combined.</li>
    </ul>

    <h3 class="card-title">Data representativeness</h3>
    <ul>
      <li>Separate datasets by ground truth labels.</li>
      <li>Perform univariate analysis. For example, look at the distribution of samples of a key feature for both the entire dataset and the partitioned dataset. Repeat for all key features.</li>
      <li>All paritioned datasets must include sufficient volume of each ground truth labels across key features.</li>
      <li>Bin size may be increased or bins could be combined as a result of above analysis.</li>
    </ul>

    <h3 class="card-title">Model development</h3>
    <ul>
      <li>Define metric to evaluate the model.</li>
      <li>Score the model by the desired metric.</li>
      <li>Check model performace by month to check seasonality.</li>
      <li>Kubernetes based</li>
      <ul>
        <li>Make changes to code.</li>
        <li>Build, test, package the code.</li>
        <li>Build an image which includes the code package.</li>
        <li>Deploy the image to Kubenetes using tools like Skaffold / Helm.</li>
        <li>Run commands inside the image to execute the code.</li>
      </ul>
      <li>Databricks based</li>
      <ul>
        <li>Make changes to code.</li>
        <li>Build, test, package, publish the code.</li>
        <li>Import the code from the notebook.</li>
        <li>Write additional code on the notebook as needed.</li>
        <li>Run commands on the notebook execute the code.</li>
      </ul>
    </ul>

    <h3 class="card-title">Feature selection</h3>
    <ul>
      <li>Start with initial set of feature ~ 1500</li>
      <li>First gate</li>
      <ul>
        <li>Data preparation</li>
        <li>Feature engineering</li>
      </ul>
      <li>After exclusions ~ 1000</li>
      <li>First stage</li>
      <ul>
        <li>Assess each feature individually.</li>
        <li>Select a metric.</li>
        <li>Use 4-folds for training and 1 fold for validation.</li>
        <li>Train a shallow model with a single feature as input and compute performance in validation set.</li>
        <li>Rank individual features.</li>
        <li>Then, train a model wtih all features.</li>
        <li>Then again, rank the features.</li>
      </ul>
      <ul>
        <li>Perform recursive feature search.</li>
        <li>Use 4-folds for training and 1 fold for validation. (Validation fold must be different from previous step)</li>
        <li>For each feature in top N features, train a model and score performance on validation set.</li>
        <li>Add the best scoring features to the candidate features.</li>
      </ul>
      <li>Candidate features ~ 100</li>
      <li>Second gate</li>
      <ul>
        <li>Highly correlated features should be justified or redundant features should be removed.</li>
        <li>Future retraining starts from the candidate features.</li>
      </ul>
      <li>Second stage</li>
      <ul>
        <li>Perform 5-fold cross validation with each candidate feature.</li>
        <li>Compute shapley values on the validation set.</li>
        <li>Select stable features via union of top features across the folds.</li>
      </ul>
      <li>Stable features ~ 40</li>
    </ul>

    <h3 class="card-title">Hyperparameter tuning</h3>
    <ul>
      <li>Ex. grid search, random search, bayesian optimization.</li>
    </ul>

    <h3 class="card-title">Final model training</h3>
    <ul>
      <li>Train the model using stable features and tuned hyperparameters.</li>
      <li>Evaluate on OOS (seasonality) and OOT (final evaluation) test sets.</li>
    </ul>

    <h3 class="card-title">Monitoring</h3>
    <ul>
      <li>Partial dependency plot (PDP) assesses marginal effect of a feature to the model output.</li>
      <li>Indvidual conditional expectation (ICDE) is equivalent to PDP but for an individual data point.</li>
      <li>Feature contribution is assessed via shapley values.</li>
    </ul>
  </div> -->
  <div class="card-footer text-muted">
    Reference: Machine Learning System Design Interview, Ali Aminian & Alex Xu
  </div>
</div>
<!-- Machine learning system design END -->

</div> <!-- /.col-md-12 -->
</div> <!-- /.row -->
</div> <!-- /.container -->

<include src="/footer.html"></include>

</body>

</html>