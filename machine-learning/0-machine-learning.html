<!DOCTYPE html>

<html lang="en">

<head>

<!-- Metadata -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="">
<meta name="author" content="Seungmoon Rieh">
<meta name="keywords" content="">

<!-- Title and image -->
<title>Seungmoon Rieh</title>
<link href="/img/seungmoonrieh.jpg" rel="icon">

<!-- CSS -->
<link href="/css/bootstrap.min.css" rel="stylesheet">
<link href="/css/monokai-sublime.css" rel="stylesheet">
<link href="/css/site.css" rel="stylesheet">

<!-- JavaScript -->
<script src="/js/jquery.min.js" type="text/javascript"></script>
<script src="/js/bootstrap.bundle.min.js" type="text/javascript"></script>
<script src="/js/highlight.pack.js" type="text/javascript"></script>
<script src="/js/include_html.js" type="text/javascript"></script>
<script src="/js/mathjax/tex-chtml.js" type="text/javascript"></script>
<script src="/js/site.js" type="text/javascript"></script>

</head>

<body>

<include src="/header.html"></include>

<div class="container">
<div class="row">
<div class="col-md-12">
<h1 class="my-4">Machine Learning</h1>

<!-- Machine Learning BEGIN -->
<div class="card mb-4" id="subject">
  <div class="card-body">
    <h2 class="card-title">Machine Learning</h2>
    <!-- <ul class="list-unstyled mb-0"> -->
    <ul>
      <li>Structure</li>
      <ul>
        <li>Fundamental - think about question for 5 seconds. Then, answer</li>
        <li>Coding - read question for 5 mins. Then, start coding</li>
      </ul>
      <li>ML Breath - Common <b>(in-scope)</b></li>
      <ul>
        <li>Fundamental</li>
        <ul>
          <li><a href="#machine-learning-">Math and Statistics</a></li>
          <li><a href="#machine-learning-">Supervised and Unsupervised Learning</a></li>
          <li><a href="#machine-learning-">Feature Engineering</a></li>
          <li><a href="#machine-learning-">Bias and Variance</a></li>
          <li><a href="#machine-learning-">Ensemble</a></li>
          <li><a href="#machine-learning-">Neural Network</a></li>
        </ul>
        <li>Coding</li>
        <ul>
          <li><a href="/machine-learning/1-linear-regression.html">Linear Regression</a></li>
          <li><a href="/machine-learning/1-logistic-regression.html">Logistic Regression</a></li>
          <li><a href="/machine-learning/1-nearest-neighbor.html">K-nearest Neighbor</a></li>
          <li><a href="/machine-learning/1-clustering.html">K-means clustering</a></li>
          <li><a href="/machine-learning/1-neural-network.html">Neural Networks</a></li>
        </ul>
      </ul>
      <li>ML Breath - Specialized</li>
      <ul>
        <li>Fundamental</li>
        <ul>
          <li><a href="#machine-learning-">Time-series and sequential data</a>  (out-of-scope)</li>
          <li><a href="#machine-learning-">Natural language processing</a> <b>(in-scope)</b></li>
          <li><a href="#machine-learning-">Computer vision</a>  (out-of-scope)</li>
          <li><a href="#machine-learning-">Reinforcement learning</a>  (out-of-scope)</li>
        </ul>
        <li>Coding</li>
        <ul>
          <li><a href="/machine-learning/2-transformer.html">Transformer</a> <b>(in-scope)</b></li>
        </ul>
      </ul>
      <li>ML Depth - Platform Engineering (out-of-scope)</li>
      <ul>
        <li><a href="#machine-learning-">Data Engineering</a></li>
        <li><a href="#machine-learning-">Model Serving - Discriminative</a></li>
        <li><a href="#machine-learning-">Model Serving - Generative</a></li>
        <li><a href="#machine-learning-">Non-functional Requirement</a></li>
        <li><a href="#machine-learning-">Deployment</a></li>
        <li><a href="#machine-learning-">Distributed System</a></li>
        <li><a href="#machine-learning-">Recommendation System</a></li>
      </ul>
    </ul>
  </div>
</div>

<div class="card mb-4" id="machine-learning-">
  <div class="card-body">
    <h2 class="card-title">Math and Statistics</h2>
    <ul>
      <li>Explain central limit theorem</li>
      <ul>
        <li>If take large enough sample from population, distribution will be normally</li>
        <li>We can use tests that assumed normality even when raw data is skewed</li>
      </ul>
      <li>Explain hypothesis testing and p-value</li>
      <ul>
        <li>Null hypothesis - there is no relationship</li>
        <li>Alternative hypothesis - there is relationship</li>
        <li>Low p-value - low probability that observed results are due to random chance, null hypothesis is rejected</li>
        <li>High p-value - high probability that observed results are due to random chance, null hypothesis is accpeted</li>
      </ul>
      <li>Explain difference between type I and type II error</li>
      <ul>
        <li>Type 1 error - false positive - ground truth is negative but prediction is positive</li>
        <li>Type 2 error - false negative - ground truth is positive but prediction is negative</li>
      </ul>
      <li>Explain correlation and correlation coefficient</li>
      <ul>
        <li>Both strength and direction of relationship between two variables</li>
        <li>Coefficient - positive or negative depending on direction, magnitude represents strength</li>
      </ul>
      <li>Explain statistical power</li>
      <ul>
        <li>Probability of avoiding false negative (power = 1 - probabilty of false negative)</li>
        <li>When p-value is high, before accepting null hypothesis, check power</li>
      </ul>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: 
  </div>
</div>

<div class="card mb-4" id="machine-learning-">
  <div class="card-body">
    <h2 class="card-title">Supervised and Unsupervised Learning</h2>
    <ul>
      <li>Explain how to explain machine learning to a kid</li>
      <ul>
        <li>Computers watch what others do, learn and copy that actions</li>
        <li>Computers learn from previous mistakes</li>
      </ul>
      <li>Explain supervised, unsupervised, weakly supervised, semi-supervised, active, reinforcement learning</li>
      <ul>
        <li>Supervised - data is labeld (regression and classification using neural network, tree)</li>
        <li>Unsupervised - data is not labeled, learn pattern from data (clustering, dimensionaity reduction)</li>
        <li>Weekly supervised - uses nosiy label produced by heuristics, scale and speed over quality</li>
        <li>Semi-supervised - uses small labeled data and large unlabled data</li>
        <li>Active - </li>
        <li>Reinforcement - learn to maximize reward via trial and error</li>
      </ul>
      <li>Explain regression algorithm</li>
      <ul>
        <li>Predict continuous variable from a set of features</li>
        <li>Explain linear regression</li>
        <ul>
          <li>Learn coefficients from training data and inference using only independant variables</li>
          <li>Explain assumptions made in linear regression</li>
          <ul>
            <li>Assume linear relationship between features and target</li>
            <li>Assume errors are independently and identically normally distributed</li>
            <li>Assume data are independent</li>
          </ul>
        </ul>
        <li>Explain evaluation used for regression</li>
        <ul>
          <li>MSE - computation is simpler</li>
          <li>MAE - more robust to outliers</li>
        </ul>
      </ul>
      <li>Explain classification algorithm</li>
      <ul>
        <li>Predict category or class from a set of features</li>
        <li>Dataset is bernoulli (binary) or multinomial (multi-class)</li>
        <li>Explain logistic regression</li>
        <ul>
          <li>Assume linear relationship between input features and log-odds of outputs</li>
          <li>Explain what happens feature scaling isn't applied to logistic regression</li>
          <li>Explain why log-loss is used over MSE in logistic regression</li>
        </ul>
        <li>Explain evaluation used for classification</li>
        <ul>
          <li>Precision - how often model is correct when predicting target class, true positive / (true positive + false positive)</li>
          <li>Recall - whether model can find target class, true positive / (true positive + false negative)</li>
          <li>Accuracy - percentage of predictions that were correct, (True Positive + True Negative) / (True Negative + True Positive + False Negative + False Positive)</li>
          <li>F1 - average of precision and recall</li>
          <li>Explain confusion matrix</li>
          <ul>
            <li>Table showing predicted versus actual (true positive, false negative, false positive, true negative)</li>
          </ul>
          <li>Explain difference between PR-AUC and ROC-AUC</li>
          <ul>
            <li>ROC-AUC - true positive vs false positive, area beneath the curve (If 0.5, random guess, If 1, perfect model)</li>
            <li>PR-AUC - precision vs recall, area beneath the curve</li>
          </ul>
          <li>Explain when to use precision vs recall</li>
          <ul>
            <li>Precision - when missing target class is less critical than incorrectly labeling target class (Ex. spam detection)</li>
            <li>Recall - when incorrect lable is less important than missing target class (Ex. fraud detection)</li>
            <li>Both - (Ex. cancer detection)</li>
          </ul>
        </ul>
      </ul>
      <li>Explain if classification can be converted to regression or vice versa</li>
      <li>Explain KNN</li>
      <ul>
        <li>From a point, find k closest number of points</li>
        <li>No training needed</li>
        <li>High K leads to high bias and low variance</li>
        <li>Classification - return majority vote of nearest neighbors</li>
        <li>Regression - return average value of nearest neighbors</li>
        <li>Explain how to choose value of k</li>
        <li>Explain what happens when increasing and decreasing k</li>
        <li>Explain how value of k impact the bias and variance?</li>
      </ul>
      <li>Explain ANN</li>
      <ul>
        <li>Approximate k points that are closest to a datapoint</li>
      </ul>
      <li>Explain k-means clustering</li>
      <ul>
        <li>Partition data into K clusters</li>
        <ul>
          <li>Elbow method - plot variation (within-cluster sum of squared errors) against number of clsuters, when the curve elbows, adding more cluster does not benefit</li>
          <li>Silhouette score - measures how similar an object is to its own cluster versus other clusters (iterate over different values of k to see which k results in best average score), more computationally expensive than elbow method</li>
        </ul>
        <li>Randomly pick centroids of each cluster</li>
        <li>Assign points to closest cluster, then update centroid</li>
        <li>Repeat until convergence</li>
        <ul>
          <li>Algorithm finished grouping data points into k clusters</li>
          <li>It is when the centroid of last two clusters are very similar</li>
        </ul>
        <li>Explain how to evaluate k-clustering</li>
      </ul>
      <li>Explain curse of dimensionality</li>
      <ul>
        <li>High dimensional data is extremely sparse</li>
        <li>It's hard to do machine learning on sparse data</li>
      </ul>
      <li>Explain principal component analysis</li>
      <ul>
        <li>Special type of SVD</li>
        <li>Left matrix and right matrix are eigenvectors</li>
        <li>Diagonal matrix is eigenvalues</li>
        <li>Sigular value decomposition</li>
        <ul>
          <li>Refactor a matrix into three pieces - left matrix, diagonal matrix, right matrix</li>
        </ul>
      </ul>
      <li>Explain t-SNE</li>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: 
  </div>
</div>

<div class="card mb-4" id="machine-learning-">
  <div class="card-body">
    <h2 class="card-title">Feature engineering</h2>
    <ul>
      <li>Explain why feature selection is needed</li>
      <ul>
        <li>Too many features can cause overfitting</li>
        <li>Become technical debt</li>
        <li>Slower inference if dynamic feature computation is needed</li>
        <li>Increase memory requirement</li>
        <li>Explain feature selection technique</li>
        <ul>
          <li>Filter - evaluate each feature with target one by one, but this ignores feature interactions (Ex. chi-squared test)</li>
          <li>Wrapper - try different combination of features (Ex. recursive feature elimination)</li>
          <li>Embedded - during training, model decides which features are important (Ex. LASSO regression)</li>
        </ul>
      </ul>
      <li>Explain SHAP (SHapley Additive exPlanations)</li>
      <ul>
        <li>Contribution of each feature to prediction</li>
        <li>Compute combination of features and their predictions</li>
      </ul>
      <li>Explain feature scaling</li>
      <ul>
        <li>Help gradient descent to find optimum faster</li>
        <li>Ex. standardization (z-score normalization) where mean is 0 and std is 1</li>
        <li>Assumes data is normallly distributed</li>
      </ul>
      <li>Explain how to encode categorical data</li>
      <ul>
        <li>One-hot encoding - for dense features, assign binary vector to each category</li>
        <li>Embedding - for sparse features, either learn vector for each categorical value or use pre-trained model</li>
      </ul>
      <li>Explain how to handle missing data</li>
      <ul>
        <li>Delete - data is lost</li>
        <li>Impute - data gets noisy (supplyment with mean, median, etc)</li>
        <li>Use model that can handel missing data (Ex. XGBoost)</li>
      </ul>
      <li>Explain how to handle duplicate data</li>
      <ul>
        <li>Explain when to remove duplicate and when not to</li>
        <li>Explain what happens if accidentally duplicate every data point in train set or in test set?</li>
      </ul>
      <li>Explain class inbalance</li>
      <ul>
        <li>Explain how it impacts the model</li>
        <li>Explain why it is hard for ML models to perform well on data with class imbalance</li>
        <li>Explain how to handle imbalanced dataset</li>
        <ul>
          <li>Change performance metric - use precision, recall, F1 score rather than accuracy</li>
          <li>Undersample or oversample</li>
          <li>Data augmentation - crop/rotate images</li>
          <li>Loss function - higher weights to under-represented data</li>
        </ul>
      </ul>
      <li>Explain how to tell if you’ve collected enough samples to train your ML model</li>
      <li>Explain data sparsity</li>
      <ul>
        <li>Explain how it impacts the model</li>
      </ul>
      <li>Explain how to avoid data leakage</li>
      <ul>
        <li>Split time-correlated data by time, not random (otherwise, information from future is leaked to training set)</li>
        <li>Split data first, then scaling (otherwise, statistics of test set can be leaked into training set)</li>
        <li>Do not fill missing data from statistics of test set</li>
        <li>Oversample data only after splitting data</li>
      </ul>
      <li>Explain feature leakage</li>
      <ul>
        <li>Explain causes of feature leakage</li>
        <li>Explain how normalization help prevent feature leakage</li>
        <li>Explain how to detect feature leakage</li>
      </ul>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: 
  </div>
</div>

<div class="card mb-4" id="machine-learning-">
  <div class="card-body">
    <h2 class="card-title">Bias and variance</h2>
    <ul>
      <li>Explain bias-variance trade-off</li>
      <ul>
        <li>Challenge to fight both underfitting and overfitting the model to data</li>
        <li>Explain underfitting</li>
        <ul>
          <li>High bias, training error, model does not work well on training data</li>
        </ul>
        <li>Explain how to tackle underfitting</li>
        <ul>
          <li>More data</li>
          <li>Better/bigger model</li>
          <li>Train longer</li>
        </ul>
        <li>Explain overfitting</li>
        <ul>
          <li>High variance, validation error, model works well on training data but not well on unseen data</li>
        </ul>
        <li>Explain how to tackle overfitting</li>
        <ul>
          <li>More data</li>
          <li>Less features</li>
          <li>Reduce model complexity</li>
          <li>Regularization</li>
          <li>Make sure validation/test set come from same distribution</li>
        </ul>
      </ul>
      <li>Explain cross-validation</li>
      <ul>
        <li>Split training set into different bins</li>
        <li>One bin is set aside as test set and all others as validation set</li>
        <li>Train on training set using validation set as label</li>
        <li>Average accuracies to compute the final accuracy score</li>
        <li>Base population must not co-exist between bins</li>
        <li>Explain why validation and test set are used</li>
        <ul>
          <li>Validation set - used for hyparameters tunung, use data unseen in training set</li>
          <li>Test set - used to test model performance, use data unseen in training and validation set</li>
        </ul>
        <li>Explain why cross-validation is not much used in in deep learning</li>
        <ul>
          <li>Computational expense</li>
        </ul>
      </ul>
      <li>Explain the problem with 80%, 10%, 10% split</li>
      <li>Explain how to perfrom error analysis</li>
      <ul>
        <li>Model not performing well on validation set</li>
        <ul>
          <li>Training-dev set - same distribution as training set, but not used in training</li>
          <li>If dev error >> training error, it could be because both variance and distribution mismatch</li>
          <li>If training-dev error >> training error, it is variance problem</li>
          <li>If dev error >> training-dev error, it is not a variance problem. It is data mismatch problem</li>
        </ul>
        <li>Model not performing well on test set</li>
        <ul>
          <li>Not enough data</li>
          <li>Data that is not representative</li>
          <li>Data with outlier, error, noise, missing values</li>
          <li>Irrelevant features</li>
        </ul>
      </ul>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: 
  </div>
</div>

<div class="card mb-4" id="machine-learning-">
  <div class="card-body">
    <h2 class="card-title">Ensemble</h2>
    <ul>
      <li>Explain decision tree</li>
      <ul>
        <li>Depth of trees increases variance, trees are pruned to control variance</li>
        <li>Main parameters - maximum tree depth, minimum samples per tree node, impurity criterion</li>
        <li>Classification - return majority vote of trees (branch - test outcome, leaf - class label)</li>
        <li>Regression - return average value of trees</li>
        <li>Explain information gain and entropy</li>
        <ul>
          <li>Metrics used to determine best way to split the tree</li>
          <li>Entropy is measure of uncertainty, information gain calculates reduction in entropy</li>
          <li>Select variable that maximizes the information gain and split dataset into groups based on that variable</li>
        </ul>
      </ul>
      <li>Explain emsembles</li>
      <ul>
        <li>They have higher scores than individual models because they gather strengths from many models</li>
        <li>Explain bagging</li>
        <ul>
          <li>Train models independently in parallel and combine the result (Ex. random forest)</li>
          <li>Less prone to overfitting, faster training, less hyperparameter tuning</li>
        </ul>
        <li>Explain boosting</li>
        <ul>
          <li>Train models in sequence where later models learn from mistakes from previous models (Ex. gradient boosting)</li>
          <li>Tree is built starting from root, for each leaf, split selected to minimize MSE</li>
          <li>Predictions of individual trees are summed</li>
          <li>More accurate, works well on unbalanced datatset</li>
        </ul>
        <li>Explain how bagging and boosting are used in deep learning</li>
      </ul>
      <li>Explain XGBoost</li>
      <ul>
        <li>Good at handling sparse data</li>
        <ul>
          <li>It can handle one-hot encoded categorical features with any transformation</li>
          <li>Trees can split based on missing values or can treat missing values as distinct feature</li>
          <li>It does not focus on features that don't contribute much to target</li>
        </ul>
        <li>Not suitable for continual learning</li>
        <ul>
          <li>Any new data requires the model to be re-trained</li>
          <li>Incremental learning, where updating model without re-training, is not possible</li>
          <li>Model training involves assessing entire dataset, which is slow</li>
        </ul>
      </ul>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: 
  </div>
</div>

<div class="card mb-4" id="machine-learning-">
  <div class="card-body">
    <h2 class="card-title">Neural Network</h2>
    <ul>
      <li>Explain conditions that allowed deep learning to gain popularity in the last decade</li>
      <ul>
        <li>Data</li>
        <li>Compute - GPUs</li>
        <li>Algorithm</li>
      </ul>
      <li>Explain neural network algorithm</li>
      <ul>
        <li>Data is fed to input layer</li>
        <li>Each neuron in hidden layer calculates based on input, weight, bias, and activation function</li>
        <li>Output layer generates prediction</li>
        <li>Loss (or cost, which is sum of losses) is computed using prediction and label</li>
        <li>Gradient of loss w.r.t. weights and bias (which minimizes the cost) are computed</li>
        <li>Weights and bias are updated using gradients</li>
        <li>This process iterates over many epoch</li>
        <li>Explain cost function</li>
        <ul>
          <li>Loss is a function that computes difference between predicted and label</li>
          <li>Cost is sum of losses over all training batch</li>
        </ul>
        <li>Explain epoch, batch, and iteration, depth</li>
        <ul>
          <li>Epoch - number of times to loop through all training data</li>
          <li>Batch - amount of training data to make a single gradient update</li>
          <li>Iteration - number of times to train model through all epoch under a set of hyperparameters</li>
          <li>Depth - number of layers (input layer plus hidden layers, not counting output layer)</li>
        </ul>
        <li>Explain logits</li>
        <ul>
          <li>Unnormalized output of model</li>
        </ul>
        <li>Explain significance of bias term</li>
        <ul>
          <li>Without bias term, activation function curve always passes through origin</li>
          <li>Bias allows activation function to be shifted, making it more flexible to fit data well</li>
        </ul>
        <li>Explain why training data is shuffled after each epoch</li>
        <ul>
          <li>Model may learn pattern from ordering of data</li>
        </ul>
      </ul>
      <li>The Universal Approximation Theorem states that a neural network with 1 hidden layer can approximate any continuous function for inputs within a specific range. Then, Explain why a simple neural network reach an arbitrarily small positive error?</li>
      <ul>
        <li>Computers have finite precision</li>
        <li>Training data may not be perfectly representative, meaning it cannot generalized perfectly</li>
        <li>If true function is discontinuous, neural network may struggle to learn that sharp change</li>
      </ul>
      <li>Explain whether wide NN or deep NN is more expressive if they have same number of parameters</li>
      <ul>
        <li>Deep</li>
        <li>Early layers learn simple features and later layers learn more complex features</li>
      </ul>      
      <li>Explain challenges with very deep neural network</li>
      <ul>
        <li>Exploding and vanishing gradient - use gradient clipping</li>
        <li>Overfitting - use regularization</li>
        <li>Slow convergence - use adaptive learning rate and learning rate scheduler</li>
      </ul>
      <li>Explain hyperparameter</li>
      <ul>
        <li>Parameters that are not learned but empericially set by modelers</li>
        <li>Explain hyperparameter tuning</li>
        <ul>
          <li>Hold out validation set</li>
          <li>Try combinations of hyperparameters and test model performance on validation set</li>
          <li>Scale parameters using log scale (Ex. 0.0001 to 1, use 0.0001, 0.001, 0.01, 0.1, 1)</li>
          <li>Ex. grid search, random search, bayesian optimization</li>
          <li>Iterate and pick the best combination</li>
        </ul>
        <li>Explain hyperparameter tuning algorithms</li>
        <li>Explain which hyperparameter are important in deep learning</li>
        <ul>
          <li>Learning rate is most important</li>
          <li>Mini-batch size, the number of hidden units are second important</li>
          <li>The number of layers and learning rate decay are third important</li>
        </ul>
      </ul>
      <li>Explain skip-connection</li>
      <li>Explain activation function</li>
      <ul>
        <li>Function that determines outputs of neuron</li>
        <li>Add non-linearity to allow network to learn more complex pattern</li>
        <li>Explain different between ReLU and sigmoid</li>
        <ul>
          <li>Sigmoid - function flattens near zero, leading to vanishing gradient</li>
          <li>ReLU - neuron does not active for negative input, thus addresses vanishing gradient</li>
        </ul>
      </ul>
      <li>Explain normalization</li>
      <ul>
        <li>If features are not scaled, contours can be assymmetric, gradient descent can take long time</li>
        <li>If features are scaled, contours can be symmetric, gradient decent can find optimum faster</li>
        <li>Explain difference between normalization and standardization</li>
        <ul>
          <li>Normalization - values are between 0 and 1, does not change distribution</li>
          <li>Standardization - values have mean 0 and std 1, changes to normal distribution</li>
        </ul>
        <li>Explain difference between batch and layer normalization</li>
        <ul>
          <li>Batch - take a batch and normalize feature by feature</li>
          <li>Layer - take all features and normalize data point by data point</li>
        </ul>
      </ul>
      <li>Explain weight initialization methods like Xavier, He</li>
      <ul>
        <li>If weights are initialized to same values, all neurons in each layer output the same value</li>
        <li>He - initialize weights to small random values (If values are too large or too small, it can lead to exploding/vanish gradient)</li>
      </ul>
      <li>Explain weight constraints and how it can benefit training</li>
      <ul>
        <li>During training, if weights exceed certain value, keep it as threshold</li>
        <li>This helps preventing overfitting</li>
      </ul>
      <li>Explain regularization and why we use it</li>
      <ul>
        <li>Larger lambda leads W to become smaller, reducing the impacts of hidden units</li>
        <li>Smaller W also leads Z to become smaller</li>
        <li>The activation function could more or less be linear function when Z is small</li>
        <li>Thus, network does not compute complex function</li>
        <li>Explain difference between L1 and L2</li>
        <ul>
          <li>L1 - add sum of absolute weights to loss function, works well on sparse data, features can be removed</li>
          <li>L2 - add sum of squares of weights to loss function, works well when features are corelated</li>
          <li>Why L1 regularization tend to lead to sparsity while L2 regularization pushes weights closer to 0?</li>
        </ul>
        <li>Explain dropout</li>
        <ul>
          <li>Neurons are ramdonly shut down with probability p</li>
          <li>Neurons become reluctant to put too much weight on one feature</li>
          <li>Weight values can be smaller and more spread out</li>
        </ul>
        <li>Explain early stopping</li>
        <ul>
          <li>Stop training when validation error increases</li>
          <li>Tackle bias and variance at the same time (rather than one at a time)</li>
        </ul>
      </ul>
      <li>Explain batch size and how it is related to convergence</li>
      <ul>
        <li>Small batch - frequent and noisy gradient updates, can escape local optima well but too much osciallation and slow down convergence</li>
        <li>Large batch - stable gradient updates, may get stuck in local optima</li>
        <li>Balanced batch is needed</li>
      </ul>
      <li>Explain difference between batch, min-batch, and stochastic gradient descent</li>
      <ul>
        <li>Batch - use all training data to make a single gradient update, stable but slow</li>
        <li>Min-batch - use subset of training data</li>
        <li>Stachastic - use a single data, fast but final parameters may not be optimal</li>
      </ul>
      <li>Explain vanishing and exploding gradient</li>
      <ul>
        <li>Vanishing - gradient becomes very small</li>
        <li>Exploding - gradient becomes very large</li>
        <li>Explain how to deal with vanishing gradient</li>
        <ul>
          <li>ReLu</li>
          <li>Weight initialization</li>
          <li>Small learning rate</li>
          <li>Residual connection</li>
          <li>Batch/layer normalization</li>
        </ul>
        <li>Explain how to deal with exploding gradient</li>
        <ul>
          <li>Gradient clipping</li>
          <li>Weight initialization</li>
          <li>Small learning rate</li>
          <li>Residual connection</li>
          <li>Batch/layer normalization</li>
        </ul>
      </ul>
      <li>Explain ADAM optimizer</li>
      <ul>
        <li>Combine calculations from momentum and RMSProp</li>
        <li>Explain momentum</li>
        <ul>
          <li>On vertical axis (b), we want slower learning to reduce oscilation</li>
          <li>On horizontal axis (W), we want faster learning</li>
          <li>Apply additional computation when updating W and b with velocity of W and b</li>
        </ul>
        <li>Explain RMSProp</li>
        <ul>
          <li>Similar idea from momentum, but different calculation</li>
        </ul>
      </ul>
      <li>Explain difference between global optima and local optima</li>
      <ul>
        <li>Global - optimum value of function in its entire span</li>
        <li>Local - optimum value of function in its subrange</li>
      </ul>
      <li>Explain gradient clipping and how it help training</li>
      <ul>
        <li>If gradient exceed certain value, set it to threshold</li>
        <li>Prevents exploding gradient</li>
      </ul>
      <li>Explain learning rate</li>
      <ul>
        <li>In gradient descent, size of step you want to take in particular direction</li>
        <li>Explain adaptive learning rate</li>
        <ul>
          <li>Bigger learning rate in the beginning and smaller near the optimum</li>
        </ul>
        <li>Explain learning rate scheduler</li>
        <ul>
          <li>Similar to adaptive learning rate, but uses pre-defined schedule</li>
          <li>Normally consists of warm up, constant hold, and exponential phases</li>
        </ul>
        <li>Explain why learning rate is considered as most important hyperparameter</li>
        <ul>
          <li>It directly controls how quickly model adjusts its weights during training</li>
        </ul>
      </ul>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: 
  </div>
</div>

<div class="card mb-4" id="machine-learning-">
  <div class="card-body">
    <h2 class="card-title">Time-series and Sequential Data</h2>
    <ul>
      <li>Explain RNN</li>
      <!-- <ul>
        <li>Neural network that process words timestep by timestep</li>
        <li>Uses previous words to predict the next word</li>
      </ul> -->
      <li>Explain difference between feed forward network and recurrent network</li>
      <!-- <ul>
        <li>Feed forward - suitable for problems having single output, neurons have different weights</li>
        <li>RNN - suitable for sequential input data, all neurons in a layer have the same weights</li>
      </ul> -->
      <li>Explain LSTM</li>
      <!-- <ul>
        <li>RNN that captures long-term dependencies in sequential data</li>
        <li>Uses memory cells and gates to remember or forget information</li>
      </ul> -->
      <li>Explain how LSTM handle vanishing gradient</li>
      <!-- <ul>
        <li>Memory cells keep gradient consistant over long sequence</li>
      </ul> -->
    </ul>       
  </div>
  <div class="card-footer text-muted">
    Reference: 
  </div>
</div>

<div class="card mb-4" id="machine-learning-">
  <div class="card-body">
    <h2 class="card-title">Natural Language Processing</h2>
    <ul>
      <li>Explain word embedding</li>
      <ul>
        <li>Representing word by numerical vector</li>
        <li>Words with similar meaning has close distance</li>
        <li>Explain difference between embedding and one-hot</li>
        <ul>
          <li>One-hot encoding is suitable for dense feature, cannot capture semantic meaning</li>
          <li>Embedding can handle sparse feature, can capture semantic meaning</li>
        </ul>
        <li>Explain why embeddings are needed</li>
        <ul>
          <li>Unstructured data like text, image, video must be translated to numerical format for model to understand</li>
        </ul>
        <li>Explain difference between count-based, prediction-based, context-based word embeddings</li>
        <ul>
          <li>Count-based - generates embedding by occurance/co-occurance of words in corpus, ex. TF-IDF, GloVe</li>
          <li>Prediction-based - generates embedding by training models to predict target by its surrounding words or vice-versa, ex. Word2Vec (CBOW, Skip-gram)</li>
          <li>Context-based - generates embedding such that vectors are different in different sentence (unlike prediction-based where vectors are static), ex. BERT, ELMo</li>
        </ul>
        <li>Explain problems with context-based word embeddings</li>
        <ul>
          <li>Requires large training data, high computational cost, and high inference latency</li>
        </ul>
      </ul>
      <li>Explain sentence embedding</li>
      <ul>
        <li>Convert entire sentence into a vector</li>
        <li>Simple way is to compute word embeddings and averge them, but SentenceBERT is more popular approach</li>
        <li>Explain SentenceBERT</li>
      </ul>
      <li>Explain tokenization</li>
      <ul>
        <li>Split paragraphs or sentences into words (removing punctation, lower casing, etc)</li>
        <li>Rare words can be broken down into subwords while common words can be kept as they are</li>
        <li>High quality data (No abbrevation, spelling mistakes) is needed</li>
      </ul>
      <li>Explain Word2Vec</li>
      <ul>
        <li>NLP technique to learn embeddings of words</li>
        <li>For example, CBOW and skip-gram</li>
        <li>Explain CBOW</li>
        <ul>
          <li>Predict word by surrounding words, fill-in-the-blank task</li>
        </ul>
        <li>Explain Skip-gram</li>
        <ul>
          <li>Predict surrounding words by target word</li>
        </ul>
        <li>For n-gram language models, does increasing the context length (n) improve the model’s performance</li>
        <li>What problems might we encounter when using softmax as the last layer for word-level language models</li>
      </ul>
      <li>Explain challenges in LLM</li>
      <ul>
        <li>Knowledge is limited to what was available at the time of training</li>
        <li>LLM tends to make things up</li>
        <li>Input and output length is limited</li>
        <li>Does not work well with structured (tabular) data</li>
        <li>Can reflect bias</li>
        <li>Not good at precise math</li>
      </ul>
      <li>Explain transfer learning and its benefits</li>
      <ul>
        <li>Task A and B have the same input</li>
        <li>Task A has a lot more data than task B</li>
        <li>Delete the last layer and weights that feed to the last layer</li>
        <li>Create new randomly initialized weights for the last layer</li>
        <li>Retain the network with dataset for task B</li>
      </ul>
      <li>Explain pre-training and fine-tuning</li>
      <ul>
        <li>In this case, training the network for task A is called <strong>pre-training</strong></li>
        <li>In this case, training the network for task B is called <strong>fine-tuning</strong></li>
      </ul>
      <li>Explain self-attention</li>
      <ul>
        <li>Every word (token) is related to every other word (token)</li>
        <li>Input sentence is tokenized and represented by Q,K,V whose dimension is batch_size, seq_len, d_model</li>
        <li>Q - words that we are focusing on, K - words that we are comparing against, V - multipled to softmax output</li>
        <li>Explain attention mask</li>
        <ul>
          <li>Separate important and unimportant tokens in input</li>
        </ul>
        <li>Explain different between self-attention and RNN/CNN</li>
        <li>Explain why we need multi-headed attention instead of just one head for attention</li>
        <li>Explain how changing the number of heads in multi-headed attention affect the model’s performance</li>
      </ul>
      <li>Explain transformer</li>
      <ul>
        <li>Model architecture used to process sequence data, particularly relating input and output sequences</li>
        <li>Explain drawbacks of transformer</li>
        <ul>
          <li>Self attention runs in \( O(N^{2}D )\) where \( N \) is sequence length and \( D \) is dimension of words embedding</li>
          <li>Capturing word/token interaction requires large number of parameters, thus large memory</li>
        </ul>
        <li>Explain regularization used in transformer</li>
        <ul>
          <li>Dropout applied to fully connected layer</li>
        </ul>
        <li>Explain normalization used in transformer</li>
        <ul>
          <li>Layer normalization is typically applied just before self-attention layer</li>
          <li>Batch normalization is not normally used due to varying sequence length</li>
        </ul>
        <li>Explain how to achieve fast inference on transformer model</li>
        <ul>
          <li>Sparse attention or local attention</li>
          <li>Data parallelism or model parallelism</li>
          <li>Lower precision floating format</li>
          <li>Dynamic padding sequence</li>
        </ul>
      </ul>
      <li>Explain BERT</li>
      <ul>
        <li>Bi-drectional encoder representation from transformer</li>
        <li>Model architecture used to understand contexts of words</li>
        <li>Ex. question answering, sentiment analysis, text classification</li>
        <li>Encoder only</li>
      </ul>
      <li>Explain GPT-3</li>
      <ul>
        <li>Generative pre-trained transformers</li>
        <li>Model architecture used to generate texts</li>
        <li>Ex. text generation</li>
        <li>Decoder only</li>
      </ul>
      <li>Explain RAG</li>
      <ul>
        <li>Retrieval augmented generation</li>
        <li>Populate DB with information not available in LLM</li>
        <li>Using query embedding, search relevant information by embedding similairty</li>
        <li>Provide LLM with query and context retrieved from DB</li>
      </ul>
      <li>Explain LoRA</li>
      <ul>
        <li>Low ranking adaption</li>
        <li>Fine-tuning LLM with lower number of parameters</li>
        <li>Freeze weights of pre-trained model and train low rank matrix (adapter)</li>
      </ul>
      <li>Explain PPO</li>
      <ul>
        <li>Proximal policy optimization</li>
        <li>Policy gradient method that trains agent via rules</li>
        <li>Can be used in fune-tuning LLM to generate better dialogue</li>
      </ul>
      <li>Explain RLHF</li>
      <ul>
        <li>Reinforment learning from human feedback</li>
        <li>Human evaluates LLM outputs (rating, text summary, etc)</li>
        <li>Train reward model to predict how human would evaluate LLM response</li>
        <li>Fine-tune LLM via reinforment learning with reward model</li>
      </ul>
      <li>Explain multi-modal models</li>
      <ul>
        <li>Model that supports multiple types of inputs (text, image, audio, video)</li>
      </ul>
      <li>Explain how stability diffusion model use LLM to understand complex text prompts</li>
      <ul>
        <li>Generate image based on text description</li>
        <li>Combine response from LLM and stability diffusion model to provide final response with both text and image</li>
      </ul>
      <li>Explain how to train LLM with billions of parameters</li>
      <ul>
        <li>Text cleaning and tokenization</li>
        <li>Distribute workload among GPU nodes</li>
        <li>Optimize learning rate</li>
      </ul>
      <li>Explain how to train LLM to prevent hallucinations</li>
      <ul>
        <li>Improve data quality and diversity</li>
        <li>Alternatively, RAG/RLHF can be used</li>
      </ul>
      <li>Explain how to prevent bias and harmful prompt</li>
      <ul>
        <li>Use diverse and unharmful data</li>
        <li>Alternatively, RLHF can be used</li>
      </ul>
      <li>Explain how knowledge distillation benefits LLM</li>
      <ul>
        <li>Create smaller model from large model without losing much knowledge</li>
      </ul>
      <li>Explain few shot learning in LLM</li>
      <ul>
        <li>Provide few examples (explaining how LLM should respond to the prompt) in the prompt</li>
      </ul>
      <li>Explain how to evaluate LLM performance</li>
      <ul>
        <li>BLEU (bilingual evaluation understudy) to measure embedding similarity between LLM output and human-generated text</li>
        <li>Hallucination rate - human decides whether LLM hallucinates or not</li>
        <li>Factual accuracy - human decides whether each claim LLM makes in reponse is factual or not</li>
      </ul>
      <li>Explain how to improve factual accuracy of LLM</li>
      <ul>
        <li>Improve data quality</li>
        <li>Fine-tune on domain specific data</li>
        <li>Clear and step-by-step prompt</li>
      </ul>
      <li>Explain how to detect drift in LLM performance over time</li>
      <ul>
        <li>Monitor distribution of prompts and generated response</li>
        <li>Statistical tests (Kolmogorov–Smirnov, Mann–Whitney U-test)</li>
      </ul>
      <li>Explain how to curate dataset for training LLM</li>
      <ul>
        <li>Text cleanup - lower case, remove punctuation</li>
        <li>Tokenize - split sentences into words</li>
        <li>Token to ID - convert each word into ID</li>
      </ul>
      <li>Explain how to fine-tune LLM for domain specific applications like finance</li>
      <ul>
        <li>Prepare dataset for fune-tuning</li>
        <li>Tokenize prepared dataset</li>
        <li>Load pre-trained model</li>
        <li>Freeze weights in pre-trained model and initialize weights for fine-tuning</li>
        <li>Determine hyperparameters</li>
      </ul>
    </ul> 
  </div>
  <div class="card-footer text-muted">
    Reference: 
  </div>
</div>

<div class="card mb-4" id="machine-learning-">
  <div class="card-body">
    <h2 class="card-title">Computer Vision</h2>
    <ul>
      <li>Explain how convolution works, when inputs are gray-scale or RGB</li>
      <li>Explain why we use convolution layer rather than just FC layers</li>
      <li>Explain why we use many small kernels like 3 by 3 rather than few large kernels</li>
      <li>Explain why we use max-pooling and how it is different from average pooling</li>
      <li>Explain ResNet and skip connection</li>
      <li>Explain non-max suppression</li>
      <li>Explain data augmentation</li>
      <li>Explain autoencoder</li>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: 
  </div>
</div>

<div class="card mb-4" id="machine-learning-">
  <div class="card-body">
    <h2 class="card-title">Reinforcement Learning</h2>
    <ul>
      <li>Explain Markov Decision Process for reinforcement learning</li>
      <li>Explain main component of reinforcement learning agents</li>
      <li>Explain how to define reward function</li>
      <li>Explain difference between model-based and model-free reinforcement learning</li>
      <li>Explain the role of discount factor</li>
      <li>Explain exploration-exploitation trade-off</li>
      <li>Explain difference between policy gradient and value iteration method</li>
      <li>Explain State (Q) and Action-Value (Q) functions</li>
      <li>Explain how to handle continuous action space</li>
      <li>Explain deep reinforcement learning</li>
      <li>Explain how to ensure convergence of reinforcement learning</li>
      <li>Explain how multi-agent reinforcement learning system work</li>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: 
  </div>
</div>

<div class="card mb-4" id="machine-learning-">
  <div class="card-body">
    <h2 class="card-title">Data Engineering</h2>
    <ul>
      <li>Explain how to ingest large amount of data with minimal latency</li>
      <ul>
        <li>Use streaming data processing</li>
        <li>Edge compute at data source location, compress data, then load/transfer</li>
        <li>Parallel process using distributed compute nodes</li>
        <li>Convert data format to protobuf (rather than JSON, XML) for faster processing</li>
      </ul>
      <li>Explain how to handle schema evolution in data lake environment</li>
      <ul>
        <li>Use parquet, which supports adding columns, re-ordering columns without data re-writes</li>
      </ul>
      <li>Explain when to prefer streaming or batch processing</li>
      <ul>
        <li>Streaming - low latency decision-making is needed (Ex. fraud detection), data is written continuously</li>
        <li>Batch - cost effective</li>
      </ul>
      <li>Explain how to design data model for fast lookup and analytical queries</li>
      <ul>
        <li>For fast lookup, prepare normalized and indexed dataset. For queries, prepare de-normalized dataset</li>
      </ul>
      <li>Explain index in database</li>
      <ul>
        <li>Sorting columns to execute binary search</li>
        <li>Need to store sorted data in memory</li>
        <li>Data write is slower</li>
        <li>Not performant when many duplicate data exists</li>
      </ul>
      <li>Explain perfomance bottlenecks in Spark</li>
      <ul>
        <li>Data shuffles - moving data between partitions is expensive</li>
        <li>Data skews - partition with more data, thus slower compute, determine speed of overall execution</li>
      </ul>
      <li>Explain how to build data pipeline to handle duplicate, missing, PII data</li>
      <ul>
        <li>Enforce schema validation on arriving data</li>
        <li>Standardize data - date format, currency, etc</li>
        <li>De-dup - calculate similarity between fields (using algorithms) and merge data</li>
        <li>Missing data rule - impute/delete</li>
        <li>Implement pattern matching rule to detect PII data, then mask them in non-prod and encrypt them in prod</li>
      </ul>
      <li>Explain how to prepare data for feature store</li>
      <ul>
        <li>Data cleaning - de-dup, missing data, PII</li>
        <li>If numbers, scale and normalize</li>
        <li>If categorical - encode (one-hot, embedding)</li>
        <li>If text - tokenize and convert to ID</li>
        <li>If image - convert to 1D vector, standardize pixel value, pick color mode</li>
      </ul>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: 
  </div>
</div>

<div class="card mb-4" id="machine-learning-">
  <div class="card-body">
    <h2 class="card-title">Model Serving - Discriminative</h2>
    <ul>
      <li>Explain difference between batch serving and online serving</li>
      <li>Explain strategies for re-training the model</li>
      <li>Explain how to use model registry and model versioning</li>
      <li>Explain how to monitor the model</li>
      <li>Explain how to orchestrate workflow using Airflow/Kubeflow</li>
      <li>Explain how to track experiments using MLflow</li>
      <li>Explain how to reduce model size and optimize for deployment on smart phones</li>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: 
  </div>
</div>

<div class="card mb-4" id="machine-learning-">
  <div class="card-body">
    <h2 class="card-title">Model Serving - Generative</h2>
    <ul>
      <li>Explain how to achieve scaling, load-balancing, fast response for LLM application</li>
      <li>Explain how to use caching to improve LLM performance</li>
      <ul>
        <li>Explain caching</li>
        <li>Explain write-through cache and write-back cache</li>
      </ul>
      <li>Explain trade-off between GPU and TPU</li>
      <li>Explain how to monitor LLM in production</li>
      <li>Explain how vector database work</li>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: 
  </div>
</div>

<div class="card mb-4" id="machine-learning-">
  <div class="card-body">
    <h2 class="card-title">Non-functional Requirement</h2>
    <ul>
      <li>Explain availability</li>
      <li>Explain reliability</li>
      <li>Explain scalability</li>
      <li>Explain performance</li>
      <li>Explain consistency</li>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: 
  </div>
</div>

<div class="card mb-4" id="machine-learning-">
  <div class="card-body">
    <h2 class="card-title">Deployment</h2>
    <ul>
      <li>Explain vertical and horizontal scaling as well as scaling out and scaling in</li>
      <li>Explain how to achieve zero-downtime deployment</li>
      <li>Explain how to rollback faulty deployment</li>
      <li>Explain edge deployment</li>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: 
  </div>
</div>

<div class="card mb-4" id="machine-learning-">
  <div class="card-body">
    <h2 class="card-title">Distributed System</h2>
    <ul>
      <li>Explain partitioning</li>
      <li>Explain database sharding strategies like consistent hashing</li>
      <li>Explain CAP theorem</li>
      <li>Explain how to handle security concerns in distributed system</li>
      <li>Explain how to handle millions of events per second</li>
      <li>Explain how to achieve high availability</li>
      <li>Explain how to perform real-time analytics</li>
      <li>Explain how to ensure data consistency in micro-service architecture</li>
      <li>Explain how to track user activities on websites</li>
      <li>Explain how to optimize read-heavy and write-heavy systems</li>
      <li>Explain how to handle versioning in micro-service architecture</li>
      <li>Explain how to handle data replication and consistency in distributed database</li>
      <li>Explain how to handle hot keys in caching system</li>
      <li>Explain how to handle system failure during transactions</li>
      <li>Explain how to support multiple languages and regions</li>
      <li>Explain how to handle database connection pooling in a large scale application</li>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: 
  </div>
</div>

<div class="card mb-4" id="machine-learning-">
  <div class="card-body">
    <h2 class="card-title">Recommendation System</h2>
    <ul>
      <li>Explain collaborative filtering</li>
      <!-- <ul>
        <li>Recommendation is based on historical user-item interaction</li>
        <li>Cold start problem - cannot make recommendation for new item, cannot find similarity with other users for new user</li>
        <li>Item-based - rate an item based on ratings by users similar to current user</li>
        <li>User-based - rate an item based on similar items that current user rated</li>
      </ul> -->
      <li>Explain content-based filtering</li>
      <!-- <ul>
        <li>An approach to solve cold start problem</li>
        <li>Recommend items that are similar to items that user liked already</li>
        <li>Do not take other users into consideration</li>
      </ul> -->
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: 
  </div>
</div>
<!-- Machine Learning END -->

</div> <!-- /.col-md-12 -->
</div> <!-- /.row -->
</div> <!-- /.container -->

<include src="/footer.html"></include>

</body>

</html>