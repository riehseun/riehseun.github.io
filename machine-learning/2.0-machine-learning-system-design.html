<!DOCTYPE html>

<html lang="en">

<head>

<!-- Metadata -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="">
<meta name="author" content="Seungmoon Rieh">
<meta name="keywords" content="">

<!-- Title and image -->
<title>Seungmoon Rieh</title>
<link href="/img/seungmoonrieh.jpg" rel="icon">

<!-- CSS -->
<link href="/css/bootstrap.min.css" rel="stylesheet">
<link href="/css/monokai-sublime.css" rel="stylesheet">
<link href="/css/site.css" rel="stylesheet">

<!-- JavaScript -->
<script src="/js/jquery.min.js" type="text/javascript"></script>
<script src="/js/bootstrap.bundle.min.js" type="text/javascript"></script>
<script src="/js/highlight.pack.js" type="text/javascript"></script>
<script src="/js/include_html.js" type="text/javascript"></script>
<script src="/js/mathjax/tex-chtml.js" type="text/javascript"></script>
<script src="/js/site.js" type="text/javascript"></script>

</head>

<body>

<include src="/header.html"></include>

<div class="container">
<div class="row">
<div class="col-md-12">
<h1 class="my-4">Machine learning</h1>

<!-- Machine learning system design BEGIN -->
<div class="card mb-4" id="machine-learning-system-design">
  <div class="card-body">
    <h2 class="card-title">Machine learning system design</h2>
    <ul class="list-unstyled mb-0">
      <li><a href="#machine-learning-system-design-">Clarify requirements</a></li>
      <li><a href="#machine-learning-system-design-">Frame problem as ML task</a></li>
      <li><a href="#machine-learning-system-design-">Data preparation</a></li>
      <li><a href="#machine-learning-system-design-">Model development</a></li>
      <li><a href="#machine-learning-system-design-">Evaluation</a></li>
      <li><a href="#machine-learning-system-design-">Deployment, serving, monitoring</a></li>
      <li><a href="#machine-learning-system-design-">Infrastructure</a></li>
    </ul>
  </div>
</div>

<div class="card mb-4" id="machine-learning-system-design-">
  <div class="card-body">
    <h2 class="card-title">Clarify requirements</h2>
    <ul>
      <li>Business objective</li>
      <ul>
        <li>Ex. increase number of bookings</li>
        <li>Ex. increase revenue</li>
      </ul>
      <li>Features that the system supports</li>
      <ul>
        <li>Ex. whether users can like/dislike videos</li>
      </ul>
      <li>Data</li>
      <ul>
        <li>What are data source?</li>
        <li>How large is dataset?</li>
        <li>Is data labeled?</li>
      </ul>
      <li>Constraints</li>
      <ul>
        <li>Available computing power</li>
        <li>Mobile device vs cloud</li>
      </ul>
      <li>Scale</li>
      <ul>
        <li>How many users?</li>
      </ul>
      <li>Performance</li>
      <ul>
        <li>How fast should prediction be?</li>
        <li>Batch vs real-time</li>
        <li>Accuracy vs latency</li>
      </ul>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: Machine Learning System Design Interview, Ali Aminian & Alex Xu | Machine Learning Design Patterns, Valliappa Lakshmanan & Sara Robinson & Michael Munn | Designing Machine Learning Systems, Chip Huyen
  </div>
</div>

<div class="card mb-4" id="machine-learning-system-design-">
  <div class="card-body">
    <h2 class="card-title">Frame problem as ML task</h2>
    <ul>
      <li>Problem representation</li>
      <ul>
        <li>Reframing</li>
        <ul>
          <li>Ex. regression problem to predict rainfall amount -> classification problem to model discrete probability distribution</li>
        </ul>
        <li>Multi-labeling</li>
        <ul>
          <li>Ex. image consists of multiple animals rather than just one animal like cat, dog, rabbit</li>
          <li>Give more than one label to training example</li>
          <li>Encode the label using a multi-hot array</li>
          <li>Final output layer will have a sigmoid activation function where each value in the array is in range between \( 0 \) and \( 1 \)</li>
        </ul>
        <li>Ensembles</li>
        <ul>
          <li>Suitable for bia-variance trade-off on small to medium scale problems</li>
          <li>Bagging</li>
          <ul>
            <li>Ex. Random Forest</li>
          </ul>
          <li>Boosting</li>
          <ul>
            <li>Ex. AdaBoost, XGBoost</li>
          </ul>
          <li>Stacking</li>
          <ul>
            <li>Combines the outputs of a collection of models to make a prediction</li>
          </ul>
        </ul>
        <li>Cascade</li>
        <ul>
          <li>Break a problem into many sub-problems</li>
          <li>Ex. predict the likelihood of a customer returning an item</li>
          <ul>
            <li>Predict whether a specific transaction is by a reseller or retail buyer</li>
            <li>Train a model on sales to retail buyers</li>
            <li>Train a model on sales to resellers</li>
            <li>Combine the outputs of three seperate models to predict return likelihood</li>
          </ul>
        </ul>
        <li>Neutral class</li>
        <ul>
          <li>Ex. three-class classifier that outputs disjoint probabilities for Yes, No, Maybe</li>
        </ul>
      </ul>
      <li>Define ML objective</li>
      <ul>
        <li>Ex. increase ticket sales -> maximize number of event registration</li>
        <li>Ex. increase user engagement -> maximize time users spend watching videos</li>
        <li>Ex. increase user clicks -> maximize click-through rate</li>
        <li>Ex. improve platform's safety -> predict if content is harmful</li>
        <li>Ex. increase user network growth -> maximize number of formed connections</li>
        <li>Input</li>
        <ul>
          <li>Ex. post</li>
        </ul>
        <li>Output</li>
        <ul>
          <li>Ex. whether post is harmful or not</li>
        </ul>
      </ul>
      <li>Choose right ML category</li>
      <ul>
        <li>Supervised learning - learn from training dataset</li>
        <ul>
          <li>Support vector machines</li>
          <li>Decision trees</li>
          <li>Neural networks</li>
          <ul>
            <li>Linear network</li>
            <ul>
              <li>Linear regression</li>
              <li>Logistic regression</li>
            </ul>
            <li>Deep neural networks</li>
            <li>Convolutional neural networks</li>
            <li>Recurrent neural networks</li>
          </ul>
        </ul>
        <li>Supervised learning (alternative view)</li>
        <ul>
          <li>Classification - predict discrete class label</li>
          <ul>
            <li>Binary</li>
            <li>Multiclass</li>
            <li>Multilabel - an example can belong to more than one class</li>
          </ul>
          <li>Regression - predict continuous numeric value</li>
        </ul>
        <li>Unsupervised learning - learn pattern among data</li>
        <ul>
          <li>Clustering</li>
          <ul>
            <li>Density-based spacial clustering with noise (DBSCAN)</li>
            <li>K-means clustering</li>
          </ul>
          <li>Associative rule learning</li>
          <li>Dimension reduction</li>
          <ul>
            <li>Principle component analysis (PCA)</li>
            <li>Isomap</li>
          </ul>
        </ul>
        <li>Reinforcement learning - learn from repeated trial-and-error</li>
      </ul>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: Machine Learning System Design Interview, Ali Aminian & Alex Xu | Machine Learning Design Patterns, Valliappa Lakshmanan & Sara Robinson & Michael Munn | Designing Machine Learning Systems, Chip Huyen
  </div>
</div>

<div class="card mb-4" id="machine-learning-system-design-">
  <div class="card-body">
    <h2 class="card-title">Data preparation</h2>
    <ul>
      <li>Data engineering</li>
      <ul>
        <li>Data source</li>
        <ul>
          <li>How data is collected?</li>
          <li>How clean is data?</li>
          <li>Can data source be trusted?</li>
          <li>Is data user-generated or system-generated?</li>
          <li>How often new data comes in?</li>
          <li>Can data be stored in servers or data cannot leave user device?</li>
          <li>Does data need to be tokenized?</li>
        </ul>
        <li>Data storage (DB)</li>
        <ul>
          <li>SQL</li>
          <ul>
            <li>Relational</li>
            <ul>
              <li>Ex. MySQL, PostgreSQL</li>
            </ul>
          </ul>
          <li>NoSQL</li>
          <ul>
            <li>Key-value</li>
            <ul>
              <li>Ex. Redis, DynamoDB</li>
            </ul>
            <li>Column-based</li>
            <ul>
              <li>Ex. Cassandra, HBase</li>
            </ul>
            <li>Graph</li>
            <ul>
              <li>Ex. Neo4J</li>
            </ul>
            <li>Document</li>
            <ul>
              <li>Encoded as JSON, XML</li>
              <li>Each document has a unique key</li>
              <li>Does not enforce any schema</li>
              <li>Hard to join documents</li>
              <li>Ex. MongoDB, CouchDB</li>
            </ul>
          </ul>
          <li>In which format should data be stored?</li>
          <li>How to store multimodal data? (Data containing both image and text)</li>
        </ul>
        <li>ETL</li>
        <ul>
          <li>Extract - extract data from different data sources</li>
          <li>Transform - data is cleansed and transformed into specific format</li>
          <li>Load - transformed data is loaded into target destination</li>
        </ul>
        <li>Data types</li>
        <ul>
          <li>Structured</li>
          <ul>
            <li>Numerical</li>
            <ul>
              <li>Discrete</li>
              <li>Continuous</li>
            </ul>
            <li>Categorical</li>
            <ul>
              <li>Ordinal - data with sequential order (Ex. movie rating)</li>
              <li>Nominal - no numerical relationship between categories (Ex. male and female)</li>
            </ul>
          </ul>
          <li>Unstructured</li>
          <ul>
            <li>Audio</li>
            <li>Video</li>
            <li>Image</li>
            <li>Text</li>
          </ul>
        </ul>
        <li>Data formats</li>
        <ul>
          <li>JSON (Javascript object notation)</li>
          <ul>
            <li>Human readable, thus takes a lot of space</li>
          </ul>
          <li>CSV (comma separated values)</li>
          <ul>
            <li>Human readable, thus takes a lot of space</li>
            <li>Row major, elements in a row are stored next to each other in memory</li>
            <li>Accessing examples is fast</li>
            <li>Writing data is fast</li>
          </ul>
          <li>Parquet</li>
          <ul>
            <li>Binary, thus not human readable</li>
            <li>Column major, elements in a column are stored next to each other in memory</li>
            <li>Accessing features is fast</li>
          </ul>
        </ul>
        <li>Data flow</li>
        <ul>
          <li>Data passing through databases</li>
          <ul>
            <li>A writes to DB and B reads from DB</li>
            <li>Both A and B need access to DB</li>
            <li>DB cannot be fast for both read and write</li>
          </ul>
          <li>Data passing through services</li>
          <ul>
            <li>A requests data from B and B responds with data</li>
            <li>Ex. REST, RPC</li>
          </ul>
          <li>Data passing through real-time transport (event bus)</li>
          <ul>
            <li>Ex. pubsub (Kafka), message queue (RocketMQ, RabbitMQ)</li>
          </ul>
        </ul>
        <li>Data processing</li>
        <ul>
          <li>Batch processing</li>
          <ul>
            <li>Compute features that do not change often (static features)</li>
            <li>Ex. Spark</li>
          </ul>
          <li>Streaming processing</li>
          <ul>
            <li>Compute features that change frequently (dynamic features)</li>
            <li>Ex. Flink</li>
          </ul>
        </ul>
      </ul>
      <li>Feature engineering</li>
      <ul>
        <li>Too many features</li>
        <ul>
          <li>Increased risk of data leakage</li>
          <li>Can cause overfitting</li>
          <li>Increases memory requirement</li>
          <li>Increases inference latency, especially if prediction requires extracting features</li>
          <li>Useless features become techinical debt</li>
          <ul>
            <li>When data pipeline changes, all affected features need to adjust</li>
            <li>In theory, regularization should reduce weight of useless features to 0. However, model learns faster without useless features</li>
          </ul>
        </ul>
        <li>Missing values</li>
        <ul>
          <li>Types</li>
          <ul>
            <li>Missing not at random</li>
            <ul>
              <li>Missing due to true value itself</li>
              <li>Ex. respondants not disclosing their income and it turns out that those who don't disclose tend to have higher income</li>
            </ul>
            <li>Missing at random</li>
            <ul>
              <li>Missing due to another observed variable</li>
              <li>Ex. age value of centain gender is missing because that gender tend not to disclose their age</li>
            </ul>
            <li>Missing completely at random</li>
            <ul>
              <li>There is no pattern in which the value is missing</li>
            </ul>
          </ul>
          <li>Handle missing values</li>
          <ul>
            <li>Delete - data quantity is reduced</li>
            <ul>
              <li>Delete row to remove a data point</li>
              <li>Delete column to remove a feature</li>
            </ul>
            <li>Imputation - dataset gets noisy</li>
            <ul>
              <li>Fill with default value, mean, median, mode</li>
            </ul>
          </ul>
        </ul>
        <li>Feature scaling</li>
        <ul>
          <li>Feature scaling is not needed if using XGBoost</li>
          <li>Helps gradient descent to find the optimum faster</li>
          <li>Scale inputs to [-1,1]</li>
          <ul>
            <li>Make error function more spherical, thus gradient descent converges faster</li>
            <li>Not scaling inputs impacts regularization</li>
            <li>Outliers are also valid inputs. Do not throw them away</li>
          </ul>
          <li>Normalization (min-max scaling)</li>
          <ul>
            <li>Does not change distribution</li>
            <li>All values are \( [0,1] \)</li>
            <li>\( z = \dfrac{x-x_{min}}{x_{max}-x_{min}} \)</li>
            <li>Outliers can make real data shrunk in very narrow range</li>
          </ul>
          <li>Clipping</li>
          <ul>
            <li>Treat outliers as -1 or 1</li>
            <li>Numerical values are linearly scaled</li>
            <li>Works for uniformly distributed data</li>
          </ul>
          <li>Standardization (z-score normalization)</li>
          <ul>
            <li>Mean is \( 0 \) and standard deviation is \( 1 \)</li>
            <li>\( z = \dfrac{x-\mu}{\sigma} \)</li>
            <li>Works for normally distributed data</li>
          </ul>
          <li>Log scaling</li>
          <ul>
            <li>Mitigate skewness of a feature, so that gradient descent converges faster</li>
            <li>\( z = log(x) \)</li>
            <li>Used when data is neither uniformly or normally distributed</li>
          </ul>
        </ul>
        <li>Bucketing</li>
        <ul>
          <li>Convert numerical feature to categorical feature</li>
        </ul>
        <li>Encoding</li>
        <ul>
          <li>Convert categorical features to numerical feature</li>
          <li>Ex. integer encoding</li>
          <ul>
            <li>Integer value is assigned to each category</li>
            <li>Cannot be used for nominal features</li>
          </ul>
          <li>Ex. one-hot encoding</li>
          <ul>
            <li>Binary value is assigned to each category</li>
            <li>Not suitable for features with high cardinality</li>
            <li>Not suitable when features values are not independent</li>
          </ul>
          <li>Ex. embedding</li>
          <ul>
            <li>Learn N-D vector for each categorial value</li>
            <li>Just another hidden layer in neural network</li>
            <li>When determining embedding dimension, hyperparameter tune between these two variables</li>
            <ul>
              <li>Fourth root of the total number of unique categorical elements</li>
              <li>1.6 times the square root of the number of unique elements in the category, no less than 600</li>
            </ul>
          </ul>
        </ul>
        <li>Feature hashing</li>
        <ul>
          <li>Suitable when these problems exist</li>
          <ul>
            <li>Vocabulary is incomplete</li>
            <li>Categorical variable has high cardinality</li>
            <li>Cold start problem</li>
          </ul>
          <li>Ex. airport_id in USA where there are 347 of them</li>
          <ul>
            <li>Training data may not contain all airport_id</li>
            <li>347 results in high cardinality</li>
            <li>New airport will get built</li>
          </ul>
          <li>Convert categorical input to unique string, then apply hashing on the string</li>
          <li>The number of buckets can be treated as a hyperparameter</li>
          <li>Cons</li>
          <ul>
            <li>Model accuracy will suffer (especially when the distribution of categorical input is highly skewed)</li>
          </ul>
        </ul>
        <li>Feature cross</li>
        <ul>
          <li>Used when model complexity is insufficient to learn feature relationships</li>
          <li>Concatenate categorical features to create combinations of feature values</li>
          <li>Should never be used for numerical feature as it will result in infinite sparcity</li>
          <ul>
            <li>Numerical features can be bucketized to become categorical features before applying feature cross</li>
          </ul>
          <li>Should not cross two features that are highly correlated</li>
        </ul>
        <li>Multimodal input</li>
        <ul>
          <li>Combine different types of inputs (Ex. numerical, one-hot, embedding, etc) into one representation</li>
        </ul>
      </ul>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: Machine Learning System Design Interview, Ali Aminian & Alex Xu | Machine Learning Design Patterns, Valliappa Lakshmanan & Sara Robinson & Michael Munn | Designing Machine Learning Systems, Chip Huyen
  </div>
</div>

<div class="card mb-4" id="machine-learning-system-design-">
  <div class="card-body">
    <h2 class="card-title">Model development</h2>
    <ul>
      <li>Model selection</li>
      <ul>
        <li>Establish baseline</li>
        <ul>
          <li>Ex. recommend most popular video</li>
        </ul>
        <li>Experiment with simple models</li>
        <ul>
          <li>Logistic regression</li>
        </ul>
        <li>Try complex models</li>
        <ul>
          <li>Deep neural network</li>
        </ul>
        <li>Ensemble if needed</li>
        <ul>
          <li>Bagging, boosting, stacking</li>
        </ul>
        <li>Model option</li>
        <ul>
          <li>Logistic regression</li>
          <li>Linear regression</li>
          <li>Decision trees</li>
          <li>Gradient boosted decision trees and random forests</li>
          <li>Support vector machine</li>
          <li>Naive bayes</li>
          <li>Factorization machine (FM)</li>
        </ul>
        <li>Model consideration</li>
        <ul>
          <li>Amount of data</li>
          <li>Training speed</li>
          <li>Number of parameters and memory requirement</li>
          <li>Hyperparameters and how to tune them</li>
          <li>Continual learning requirement</li>
          <li>Compute requirement</li>
          <li>Latecy during inference</li>
          <li>Model interpretability</li>
        </ul>
      </ul>
      <li>Model training</li>
      <ul>
        <li>Construct dataset</li>
        <ul>
          <li>Collect raw data</li>
          <li>Identify features and labels</li>
          <ul>
            <li>Hand labeling - expensive, slow, data privary issue, introduce bias, require domain knowledge</li>
            <li>Natural labeling - ground truth labels are inferred</li>
            <li>Handling insufficient lables</li>
            <ul>
              <li>Weak supervision</li>
              <ul>
                <li>Use heuristics to label data</li>
                <li>Labeled data is noisy</li>
              </ul>
              <li>Semi-supervision</li>
              <ul>
                <li>Use structural assumptions to generate new labels based on initial labels</li>
                <li>Purturbation-based method</li>
                <ul>
                  <li>Assumption is that small purturbations to a sample should not change its label</li>
                  <li>Purturbation can be directly applied to the samples</li>
                  <ul>
                    <li>Ex. adding white noise to images</li>
                  </ul>
                  <li>Purturbation can be applied to the representation of samples</li>
                  <ul>
                    <li>Ex. adding small random values to embeddings of words</li>
                  </ul>
                </ul>
              </ul>
              <li>Transfer learning</li>
              <ul>
                <li>Ex. language model</li>
                <ul>
                  <li>Does not require labeled data and can be trained on any text</li>
                  <li>Given a sequence of tokens, predict the next token</li>
                  <ul>
                    <li>Ex. "I bought NVIDIA shares because I believe in the importance of"</li>
                    <li>Language model might output "hardware" or "GPU" as the next token</li>
                  </ul>
                  <li>The trained model can be used for downstream tasks</li>
                  <ul>
                    <li>Ex. sentiment analysis, intent detection, question answering</li>
                  </ul>
                  <li></li>
                </ul>
              </ul>
              <li>Active learning</li>
              <ul>
                <li>Model chooses which data samples to learn from</li>
              </ul>
            </ul>
          </ul>
          <li>Select sampling strategy</li>
          <ul>
            <li>Convenience sampling</li>
            <ul>
              <li>Samples are selected based on their availability</li>
            </ul>
            <li>Snowball sampling</li>
            <ul>
              <li>Samples are selected based on existing samples</li>
            </ul>
            <li>Stratified sampling</li>
            <ul>
              <li>Divide population into groups and sample from each group separately</li>
              <li>Each group is called stratum</li>
              <li>Problem is when one sample belongs to multiple groups (Ex. multilabel tasks)</li>
            </ul>
            <li>Weighted sampling</li>
            <ul>
              <li>If there are three samples and want them to be selected with probabilties 50%, 30%, 20%, give them weights \( 0.5, 0.3, 0.2 \)</li>
            </ul>
            <li>Reservior sampling</li>
            <ul>
              <li>Useful for streaming data</li>
              <li>Put the first k elements into the reservior</li>
              <li>For each incoming \( n^{\text{th}} \) element, generate a random number \( i \) such that \( 1 \le i \le n \)</li>
              <ul>
                <li>If \( 1 \le i \le k \), replace \( i^{\text{th}} \) element in the reservoir with \( n^{\text{th}} \) element</li>
                <li>Else, do nothing</li>
              </ul>
              <li>Then, each incoming \( n^{\text{th}} \) element has \( \frac{k}{n} \) probability of being in the reservoir</li>
            </ul>
            <li>Importance sampling</li>
            <ul>
              <li>Assume we want to sample from \( P(x) \) but \( P(x) \) is really expensive to sample from</li>
              <li>The, sample from \( Q(x) \) instead and weigh this sample by \( \frac{P(x)}{Q(x)} \)</li>
            </ul>
          </ul>
          <li>Split data</li>
          <ul>
            <li>Split must be repeatable</li>
            <ul>
              <li>Use fixed seed number</li>
              <li>If distributed training environment, store the splitted data</li>
            </ul>
          </ul>
          <li>Address imbalance</li>
          <ul>
            <li>Resample training data - oversample under-represented class or undersample over-represented class</li>
            <li>Alter loss function - give more weights to data points from minority class</li>
            <ul>
              <li>Class-balanced loss</li>
              <li>Focal loss</li>
            </ul>
          </ul>
          <li>Watch for data leakage</li>
          <ul>
            <li>Splitting time-correlated data randomly instead of by time</li>
            <ul>
              <li>Should always train on data from \( 0 \) to time \( t \) and evaluate it on \( t+1 \)</li>
              <li>If random split, information from future is leaked into training process</li>
            </ul>
            <li>Scaling before splitting</li>
            <ul>
              <li>Do not use entire training data to generate global statistics before splitting into bins</li>
              <li>If not, it leaks the mean and variance of test set into training process</li>
              <li>Always split data first, then apply scaling</li>
            </ul>
            <li>Filling in missing data with statistics from the test split</li>
            <ul>
              <li>Leaking occurs when mean or median is calculated using entire data instead of just the train split</li>
            </ul>
            <li>Poor handling of data duplication before splitting</li>
            <ul>
              <li>Same samples might appear in both train and validation/test set</li>
              <li>If oversampling data, do it after splitting</li>
            </ul>
            <li>Group leakage</li>
            <ul>
              <li>Ex. CT scans that are a week apart with the same lables, one in train set and the other in test set</li>
            </ul>
            <li>Detect data leakage</li>
            <ul>
              <li>Measure the predictive power of each feature (or a set of features) on target variable</li>
              <li>If a feature has high correlation, investigate</li>
              <li></li>
            </ul>
          </ul>
        </ul>
        <li>Choose loss function</li>
        <ul>
          <li>Regression</li>
          <ul>
            <li>MSE</li>
            <li>MAE</li>
          </ul>
          <li>Multiclass classification</li>
          <ul>
            <li>Cross-entropy</li>
          </ul>
          <li>Binary classification</li>
          <ul>
            <li>Log loss</li>
          </ul>
          <li>?</li>
          <ul>
            <li>Huber loss</li>
          </ul>
        </ul>
        <li>Regularization</li>
        <ul>
          <li>L1</li>
          <li>L2</li>
          <li>Entropy regularization</li>
          <li>K-fold CV</li>
          <li>Dropout</li>
        </ul>
        <li>Optimization</li>
        <ul>
          <li>SGD</li>
          <li>AdaGrad</li>
          <li>Momentum</li>
          <li>RMSProp</li>
        </ul>
        <li>Activation</li>
        <ul>
          <li>ELU</li>
          <li>ReLU</li>
          <li>Tanh</li>
          <li>Sigmoid</li>
        </ul>
        <li>Transfer learning</li>
        <ul>
          <li>Feature extraction</li>
          <ul>
            <li>Final layer contains the classification label or output specific to the prediction task</li>
            <li>Remove the final layer, freeze the weights, and replace the final layer with the output for the specialized prediction task</li>
            <li>Layer before the last layer is referred as bottleneck layer</li>
            <li>Suitable when dataset is small and computation budget is low</li>
            <li>Works better when the prediction taks is different than the pre-trained model</li>
          </ul>
          <li>Fine-tuning</li>
          <ul>
            <li>Leave initial layers frozen and update weights on further layers</li>
            <li>Suitable when dataset is large and computation budget is high</li>
            <li>Works better when the prediction taks is similar to the pre-trained model</li>
          </ul>
        </ul>
        <li>Checkpoints</li>
        <ul>
          <li>Save the model state at the end of every epoc</li>
          <ul>
            <li>If tree model, save the final rules for each intermediate node and the predicted value for each of the leaf nodes</li>
            <li>If linear model, save the final values of the weights and biases</li>
            <li>If neural network, save the final values of the weights and biases plus activation functions and the weights of the hidden connections</li>
          </ul>
          <li>Model state changes after every batch, but checkpointing at every batch is too expensive</li>
          <li>Fine-tuning - when retraining the model on fresh data, train from a checkpoint</li>
          <li>Instead of this</li>
<pre><code class="python">model.fit(X_train, y_train,
          batch_size=100,
          epochs=15)</code></pre>
          <li>Do this</li>
<pre><code class="python">NUM_TRAINING_EXAMPLES = 1000 * 1000
STOP_POINT = 14.3
TOTAL_TRAINING_EXAMPLES = int(STOP_POINT*NUM_TRAINING_EXAMPLES)
BATCH_SIZE = 100
NUM_CHECKPOINTS = 15
steps_per_epoch = TOTAL_TRAINING_EXAMPLES // (BATCH_SIZE*NUM_CHECKPOINTS)
cp_callback = tf.keras.callbacks.ModelCheckpoint(...)
history = model.fit(trainds,
                    validation_data=evalds,
                    epochs=NUM_CHECKPOINTS,
                    steps_per_epoch =steps_per_epoch,
                    batch_size=BATCH_SIZE,
                    callbacks=[cp_callback])</code></pre>
        </ul>
        <li>Distributed training</li>
        <ul>
          <li>Data parallelism</li>
          <ul>
            <li>Computation is split across different machines</li>
            <li>Different workers train on different subsets of training data</li>
            <li>Synchronous training</li>
            <ul>
              <li>A mini-batch of data is split among each of the separate workers</li>
              <li>Each device performs a forward pass with their portion of mini-batch and computes gradients for each parameter of the model</li>
              <li>These locally computed gradients are collected from each device and aggregated to produce a single gradient update for each parameter</li>
              <li>A central server performs the gradient step according to the gradients received from multiple workers</li>
            </ul>
            <li>Asynchronous training</li>
            <ul>
              <li>A central server computes new parameters periodically based on whichever gradient updates it received since the last computation</li>
              <li>Higher throughput since a slow worker does not block the progression of training steps</li>
              <li>Some splits of mini-batch may be lost during training</li>
            </ul>
          </ul>
          <li>Model parallelism</li>
          <ul>
            <li>Model is split</li>
            <li>Different workers carry out computation for different parts of the model</li>
          </ul>
        </ul>
        <li>Save the transformation applied to convert model inputs into features</li>
        <li>How to overcome underfitting and overfitting (bias and variance)</li>
      </ul>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: Machine Learning System Design Interview, Ali Aminian & Alex Xu | Machine Learning Design Patterns, Valliappa Lakshmanan & Sara Robinson & Michael Munn | Designing Machine Learning Systems, Chip Huyen
  </div>
</div>

<div class="card mb-4" id="machine-learning-system-design-">
  <div class="card-body">
    <h2 class="card-title">Evaluation</h2>
    <ul>
      <li>Offline - during development</li>
      <ul>
        <li>Baselines</li>
        <ul>
          <li>Random baseline</li>
          <li>Simple heuristic</li>
          <li>Zero rule baseline</li>
          <li>Human baseline</li>
          <li>Existing solutions</li>
        </ul>
        <li>Methods</li>
        <ul>
          <li>Purturbation tests</li>
          <li>Invariance tests</li>
          <li>Directional expectation tests</li>
          <li>Model calibration</li>
          <li>Confidence measurement</li>
          <li>Slice-based evaluation</li>
        </ul>
        <li>Examples</li>
        <ul>
          <li>Classification</li>
          <ul>
            <li>Precision</li>
            <li>Recall</li>
            <li>F1 score</li>
            <li>Accuracy</li>
            <li>ROC-AUC</li>
            <li>PR-AUC</li>
            <li>Confusion matrix</li>
          </ul>
          <li>Regression</li>
          <ul>
            <li>MSE</li>
            <li>MAE</li>
            <li>RMSE</li>
          </ul>
          <li>Ranking</li>
          <ul>
            <li>Precision@k</li>
            <li>Recall@k</li>
            <li>MRR</li>
            <li>mAP</li>
            <li>nDCG</li>
          </ul>
          <li>Image generation</li>
          <ul>
            <li>FID</li>
            <li>Inception score</li>
            <li></li>
            <li></li>
            <li></li>
            <li></li>
            <li></li>
          </ul>
          <li>Natural language processing</li>
          <ul>
            <li>BLUE</li>
            <li>METEOR</li>
            <li>ROUGE</li>
            <li>CIDEr</li>
            <li>SPICE</li>
          </ul>
        </ul>
      </ul>
      <li>Online - in production</li>
      <ul>
        <li>Ad click prediction - click-through rate, revenue lift</li>
        <li>Harmful content detection - prevalence, valid appeals</li>
        <li>Video recommendation - click-through rate, total watch time, number of completed videos</li>
        <li>Friend recommendation - number of requests sent per day, number of requests accepted per day</li>
      </ul>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: Machine Learning System Design Interview, Ali Aminian & Alex Xu | Machine Learning Design Patterns, Valliappa Lakshmanan & Sara Robinson & Michael Munn | Designing Machine Learning Systems, Chip Huyen
  </div>
</div>

<div class="card mb-4" id="machine-learning-system-design-">
  <div class="card-body">
    <h2 class="card-title">Deployment, serving, monitoring</h2>
    <ul>
      <li>Deployment</li>
      <ul>
        <li>Cloud vs on-device</li>
        <ul>
          <li>Cloud - simple to deploy, faster inference, fewer constraints</li>
          <li>On-device - no cloud cost, no network latency, more privacy, no internet required</li>
          <li>Two-phase prediction</li>
          <ul>
            <li>A problem is split into two parts</li>
            <li>Smaller and cheaper model deployed on device that can achieve high accuracy</li>
            <li>More complex model on cloud that is triggered only when it is needed</li>
          </ul>
        </ul>
        <li>Model compression</li>
        <ul>
          <li>Knowledge distillation - train small model to mimic larger model</li>
          <li>Pruning - find least useful parameters and set them to zero</li>
          <li>Quantization - use fewer bits to represent parameters</li>
        </ul>
        <li>Deployment strategy</li>
        <ul>
          <li>Shadow deployment</li>
          <ul>
            <li>Deploy new model in parallel with existing model</li>
            <li>Inference from existing model is served to users</li>
            <li>Double number of prediction is needed</li>
          </ul>
          <li>A/B testing</li>
          <ul>
            <li>Deploy new model in parallel with existing model</li>
            <li>Portion of traffic is routed to new model</li>
          </ul>
          <li>Canary release</li>
        </ul>
      </ul>
      <li>Serving</li>
      <ul>
        <li>Prediction pipeline</li>
        <ul>
          <li>Batch prediction</li>
          <ul>
            <li>Only uses batch features</li>
            <li>Less responsive to change in user preference</li>
            <li>Need to know beforehand what needs to be pre-computed</li>
          </ul>
          <li>Online prediction</li>
          <ul>
            <li>May use batch features only or the combination of batch and streaming features</li>
            <li>Model may take long to generate prediction</li>
            <li>Stateless serving function</li>
            <ul>
              <li>Should support millions of requests per second</li>
              <li>Model should be packaged and deployed as stateless function</li>
            </ul>
          </ul>
        </ul>
        <li>Continual learning</li>
        <ul>
          <li>Stateless retraining</li>
          <ul>
            <li>Model is trained from scratch</li>
            <li>Required when model architecture or features change</li>
          </ul>
          <li>Stateful retraining</li>
          <ul>
            <li>Model continues training on new data</li>
            <li>Called fine-tuning or incremental learning</li>
          </ul>
          <li>Challenges</li>
          <ul>
            <li>Fresh data access challenge</li>
            <li>Evaluation challenge</li>
            <li>Algorithm challenge</li>
          </ul>
          <li>Stages</li>
          <ul>
            <li>Manual, stateless retraining</li>
            <li>Automated retraining</li>
            <li>Automated, stateful training</li>
            <li>Continual learning</li>
          </ul>
        </ul>
        <li>Two-phase prediction</li>
        <ul>
          <li>Split large use case into two phases</li>
          <li>Only the simpler phase should be carried out on the edge</li>
        </ul>
        <li>Feature store</li>
        <ul>
          <li>Store precomputed features to be re-used</li>
        </ul>
        <li>Model versioning</li>
      </ul>
      <li>ML failure</li>
      <ul>
        <li>Edge cases</li>
        <li>Degenerate feedback loops</li>
        <ul>
          <li>Detecting</li>
          <li>Correcting</li>
        </ul>
      </ul>
      <li>Data distribution shift</li>
      <ul>
        <li>Types</li>
        <ul>
          <li>Covariance shift</li>
          <ul>
            <li></li>
          </ul>
          <li>Label shift</li>
          <ul>
            <li></li>
          </ul>
          <li>Concept drift</li>
          <ul>
            <li></li>
          </ul>
        </ul>
        <li>Detecting</li>
        <li>Solution</li>
        <ul>
          <li>Train on large dataset</li>
          <li>Retrain regularly</li>
        </ul>
      </ul>
      <li>Monitoring</li>
      <ul>
        <li>Accuracy</li>
        <ul>
          <li>Model performance</li>
          <li>Ex. accuracy, precision, recall, F1 score</li>
        </ul>
        <li>Predictions</li>
        <ul>
          <li>Distribution drift of prediction</li>
          <li>Ex. prediction PSI</li>
        </ul>
        <li>Features</li>
        <ul>
          <li>Distribution drift of features</li>
          <li>Ex. feature PSI</li>
        </ul>
        <li>Raw inputs</li>
      </ul>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: Machine Learning System Design Interview, Ali Aminian & Alex Xu | Machine Learning Design Patterns, Valliappa Lakshmanan & Sara Robinson & Michael Munn | Designing Machine Learning Systems, Chip Huyen
  </div>
</div>

<div class="card mb-4" id="machine-learning-system-design-">
  <div class="card-body">
    <h2 class="card-title">Infrastructure</h2>

    <!-- <h3 class="card-title">High level design</h3>
    <img class="img-fluid" class="card-img-top" src="/img/system-design/ml-highlevel.png" alt="Card image cap">

    <h3 class="card-title">Component design</h3>
    <img class="img-fluid" class="card-img-top" src="/img/system-design/ml-development-v1.png" alt="Card image cap">
    <img class="img-fluid" class="card-img-top" src="/img/system-design/ml-training-v1.png" alt="Card image cap">
    <img class="img-fluid" class="card-img-top" src="/img/system-design/ml-inference-batch-v1.png" alt="Card image cap">
    <ul>
      <li>Performce for out-of-sample and out-of-time.</li>
      <li>Features should be mostly unchanged when input data is slighly perturbed. (Could be challgenging when data contains similar/duplicate features)</li>
      <li>Interpretation should not be depending on training sample choices.</li>
    </ul>

    <h3 class="card-title">Problem definition</h3>
    <ul>
      <li>Construct ground truth.</li>
      <li>Metrics.</li>
    </ul>

    <h3 class="card-title">Data prep</h3>
    <ul>
      <li>Proportion of null/zero values across columns.</li>
      <li>Columns that should be rejected due to zero or null values.</li>
      <li>Categorical columns with high cardinality.</li>
      <li>Columns with high sparcity.</li>
      <li>Duplicate columns.</li>
      <li>Compute per-month count of rows and selected attribute to see if they make sense.</li>
      <li>Verify that join keys are consistent across tables.</li>
      <li>Verify that labels/values to apply exclusions are available in the tables.</li>
      <li>Verify that labels/values to apply ground truth are available in the tables.</li>
      <li>Rectify missing values in columns.</li>
      <ul>
      <li>Random missing value - value is missing for unexplained reason.</li>
      <li>Structural missing value - value is missing for a reason. (For example, the fact that it is missing means something)</li>
      <li>Engineered missing value - feature engineering introduced the missing values. (For example, divide by zero)</li>
      <li>Table join mismatch - IDs in one table were missing in another table, so missing values are created after join.</li>
    </ul>
    </ul>

    <h3 class="card-title">Data profiling</h3>
    <ul>
      <li>General statistics.</li>
      <li>Columns with large portion of missing values. (For example, > 10%)</li>
      <ul>
        <li>These columns should generally be retained for they may turn out to be quite predictive.</li>
        <li>XGBoost has build-in capability to handle missing values.</li>
      </ul>
      <li>Similar/duplicate columns. (For example, prefer one feature over another? Can use univariate analysis)</li>
      <li>Columns with consistant values should be removed because they provide no signal.</li>
      <li>Remove sensitive features from model inputs. (For example, age / gender)</li>
      <li>Data imputation (For example, NULL to 0) should rarely happens when feature is critical and will undergo feature engineering and treatment is known.</li>
    </ul>

    <h3 class="card-title">Exclusion</h3>
    <ul>
      <li>Exclude rows based on certain attributes. (This is driven by business reasons)</li>
    </ul>

    <h3 class="card-title">Ground truth construction</h3>

    <h3 class="card-title">Data split</h3>
    <ul>
      <li>Training: In-Time and In-Sample. Split into 5-folds for cross-validation.</li>
      <li>Testing: Out-of-Time test and In-Time & Out-of-Sample test.</li>
      <li>If a data about person is seen at many different dates, all data about that person must be assigned to the same split. (either Out-of-Sample or one of the folds) Otherwise, there is information leak bwetween training and validation.</li>
      <li>Final model is retrained with all 5 folds combined.</li>
    </ul>

    <h3 class="card-title">Data representativeness</h3>
    <ul>
      <li>Separate datasets by ground truth labels.</li>
      <li>Perform univariate analysis. For example, look at the distribution of samples of a key feature for both the entire dataset and the partitioned dataset. Repeat for all key features.</li>
      <li>All paritioned datasets must include sufficient volume of each ground truth labels across key features.</li>
      <li>Bin size may be increased or bins could be combined as a result of above analysis.</li>
    </ul>

    <h3 class="card-title">Model development</h3>
    <ul>
      <li>Define metric to evaluate the model.</li>
      <li>Score the model by the desired metric.</li>
      <li>Check model performace by month to check seasonality.</li>
      <li>Kubernetes based</li>
      <ul>
        <li>Make changes to code.</li>
        <li>Build, test, package the code.</li>
        <li>Build an image which includes the code package.</li>
        <li>Deploy the image to Kubenetes using tools like Skaffold / Helm.</li>
        <li>Run commands inside the image to execute the code.</li>
      </ul>
      <li>Databricks based</li>
      <ul>
        <li>Make changes to code.</li>
        <li>Build, test, package, publish the code.</li>
        <li>Import the code from the notebook.</li>
        <li>Write additional code on the notebook as needed.</li>
        <li>Run commands on the notebook execute the code.</li>
      </ul>
    </ul>

    <h3 class="card-title">Feature selection</h3>
    <ul>
      <li>Start with initial set of feature ~ 1500</li>
      <li>First gate</li>
      <ul>
        <li>Data preparation</li>
        <li>Feature engineering</li>
      </ul>
      <li>After exclusions ~ 1000</li>
      <li>First stage</li>
      <ul>
        <li>Assess each feature individually.</li>
        <li>Select a metric.</li>
        <li>Use 4-folds for training and 1 fold for validation.</li>
        <li>Train a shallow model with a single feature as input and compute performance in validation set.</li>
        <li>Rank individual features.</li>
        <li>Then, train a model wtih all features.</li>
        <li>Then again, rank the features.</li>
      </ul>
      <ul>
        <li>Perform recursive feature search.</li>
        <li>Use 4-folds for training and 1 fold for validation. (Validation fold must be different from previous step)</li>
        <li>For each feature in top N features, train a model and score performance on validation set.</li>
        <li>Add the best scoring features to the candidate features.</li>
      </ul>
      <li>Candidate features ~ 100</li>
      <li>Second gate</li>
      <ul>
        <li>Highly correlated features should be justified or redundant features should be removed.</li>
        <li>Future retraining starts from the candidate features.</li>
      </ul>
      <li>Second stage</li>
      <ul>
        <li>Perform 5-fold cross validation with each candidate feature.</li>
        <li>Compute shapley values on the validation set.</li>
        <li>Select stable features via union of top features across the folds.</li>
      </ul>
      <li>Stable features ~ 40</li>
    </ul>

    <h3 class="card-title">Hyperparameter tuning</h3>
    <ul>
      <li>Ex. grid search, random search, bayesian optimization.</li>
    </ul>

    <h3 class="card-title">Final model training</h3>
    <ul>
      <li>Train the model using stable features and tuned hyperparameters.</li>
      <li>Evaluate on OOS (seasonality) and OOT (final evaluation) test sets.</li>
    </ul>

    <h3 class="card-title">Monitoring</h3>
    <ul>
      <li>Partial dependency plot (PDP) assesses marginal effect of a feature to the model output.</li>
      <li>Indvidual conditional expectation (ICDE) is equivalent to PDP but for an individual data point.</li>
      <li>Feature contribution is assessed via shapley values.</li>
    </ul> -->
  </div>
  <div class="card-footer text-muted">
    Reference: Machine Learning System Design Interview, Ali Aminian & Alex Xu | Machine Learning Design Patterns, Valliappa Lakshmanan & Sara Robinson & Michael Munn | Designing Machine Learning Systems, Chip Huyen
  </div>
</div>
<!-- Machine learning system design END -->

</div> <!-- /.col-md-12 -->
</div> <!-- /.row -->
</div> <!-- /.container -->

<include src="/footer.html"></include>

</body>

</html>