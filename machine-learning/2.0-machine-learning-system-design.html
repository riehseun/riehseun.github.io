<!DOCTYPE html>

<html lang="en">

<head>

<!-- Metadata -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="">
<meta name="author" content="Seungmoon Rieh">
<meta name="keywords" content="">

<!-- Title and image -->
<title>Seungmoon Rieh</title>
<link href="/img/seungmoonrieh.jpg" rel="icon">

<!-- CSS -->
<link href="/css/bootstrap.min.css" rel="stylesheet">
<link href="/css/monokai-sublime.css" rel="stylesheet">
<link href="/css/site.css" rel="stylesheet">

<!-- JavaScript -->
<script src="/js/jquery.min.js" type="text/javascript"></script>
<script src="/js/bootstrap.bundle.min.js" type="text/javascript"></script>
<script src="/js/highlight.pack.js" type="text/javascript"></script>
<script src="/js/include_html.js" type="text/javascript"></script>
<script src="/js/mathjax/tex-chtml.js" type="text/javascript"></script>
<script src="/js/site.js" type="text/javascript"></script>

</head>

<body>

<include src="/header.html"></include>

<div class="container">
<div class="row">
<div class="col-md-12">
<h1 class="my-4">Machine learning</h1>

<!-- Machine learning system design BEGIN -->
<div class="card mb-4" id="machine-learning-system-design">
  <div class="card-body">
    <h2 class="card-title">Machine learning system design</h2>
    <ul class="list-unstyled mb-0">
      <li><a href="#machine-learning-system-design-">Machine learning system design</a></li>
      <li><a href="#machine-learning-system-design-">Requirement</a></li>
      <li><a href="#machine-learning-system-design-">Problem</a></li>
      <li><a href="#machine-learning-system-design-">Data preparation</a></li>
      <li><a href="#machine-learning-system-design-">Model development</a></li>
      <li><a href="#machine-learning-system-design-">Evaluation</a></li>
      <li><a href="#machine-learning-system-design-">Serving</a></li>
    </ul>
  </div>
</div>

<div class="card mb-4" id="machine-learning-system-design-">
  <div class="card-body">
    <h2 class="card-title">Machine learning system design</h2>
    <ul>
      <li>Talking points</li>
      <ul>
        <li>Discuss business requiremnent</li>
        <li>Discuss machine learning problem</li>
        <li>Discuss business metrics</li>
        <li>Discuss online metrics</li>
        <li>Discuss architectural component</li>
        <li>Discuss training data</li>
        <li>Discuss offline metrics</li>
        <li>Discuss features</li>
        <li>Discuss model</li>
      </ul>
    </ul>

    <h3 class="card-title">Design problems</h3>
    <ul>
      <li>Reference</li>
      <ul>
        <li><a href="/machine-learning/2.0-machine-learning-design-patterns.html">Machine learning design patterns</a></li>
        <li><a href="/machine-learning/2.0-designing-machine-learning-systems.html">Designing machine learning systems</a></li>
      </ul>
      <li>Practice</li>
      <ul>
        <li><a href="/machine-learning/2.3-instagram.html">Design instragram</a></li>
        <li><a href="/machine-learning/2.3-chatbot.html">Design chatbot</a></li>
        <li><a href="/machine-learning/2.3-banking.html">Design banking</a></li>
      </ul>
      <li>Example</li>
      <ul>
        <li><a href="/machine-learning/2.1-visual-search-system.html">Design visual search system</a></li>
        <li><a href="/machine-learning/2.1-google-street-view-blurring-system.html">Design google street view blurring system</a></li>
        <li><a href="/machine-learning/2.1-youtube-video-search.html">Design youtube video search</a></li>
        <li><a href="/machine-learning/2.1-harmful-content-detection.html">Design harmful content detection</a></li>
        <li><a href="/machine-learning/2.1-video-recommendation-system.html">Design video recommendation system</a></li>
        <li><a href="/machine-learning/2.1-event-recommendation-system.html">Design event recommendation system</a></li>
        <li><a href="/machine-learning/2.1-ad-click-prediction-on-social-platform.html">Design ad click prediction on social platform</a></li>
        <li><a href="/machine-learning/2.1-similar-listings-on-vacation-rental-platforms.html">Design similar listings</a></li>
        <li><a href="/machine-learning/2.1-personalized-news-feed.html">Design personalized newsfeed</a></li>
        <li><a href="/machine-learning/2.1-people-you-may-know.html">Design people you may know</a></li>
        <li><a href="/machine-learning/2.2-gmail-smart-compose.html">Design google smart compose</a></li>
        <li><a href="/machine-learning/2.2-google-translate.html">Design google translate</a></li>
        <li><a href="/machine-learning/2.2-chatgpt.html">Design ChatGPT</a></li>
        <li><a href="/machine-learning/2.2-image-captioning.html">Design image captioning</a></li>
        <li><a href="/machine-learning/2.2-rag.html">Design RAG</a></li>
        <li><a href="/machine-learning/2.2-realistic-face-generation.html">Design realistic face generation</a></li>
        <li><a href="/machine-learning/2.2-high-resolution-image-synthesis.html">Design high-resolution image synthesis</a></li>
        <li><a href="/machine-learning/2.2-text-to-image-generation.html">Design text-to-image generation</a></li>
        <li><a href="/machine-learning/2.2-personalized-headshot-generation.html">Design personalized headshot generation</a></li>
        <li><a href="/machine-learning/2.2-text-to-video-generation.html">Design text-to-video generation</a></li>
      </ul>
    </ul>
    <h3 class="card-title">Other problems</h3>
    <ul>
      <li>Explain difference between batch serving and online serving</li>
      <li>Explain strategies for re-training the model</li>
      <li>Explain how to use model registry and model versioning</li>
      <li>Explain how to monitor the model</li>
      <li>Explain how to orchestrate workflow using Airflow/Kubeflow"</li>
      <li>Explain how to track experiments using MLflow</li>
      <li>Explain how to achieve scaling, load-balancing, fast response for LLM application</li>
      <li>Explain how to use caching to improve LLM performance</li>
      <li>Explain how to reduce model size and optimize for deployment on smart phones</li>
      <li>Explain trade-off between GPU and TPU</li>
      <li>Explain how to monitor LLM in production</li>
      <li>Explain how vector database work</li>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: 
  </div>
</div>

<div class="card mb-4" id="machine-learning-system-design-">
  <div class="card-body">
    <h2 class="card-title">Requirement</h2>
    <ul>
      <li>Business objective</li>
      <ul>
        <li>What is purpose of the system?</li>
        <li>Should the model output be personalized to each user?</li>
        <li>Should the model output handle bias and fairness concern?</li>
        <li>Should the system support multiple languages?</li>
      </ul>
      <li>System features</li>
      <ul>
        <li>What kind of user actions are available?</li>
      </ul>
      <li>Data</li>
      <ul>
        <li>Does the system have training data available? What is the size of dataset?</li>
        <li>Should the system collect data from user interactions?</li>
      </ul>
      <li>Constraints</li>
      <ul>
        <li>How much computing power does the system have?</li>
        <li>Should the model be deployed to mobile devices?</li>
        <li>Should the model be continuouly trained?</li>
      </ul>
      <li>Scale</li>
      <ul>
        <li>How many total users are there? How many daily active users are there?</li>
        <li>How many inferences does each user make daily?</li>
      </ul>
      <li>Performance</li>
      <ul>
        <li>What should be the latency of prediction?</li>
        <li>Can the inference be batch or should it be in real-time?</li>
      </ul>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: Machine Learning System Design Interview, Ali Aminian & Alex Xu
  </div>
</div>

<div class="card mb-4" id="machine-learning-system-design-">
  <div class="card-body">
    <h2 class="card-title">Problem</h2>

    <h3 class="card-title">ML problem</h3>
    <ul>
      <li>What is input and output of model</li>
    </ul>

    <h3 class="card-title">ML category</h3>
    <ul>
      <li>Discrimitive</li>
      <ul>
        <li>Supervised learning</li>
        <ul>
          <li>Neural networks</li>
          <ul>
            <li>Linear network</li>
            <ul>
              <li>Linear regression (regression only)</li>
              <li>Logistic regression (classification only - binary, multi-class, multi-label where example belonging to more than one class)</li>
            </ul>
            <li>Deep neural networks</li>
            <li>Convolutional neural networks</li>
            <li>Recurrent neural networks</li>
          </ul>
          <li>Decision trees</li>
          <li>k-nearest neighbors (KNN)</li>
        </ul>
        <li>Unsupervised learning</li>
        <ul>
          <li>Clustering</li>
          <ul>
            <li>Density-based spacial clustering with noise (DBSCAN)</li>
            <li>K-means clustering - partition points into K subsets, compute centroid of current partitioning, assign each point to cluster</li>
          </ul>
          <li>Dimension reduction</li>
          <ul>
            <li>Principle component analysis (PCA)</li>
            <li>Isomap</li>
          </ul>
        </ul>
      </ul>
      <li>Generative</li>
      <ul>
        <li>Text</li>
        <ul>
          <li>Pre-training</li>
          <ul>
            <li>Encoder-decoder</li>
            <li>Encoder only</li>
            <li>Decoder only</li>
          </ul>
          <li>Fine-tuning</li>
          <li>Retrieval augmented generation</li>
        </ul>
        <li>Image</li>
        <ul>
          <li>Variational autoencoders (VAEs)</li>
          <li>Generative adversarial networks (GANs)</li>
          <li>Diffusion models</li>
          <li>Autoregressive models</li>
        </ul>
      </ul>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: Machine Learning System Design Interview, Ali Aminian & Alex Xu | Designing Machine Learning Systems, Chip Huyen
  </div>
</div>

<div class="card mb-4" id="machine-learning-system-design-">
  <div class="card-body">
    <h2 class="card-title">Data preparation</h2>

    <h3 class="card-title">Data engineering</h3>
    <ul>
      <li>Training data</li>
      <ul>
        <li>Collect raw data</li>
        <ul>
          <li>Data source</li>
          <ul>
            <li>How data is collected?</li>
            <li>How clean is data?</li>
            <li>Can data source be trusted?</li>
            <li>Is data user-generated or system-generated?</li>
            <li>How often new data comes in?</li>
            <li>Can data be stored in servers or data cannot leave user device?</li>
            <li>Does data need to be tokenized?</li>
          </ul>
          <li>Data types</li>
          <ul>
            <li>Structured</li>
            <ul>
              <li>Numerical</li>
              <ul>
                <li>Discrete</li>
                <li>Continuous</li>
              </ul>
              <li>Categorical</li>
              <ul>
                <li>Ordinal - data with sequential order (Ex. movie rating)</li>
                <li>Nominal - no numerical relationship between categories (Ex. male and female)</li>
              </ul>
            </ul>
            <li>Unstructured</li>
            <ul>
              <li>Audio</li>
              <li>Video</li>
              <li>Image</li>
              <li>Text</li>
            </ul>
          </ul>
          <li>Data formats</li>
          <ul>
            <li>JSON (Javascript object notation)</li>
            <ul>
              <li>Human readable, thus takes a lot of space</li>
            </ul>
            <li>CSV (comma separated values)</li>
            <ul>
              <li>Human readable, thus takes a lot of space</li>
              <li>Row major, elements in a row are stored next to each other in memory</li>
              <li>Accessing examples is fast</li>
              <li>Writing data is fast</li>
            </ul>
            <li>Parquet</li>
            <ul>
              <li>Binary, thus not human readable</li>
              <li>Column major, elements in a column are stored next to each other in memory</li>
              <li>Accessing features is fast</li>
            </ul>
          </ul>
          <li>Data flow</li>
          <ul>
            <li>Data passing through databases</li>
            <ul>
              <li>A writes to DB and B reads from DB</li>
              <li>Both A and B need access to DB</li>
              <li>DB cannot be fast for both read and write</li>
            </ul>
            <li>Data passing through services</li>
            <ul>
              <li>A requests data from B and B responds with data</li>
              <li>Ex. REST, RPC</li>
            </ul>
            <li>Data passing through real-time transport (event bus)</li>
            <ul>
              <li>Ex. pubsub (Kafka), message queue (RocketMQ, RabbitMQ)</li>
            </ul>
          </ul>
        </ul>
        <li>Identify features and labels</li>
        <ul>
          <li>Hand labeling</li>
          <ul>
            <li>Expensive, slow, data privary issue, introduce bias, require domain knowledge</li>
          </ul>
          <li>Natural labeling</li>
          <ul>
            <li>Ground truth labels are inferred</li>
          </ul>
          <li>Handling insufficient lables</li>
          <ul>
            <li>Weak supervision</li>
            <ul>
              <li>Use heuristics to label data</li>
              <li>Labeled data is noisy</li>
            </ul>
            <li>Semi-supervision</li>
            <ul>
              <li>Use structural assumptions to generate new labels based on initial labels</li>
              <li>Purturbation-based method</li>
              <ul>
                <li>Assumption is that small purturbations to a sample should not change its label</li>
                <li>Purturbation can be directly applied to the samples</li>
                <ul>
                  <li>Ex. adding white noise to images</li>
                </ul>
                <li>Purturbation can be applied to the representation of samples</li>
                <ul>
                  <li>Ex. adding small random values to embeddings of words</li>
                </ul>
              </ul>
            </ul>
            <li>Transfer learning</li>
            <ul>
              <li>Ex. language model</li>
              <ul>
                <li>Does not require labeled data and can be trained on any text</li>
                <li>Given a sequence of tokens, predict the next token</li>
                <ul>
                  <li>Ex. "I bought NVIDIA shares because I believe in the importance of"</li>
                  <li>Language model might output "hardware" or "GPU" as the next token</li>
                </ul>
                <li>The trained model can be used for downstream tasks</li>
                <ul>
                  <li>Ex. sentiment analysis, intent detection, question answering</li>
                </ul>
              </ul>
            </ul>
            <li>Active learning</li>
            <ul>
              <li>Model chooses which data samples to learn from</li>
            </ul>
          </ul>
        </ul>
        <li>Select sampling strategy</li>
        <ul>
          <li>Convenience sampling</li>
          <ul>
            <li>Samples are selected based on their availability</li>
          </ul>
          <li>Snowball sampling</li>
          <ul>
            <li>Samples are selected based on existing samples</li>
          </ul>
          <li>Stratified sampling</li>
          <ul>
            <li>Divide population into groups and sample from each group separately</li>
            <li>Each group is called stratum</li>
            <li>Problem is when one sample belongs to multiple groups (Ex. multilabel tasks)</li>
          </ul>
          <li>Weighted sampling</li>
          <ul>
            <li>If there are three samples and want them to be selected with probabilties 50%, 30%, 20%, give them weights \( 0.5, 0.3, 0.2 \)</li>
          </ul>
          <li>Reservior sampling</li>
          <ul>
            <li>Useful for streaming data</li>
            <li>Put the first k elements into the reservior</li>
            <li>For each incoming \( n^{\text{th}} \) element, generate a random number \( i \) such that \( 1 \le i \le n \)</li>
            <ul>
              <li>If \( 1 \le i \le k \), replace \( i^{\text{th}} \) element in the reservoir with \( n^{\text{th}} \) element</li>
              <li>Else, do nothing</li>
            </ul>
            <li>Then, each incoming \( n^{\text{th}} \) element has \( \frac{k}{n} \) probability of being in the reservoir</li>
          </ul>
          <li>Importance sampling</li>
          <ul>
            <li>Assume we want to sample from \( P(x) \) but \( P(x) \) is really expensive to sample from</li>
            <li>The, sample from \( Q(x) \) instead and weigh this sample by \( \frac{P(x)}{Q(x)} \)</li>
          </ul>
        </ul>
        <li>Split data</li>
        <ul>
          <li>Split must be repeatable</li>
          <ul>
            <li>Use fixed seed number</li>
            <li>If distributed training environment, store the splitted data</li>
          </ul>
        </ul>
        <li>Address imbalance</li>
        <ul>
          <li>Resample training data - oversample under-represented class or undersample over-represented class</li>
          <li>Alter loss function - give more weights to data points from minority class</li>
          <ul>
            <li>Class-balanced loss</li>
            <li>Focal loss</li>
          </ul>
        </ul>
        <li>Watch for data leakage</li>
        <ul>
          <li>Splitting time-correlated data randomly instead of by time</li>
          <ul>
            <li>Should always train on data from \( 0 \) to time \( t \) and evaluate it on \( t+1 \)</li>
            <li>If random split, information from future is leaked into training process</li>
          </ul>
          <li>Scaling before splitting</li>
          <ul>
            <li>Do not use entire training data to generate global statistics before splitting into bins</li>
            <li>If not, it leaks the mean and variance of test set into training process</li>
            <li>Always split data first, then apply scaling</li>
          </ul>
          <li>Filling in missing data with statistics from the test split</li>
          <ul>
            <li>Leaking occurs when mean or median is calculated using entire data instead of just the train split</li>
          </ul>
          <li>Poor handling of data duplication before splitting</li>
          <ul>
            <li>Same samples might appear in both train and validation/test set</li>
            <li>If oversampling data, do it after splitting</li>
          </ul>
          <li>Group leakage</li>
          <ul>
            <li>Ex. CT scans that are a week apart with the same lables, one in train set and the other in test set</li>
          </ul>
        </ul>
        <li>Detect data leakage</li>
        <ul>
          <li>Measure the predictive power of each feature (or a set of features) on target variable</li>
          <li>If a feature has high correlation, investigate</li>
        </ul>
      </ul>
      <li>Database</li>
      <ul>
        <li>SQL</li>
        <ul>
          <li>Relational</li>
          <ul>
            <li>Ex. MySQL, PostgreSQL</li>
          </ul>
        </ul>
        <li>NoSQL</li>
        <ul>
          <li>Key-value</li>
          <ul>
            <li>Ex. Redis, DynamoDB</li>
          </ul>
          <li>Column-based</li>
          <ul>
            <li>Ex. Cassandra, HBase</li>
          </ul>
          <li>Graph</li>
          <ul>
            <li>Ex. Neo4J</li>
          </ul>
          <li>Document</li>
          <ul>
            <li>Encoded as JSON, XML</li>
            <li>Each document has a unique key</li>
            <li>Does not enforce any schema</li>
            <li>Hard to join documents</li>
            <li>Ex. MongoDB, CouchDB</li>
          </ul>
        </ul>
        <li>In which format should data be stored?</li>
        <li>How to store multimodal data? (Data containing both image and text)</li>
      </ul>
      <li>ETL</li>
      <ul>
        <li>Extract - extract data from different data sources</li>
        <li>Transform - data is cleansed and transformed into specific format</li>
        <li>Load - transformed data is loaded into target destination</li>
      </ul>
      <li>Data processing</li>
      <ul>
        <li>Batch processing</li>
        <ul>
          <li>Compute features that do not change often (static features)</li>
          <li>Ex. Spark</li>
        </ul>
        <li>Streaming processing</li>
        <ul>
          <li>Compute features that change frequently (dynamic features)</li>
          <li>Ex. Flink</li>
        </ul>
      </ul>
    </ul>

    <h3 class="card-title">Feature engineering</h3>
    <ul>
      <li>Too many features</li>
      <ul>
        <li>Increased risk of data leakage</li>
        <li>Can cause overfitting</li>
        <li>Increases memory requirement</li>
        <li>Increases inference latency, especially if prediction requires extracting features</li>
        <li>Useless features become techinical debt</li>
        <ul>
          <li>When data pipeline changes, all affected features need to adjust</li>
          <li>In theory, regularization should reduce weight of useless features to 0. However, model learns faster without useless features</li>
        </ul>
      </ul>
      <li>Feature selection</li>
      <ul>
        <li>Filter method</li>
        <ul>
          <li>Assign score to each feature</li>
          <li>Often considers features independent</li>
          <li>Ex. chi squared test, information gain, correlation coefficient scores</li>
        </ul>
        <li>Embedded method</li>
        <ul>
          <li>Learn which features are contributing to the accuracy of model</li>
          <li>Ex. regularization (LASSO, elastic net, ridge regression)</li>
        </ul>
      </ul>
      <li>Missing values</li>
      <ul>
        <li>Types</li>
        <ul>
          <li>Missing not at random</li>
          <ul>
            <li>Missing due to true value itself</li>
            <li>Ex. respondants not disclosing their income and it turns out that those who don't disclose tend to have higher income</li>
          </ul>
          <li>Missing at random</li>
          <ul>
            <li>Missing due to another observed variable</li>
            <li>Ex. age value of centain gender is missing because that gender tend not to disclose their age</li>
          </ul>
          <li>Missing completely at random</li>
          <ul>
            <li>There is no pattern in which the value is missing</li>
          </ul>
        </ul>
        <li>Handle missing values</li>
        <ul>
          <li>Delete - data quantity is reduced</li>
          <ul>
            <li>Delete row to remove a data point</li>
            <li>Delete column to remove a feature</li>
          </ul>
          <li>Imputation - dataset gets noisy</li>
          <ul>
            <li>Fill with default value, mean, median, mode</li>
          </ul>
        </ul>
      </ul>
      <li>Feature scaling</li>
      <ul>
        <li>Feature scaling is not needed if using XGBoost</li>
        <li>Helps gradient descent to find the optimum faster</li>
        <li>Scale inputs to [-1,1]</li>
        <ul>
          <li>Make error function more spherical, thus gradient descent converges faster</li>
          <li>Not scaling inputs impacts regularization</li>
          <li>Outliers are also valid inputs. Do not throw them away</li>
        </ul>
        <li>Normalization (min-max scaling)</li>
        <ul>
          <li>Does not change distribution</li>
          <li>All values are \( [0,1] \)</li>
          <li>\( z = \dfrac{x-x_{min}}{x_{max}-x_{min}} \)</li>
          <li>Outliers can make real data shrunk in very narrow range</li>
        </ul>
        <li>Clipping</li>
        <ul>
          <li>Treat outliers as -1 or 1</li>
          <li>Numerical values are linearly scaled</li>
          <li>Works for uniformly distributed data</li>
        </ul>
        <li>Standardization (z-score normalization)</li>
        <ul>
          <li>Mean is \( 0 \) and standard deviation is \( 1 \)</li>
          <li>\( z = \dfrac{x-\mu}{\sigma} \)</li>
          <li>Works for normally distributed data</li>
        </ul>
        <li>Log scaling</li>
        <ul>
          <li>Mitigate skewness of a feature, so that gradient descent converges faster</li>
          <li>\( z = log(x) \)</li>
          <li>Used when data is neither uniformly or normally distributed</li>
        </ul>
      </ul>
      <li>Bucketing</li>
      <ul>
        <li>Convert numerical feature to categorical feature</li>
      </ul>
      <li>Encoding</li>
      <ul>
        <li>Convert categorical features to numerical feature</li>
        <li>Ex. integer encoding</li>
        <ul>
          <li>Integer value is assigned to each category</li>
          <li>Cannot be used for nominal features</li>
        </ul>
        <li>Ex. one-hot encoding</li>
        <ul>
          <li>Binary value is assigned to each category</li>
          <li>Not suitable for features with high cardinality</li>
          <li>Not suitable when features values are not independent</li>
        </ul>
        <li>Ex. embedding</li>
        <ul>
          <li>Learn N-D vector for each categorial value</li>
          <li>Just another hidden layer in neural network</li>
          <li>When determining embedding dimension, hyperparameter tune between these two variables</li>
          <ul>
            <li>Fourth root of the total number of unique categorical elements</li>
            <li>1.6 times the square root of the number of unique elements in the category, no less than 600</li>
          </ul>
        </ul>
      </ul>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: Machine Learning System Design Interview, Ali Aminian & Alex Xu
  </div>
</div>

<div class="card mb-4" id="machine-learning-system-design-">
  <div class="card-body">
    <h2 class="card-title">Model development</h2>

    <h3 class="card-title">Model selection</h3>
    <ul>
      <li>Establish baseline</li>
      <ul>
        <li>Ex. recommend most popular video</li>
      </ul>
      <li>Experiment with simple models</li>
      <ul>
        <li>Logistic regression</li>
      </ul>
      <li>Try complex models</li>
      <ul>
        <li>Deep neural network</li>
      </ul>
      <li>Ensemble if needed</li>
      <ul>
        <li>Bagging, boosting, stacking</li>
      </ul>
      <li>Model option</li>
      <ul>
        <li>Logistic regression</li>
        <li>Linear regression</li>
        <li>Decision trees</li>
        <li>Gradient boosted decision trees and random forests</li>
        <li>Support vector machine</li>
        <li>Naive bayes</li>
        <li>Factorization machine (FM)</li>
      </ul>
      <li>Model consideration</li>
      <ul>
        <li>Amount of data</li>
        <li>Training speed</li>
        <li>Number of parameters and memory requirement</li>
        <li>Hyperparameters and how to tune them</li>
        <li>Continual learning requirement</li>
        <li>Compute requirement</li>
        <li>Latecy during inference</li>
        <li>Model interpretability</li>
      </ul>
    </ul>

    <h3 class="card-title">Model training</h3>
    <ul>
      <li>Loss function</li>
      <ul>
        <li>Regression</li>
        <ul>
          <li>MSE</li>
          <li>MAE</li>
        </ul>
        <li>Multiclass classification</li>
        <ul>
          <li>Cross-entropy</li>
        </ul>
        <li>Binary classification</li>
        <ul>
          <li>Log loss</li>
        </ul>
        <li>?</li>
        <ul>
          <li>Huber loss</li>
        </ul>
      </ul>
      <li>Regularization</li>
      <ul>
        <li>L1</li>
        <li>L2</li>
        <li>Entropy regularization</li>
        <li>K-fold CV</li>
        <li>Dropout</li>
      </ul>
      <li>Optimization</li>
      <ul>
        <li>SGD</li>
        <li>AdaGrad</li>
        <li>Momentum</li>
        <li>RMSProp</li>
      </ul>
      <li>Activation</li>
      <ul>
        <li>ELU</li>
        <li>ReLU</li>
        <li>Tanh</li>
        <li>Sigmoid</li>
      </ul>
      <li>Save the transformation applied to convert model inputs into features</li>
      <li>How to overcome underfitting and overfitting (bias and variance)</li>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: Machine Learning System Design Interview, Ali Aminian & Alex Xu
  </div>
</div>

<div class="card mb-4" id="machine-learning-system-design-">
  <div class="card-body">
    <h2 class="card-title">Evaluation</h2>

    <h3 class="card-title">Offline</h3>
    <ul>
      <li>Baselines</li>
      <ul>
        <li>Random baseline</li>
        <li>Simple heuristic</li>
        <li>Zero rule baseline</li>
        <li>Human baseline</li>
        <li>Existing solutions</li>
      </ul>
      <li>Methods</li>
      <ul>
        <li>Purturbation tests</li>
        <li>Invariance tests</li>
        <li>Directional expectation tests</li>
        <li>Model calibration</li>
        <li>Confidence measurement</li>
        <li>Slice-based evaluation</li>
      </ul>
      <li>Examples</li>
      <ul>
        <li>Classification</li>
        <ul>
          <li>Precision</li>
          <li>Recall</li>
          <li>F1 score</li>
          <li>Accuracy</li>
          <li>ROC-AUC</li>
          <li>PR-AUC</li>
          <li>Confusion matrix</li>
        </ul>
        <li>Regression</li>
        <ul>
          <li>MSE</li>
          <li>MAE</li>
          <li>RMSE</li>
        </ul>
        <li>Ranking</li>
        <ul>
          <li>Precision@k</li>
          <li>Recall@k</li>
          <li>MRR</li>
          <li>mAP</li>
          <li>nDCG</li>
        </ul>
        <li>Text generation</li>
        <ul>
          <li>BLUE</li>
          <li>METEOR</li>
          <li>ROUGE</li>
          <li>CIDEr</li>
          <li>SPICE</li>
        </ul>
        <li>Image generation</li>
        <ul>
          <li>FID</li>
          <li>Inception score</li>
        </ul>
      </ul>
    </ul>

    <h3 class="card-title">Online</h3>
    <ul>
      <li>Increase in revenue</li>
      <li>Increase in number of users</li>
      <li>Improvement in user engagement metrics</li>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: Machine Learning System Design Interview, Ali Aminian & Alex Xu
  </div>
</div>

<div class="card mb-4" id="machine-learning-system-design-">
  <div class="card-body">
    <h2 class="card-title">Serving</h2>

    <h3 class="card-title">Data pipeline</h3>

    <img class="img-fluid" class="card-img-top" src="/machine-learning/image/machine-learning-system-design-1.png" alt="Card image cap">

    <h3 class="card-title">Training pipeline</h3>

    <img class="img-fluid" class="card-img-top" src="/machine-learning/image/machine-learning-system-design-2.png" alt="Card image cap">

    <h3 class="card-title">Serving pipeline</h3>

    <img class="img-fluid" class="card-img-top" src="/machine-learning/image/machine-learning-system-design-3.png" alt="Card image cap">

    <ul>
      <li>Deployment</li>
      <ul>
        <li>Cloud vs on-device</li>
        <ul>
          <li>Cloud - simple to deploy, faster inference, fewer constraints</li>
          <li>On-device - no cloud cost, no network latency, more privacy, no internet required</li>
          <li>Two-phase prediction</li>
          <ul>
            <li>A problem is split into two parts</li>
            <li>Smaller and cheaper model deployed on device that can achieve high accuracy</li>
            <li>More complex model on cloud that is triggered only when it is needed</li>
          </ul>
        </ul>
        <li>Model compression</li>
        <ul>
          <li>Knowledge distillation - train small model to mimic larger model</li>
          <li>Pruning - find least useful parameters and set them to zero</li>
          <li>Quantization - use fewer bits to represent parameters</li>
        </ul>
        <li>Deployment strategy</li>
        <ul>
          <li>Shadow deployment</li>
          <ul>
            <li>Deploy new model in parallel with existing model</li>
            <li>Inference from existing model is served to users</li>
            <li>Double number of prediction is needed</li>
          </ul>
          <li>A/B testing</li>
          <ul>
            <li>Deploy new model in parallel with existing model</li>
            <li>Portion of traffic is routed to new model</li>
          </ul>
          <li>Canary release</li>
        </ul>
      </ul>
      <li>Serving</li>
      <ul>
        <li>Prediction pipeline</li>
        <ul>
          <li>Batch prediction</li>
          <ul>
            <li>Only uses batch features</li>
            <li>Less responsive to change in user preference</li>
            <li>Need to know beforehand what needs to be pre-computed</li>
          </ul>
          <li>Online prediction</li>
          <ul>
            <li>May use batch features only or the combination of batch and streaming features</li>
            <li>Model may take long to generate prediction</li>
            <li>Stateless serving function</li>
            <ul>
              <li>Should support millions of requests per second</li>
              <li>Model should be packaged and deployed as stateless function</li>
            </ul>
          </ul>
        </ul>
        <li>Continual learning</li>
        <ul>
          <li>Stateless retraining</li>
          <ul>
            <li>Model is trained from scratch</li>
            <li>Required when model architecture or features change</li>
          </ul>
          <li>Stateful retraining</li>
          <ul>
            <li>Model continues training on new data</li>
            <li>Called fine-tuning or incremental learning</li>
          </ul>
          <li>Challenges</li>
          <ul>
            <li>Fresh data access challenge</li>
            <li>Evaluation challenge</li>
            <li>Algorithm challenge</li>
          </ul>
          <li>Stages</li>
          <ul>
            <li>Manual, stateless retraining</li>
            <li>Automated retraining</li>
            <li>Automated, stateful training</li>
            <li>Continual learning</li>
          </ul>
        </ul>
        <li>Two-phase prediction</li>
        <ul>
          <li>Split large use case into two phases</li>
          <li>Only the simpler phase should be carried out on the edge</li>
        </ul>
        <li>Feature store</li>
        <ul>
          <li>Store precomputed features to be re-used</li>
        </ul>
        <li>Model versioning</li>
      </ul>
      <li>ML failure</li>
      <ul>
        <li>Edge cases</li>
        <li>Degenerate feedback loops</li>
        <ul>
          <li>Detecting</li>
          <li>Correcting</li>
        </ul>
      </ul>
      <li>Data distribution shift</li>
      <ul>
        <li>Types</li>
        <ul>
          <li>Covariance shift</li>
          <ul>
            <li></li>
          </ul>
          <li>Label shift</li>
          <ul>
            <li></li>
          </ul>
          <li>Concept drift</li>
          <ul>
            <li></li>
          </ul>
        </ul>
        <li>Detecting</li>
        <li>Solution</li>
        <ul>
          <li>Train on large dataset</li>
          <li>Retrain regularly</li>
        </ul>
      </ul>
      <li>Monitoring</li>
      <ul>
        <li>Accuracy</li>
        <ul>
          <li>Model performance</li>
          <li>Ex. accuracy, precision, recall, F1 score</li>
        </ul>
        <li>Predictions</li>
        <ul>
          <li>Distribution drift of prediction</li>
          <li>Ex. prediction PSI</li>
        </ul>
        <li>Features</li>
        <ul>
          <li>Distribution drift of features</li>
          <li>Ex. feature PSI</li>
        </ul>
        <li>Raw inputs</li>
      </ul>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: Machine Learning System Design Interview, Ali Aminian & Alex Xu
  </div>
</div>
<!-- Machine learning system design END -->

</div> <!-- /.col-md-12 -->
</div> <!-- /.row -->
</div> <!-- /.container -->

<include src="/footer.html"></include>

</body>

</html>