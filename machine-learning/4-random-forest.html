<!DOCTYPE html>

<html lang="en">

<head>

<!-- Metadata -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="">
<meta name="author" content="Seungmoon Rieh">
<meta name="keywords" content="">

<!-- Title and image -->
<title>Seungmoon Rieh</title>
<link href="/img/seungmoonrieh.jpg" rel="icon">

<!-- CSS -->
<link href="/css/bootstrap.min.css" rel="stylesheet">
<link href="/css/monokai-sublime.css" rel="stylesheet">
<link href="/css/site.css" rel="stylesheet">

<!-- JavaScript -->
<script src="/js/jquery.min.js" type="text/javascript"></script>
<script src="/js/bootstrap.bundle.min.js" type="text/javascript"></script>
<script src="/js/highlight.pack.js" type="text/javascript"></script>
<script src="/js/include_html.js" type="text/javascript"></script>
<script src="/js/mathjax/tex-chtml.js" type="text/javascript"></script>
<script src="/js/site.js" type="text/javascript"></script>

</head>

<body>

<include src="/header.html"></include>

<div class="container">
<div class="row">
<div class="col-md-12">
<h1 class="my-4">Machine Learning</h1>

<!-- Random forest BEGIN -->
<div class="card mb-4" id="subject">
  <div class="card-body">
    <h2 class="card-title">Random forest</h2>
    <ul class="list-unstyled mb-0">
      <li><a href="#random-forest-">Random forest</a></li>
      <li><a href="#random-forest-">Hyperparameters</a></li>
      <li><a href="#random-forest-">Example - bike rental</a></li>
    </ul>
  </div>
</div>

<div class="card mb-4" id="random-forest-">
  <div class="card-body">
    <h2 class="card-title">Random forest</h2>
    <ul>
      <li>Bagging - aggregate predictions of bootstrapped decision trees</li>
      <li>Limitation - if all individual trees make the same mistake, random forest makes the mistake</li>
    </ul>

<pre><code class="python">import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')

df_census = pd.read_csv('census_cleaned.csv')
X_census = df_census.iloc[:,:-1]
y_census = df_census.iloc[:,-1]</code></pre>

<pre><code class="python">from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score

# Initialize the classifier
rf = RandomForestClassifier(n_estimators=10, random_state=2, n_jobs=-1)

# Obtain scores of cross-validation
scores = cross_val_score(rf, X_census, y_census, cv=5)</code></pre>
  </div>
  <div class="card-footer text-muted">
    Reference: Hands-On Gradient Boosting with XGBoost and scikit-learn, Corey Wade
  </div>
</div>

<div class="card mb-4" id="random-forest-">
  <div class="card-body">
    <h2 class="card-title">Hyperparameters</h2>
    <ul>
      <li>Decision tree parameters are not as significant because random forest cut down on variance by design</li>
      <li><code>oob_score</code></li>
      <ul>
        <li>Use samples that are not chosen during bagging as test set</li>
        <li>True by default</li>
        <li><pre><code class="python">rf = RandomForestClassifier(oob_score=True, n_estimators=10, random_state=2, n_jobs=-1)
rf.fit(X_census, y_census)
rf.oob_score_  # Ouputs test score</code></pre></li>
      </ul>
      <li><code>n_estimators</code></li>
      <ul>
        <li>Number of trees</li>
        <li>100 by default</li>
      </ul>
      <li><code>warm_start</code></li>
      <ul>
        <li>Adding more trees does not require starting from scratch</li>
        <li>Could be used to plot various score with a range of <code>n_estimators</code></li>
        <li><pre><code class="python">import matplotlib.pyplot as plt
import seaborn as sns

sns.set()
oob_scores = []

rf = RandomForestClassifier(n_estimators=50, warm_start=True, oob_score=True, n_jobs=-1, random_state=2)
rf.fit(X_census, y_census)
oob_scores.append(rf.oob_score_)
est = 50
estimators=[est]

for i in range(9):
    est += 50
    estimators.append(est)
    rf.set_params(n_estimators=est)
    rf.fit(X_census, y_census)
    oob_scores.append(rf.oob_score_)

plt.figure(figsize=(15,7))
plt.plot(estimators, oob_scores)
plt.xlabel('Number of Trees')
plt.ylabel('oob_score_')
plt.title('Random Forest Warm Start', fontsize=15)
plt.savefig('Random_Forest_Warm_Start', dpi=325)
plt.show()</code></pre></li>
      </ul>
      <li><code>bootstrap</code></li>
      <li><code>verbose</code></li>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: Hands-On Gradient Boosting with XGBoost and scikit-learn, Corey Wade
  </div>
</div>

<div class="card mb-4" id="random-forest-">
  <div class="card-body">
    <h2 class="card-title">Example - bike rental</h2>

<pre><code class="python">from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X_bikes, y_bikes, random_state=2)</code></pre>

<pre><code class="python"># Initalize Random Forest as rf with 50 estimators, warm_start=True, and oob_score=True
rf = RandomForestRegressor(n_estimators=50, warm_start=True, n_jobs=-1, random_state=2)

# Obtain scores of cross-validation using num_splits and mean squared error
scores = cross_val_score(rf, X_bikes, y_bikes, scoring='neg_mean_squared_error', cv=10)

# Take square root of the scores
rmse = np.sqrt(-scores)

# Display accuracy
print('RMSE:', np.round(rmse, 3))

# Display mean score
print('RMSE mean: %0.3f' % (rmse.mean()))

# RMSE: [ 836.482  541.898  533.086  812.782  894.877  881.117  794.103  828.968  772.517  2128.148]
# RMSE mean: 902.398</code></pre>

    <h3 class="card-title">RandomizedSearchCV</h3>

<pre><code class="python">from sklearn.model_selection import RandomizedSearchCV

def randomized_search_reg(params, runs=16, reg=RandomForestRegressor(random_state=2, n_jobs=-1)):

    # Instantiate RandomizedSearchCV as grid_reg
    rand_reg = RandomizedSearchCV(reg, params, n_iter=runs, scoring='neg_mean_squared_error',
                                  cv=10, n_jobs=-1, random_state=2)

    # Fit grid_reg on X_train and y_train
    rand_reg.fit(X_train, y_train)

    # Extract best estimator
    best_model = rand_reg.best_estimator_

    # Extract best params
    best_params = rand_reg.best_params_

    # Print best params
    print("Best params:", best_params)

    # Compute best score
    best_score = np.sqrt(-rand_reg.best_score_)

    # Print best score
    print("Training score: {:.3f}".format(best_score))

    # Predict test set labels
    y_pred = best_model.predict(X_test)

    # Import mean_squared_error from sklearn.metrics as MSE
    from sklearn.metrics import mean_squared_error as MSE

    # Compute rmse_test
    rmse_test = MSE(y_test, y_pred)**0.5

    # Print rmse_test
    print('Test set score: {:.3f}'.format(rmse_test))</code></pre>

<pre><code class="python"></code></pre>
  </div>
  <div class="card-footer text-muted">
    Reference: Hands-On Gradient Boosting with XGBoost and scikit-learn, Corey Wade
  </div>
</div>
<!-- Random forest END -->

</div> <!-- /.col-md-12 -->
</div> <!-- /.row -->
</div> <!-- /.container -->

<include src="/footer.html"></include>

</body>

</html>