<!DOCTYPE html>

<html lang="en">

<head>

<!-- Metadata -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="">
<meta name="author" content="Seungmoon Rieh">
<meta name="keywords" content="">

<!-- Title and image -->
<title>Seungmoon Rieh</title>
<link href="/img/seungmoonrieh.jpg" rel="icon">

<!-- CSS -->
<link href="/css/bootstrap.min.css" rel="stylesheet">
<link href="/css/monokai-sublime.css" rel="stylesheet">
<link href="/css/site.css" rel="stylesheet">

<!-- JavaScript -->
<script src="/js/jquery.min.js" type="text/javascript"></script>
<script src="/js/bootstrap.bundle.min.js" type="text/javascript"></script>
<script src="/js/highlight.pack.js" type="text/javascript"></script>
<script src="/js/include_html.js" type="text/javascript"></script>
<script src="/js/mathjax/tex-chtml.js" type="text/javascript"></script>
<script src="/js/site.js" type="text/javascript"></script>

</head>

<body>

<include src="/header.html"></include>

<div class="container">
<div class="row">
<div class="col-md-12">
<h1 class="my-4">Machine Learning</h1>

<!-- Long short term memory BEGIN -->
<div class="card mb-4" id="subject">
  <div class="card-body">
    <h2 class="card-title">Long short term memory</h2>
    <ul class="list-unstyled mb-0">
      <li><a href="#Long-short-term-memory-">Long short term memory</a></li>
      <!-- <li><a href="#Long-short-term-memory-">Example - generate music</a></li> -->
    </ul>
  </div>
</div>

<div class="card mb-4" id="Long-short-term-memory-">
  <div class="card-body">
    <h2 class="card-title">Long short term memory</h2>
    <ul>
      <li>Addresses vanishing gradient</li>
      <ul>
        <li>Uses gating mechanisms to control the flow of information and gradients</li>
      </ul>
      <li>If exploding gradient, apply gradient clipping</li>
    </ul>

    <h3 class="card-title">Vanishing gradients with RNNs</h3>
    <ul>
      <li>Ex. "the cats, which, ..., were full" vs "the cat, which, ..., was full"</li>
      <li>Capturing long-term dependencies is hard</li>
    </ul>

    <h3 class="card-title">Candidate value</h3>
    <ul>
      <li>Information from current time step that may be stored in current cell state \( c^{\langle t \rangle} \)</li>
      <li>Tensor containing values between -1 to 1</li>
      <li>\( \tilde{c}^{\langle t \rangle} = \text{tanh}(W_{c}[a^{\langle t-1 \rangle}, x^{\langle t \rangle}] + b_{c}) \)</li>
    </ul>

    <h3 class="card-title">Cell state</h3>
    <ul>
      <li>The memory that gets passed onto future time steps</li>
      <li>The new cell state is a combination of the previous cell state and the candidate value</li>
      <li>\( c^{\langle t \rangle} = \Gamma_{u} * \tilde{c}^{\langle t \rangle} + \Gamma_{f} c^{\langle t-1 \rangle} \)</li>
    </ul>

    <h3 class="card-title">Hidden state</h3>
    <ul>
      <li>Hidden state gets passed to the LSTM cell's next time step</li>
      <li>Determines the three gates of the next time step</li>
      <li>Hidden state is also used for the prediction</li>
      <li>\( a^{\langle t \rangle} = \Gamma_{o} * tanh(c^{\langle t \rangle}) \)</li>
    </ul>

    <h3 class="card-title">Update gate</h3>
    <ul>
      <li>Decides what parts of a candidate tensor \( \tilde{c}^{\langle t \rangle} \) are passed onto the cell state \( c^{\langle t \rangle} \)</li>
      <li>Tensor containing values between 0 and 1</li>
      <ul>
        <li>If unit in update gate is close to 1, it allows the value of the candidate to be passed onto the hidden state</li>
        <li>If unit in update gate is close to 0, it prevents the corresponding value in the candidate from being passed onto the hidden state</li>
      </ul>
      <li>\( \Gamma_{u}^{\langle t \rangle} = \sigma(W_{u}[a^{\langle t-1 \rangle}, x^{\langle t \rangle}] + b_{u}) \)</li>
    </ul>

    <h3 class="card-title">Forget gate</h3>
    <ul>
      <li>When memory of the previous state becomes outdated, forget that outdated state</li>
      <li>Tensor containing values between 0 and 1</li>
      <ul>
        <li>If unit in forget gate has value close to 0, LSTM will "forget" the stored state in the previous cell state</li>
        <li>If unit in forget gate has value close to 1, LSTM will mostly remember the corresponding value in the stored state</li>
      </ul>
      <li>\( \Gamma_{f}^{\langle t \rangle} = \sigma(W_{f}[a^{\langle t-1 \rangle}, x^{\langle t \rangle}] + b_{f}) \)</li>
    </ul>

    <h3 class="card-title">Output gate</h3>
    <ul>
      <li>Decides what gets sent as the prediction (output) of the time step</li>
      <li>\( \Gamma_{o}^{\langle t \rangle} = \sigma(W_{o}[a^{\langle t-1 \rangle}, x^{\langle t \rangle}] + b_{o}) \)</li>
    </ul>

    <h3 class="card-title">Prediction</h3>
    <ul>
      <li>Ex. softmax</li>
      <li>\( y_{pred}^{\langle t \rangle} = \text{softmax}(W_{y} a^{\langle t \rangle} + b_{y}) \)</li>
    </ul>

    <h3 class="card-title">Forward prop</h3>

<pre><code class="python">def lstm_cell_forward(xt, a_prev, c_prev, parameters):
    """
    Args:
    xt -- input data at timestep "t", of shape (n_x, m).
    a_prev -- hidden state at timestep "t-1", of shape (n_a, m)
    c_prev -- memory state at timestep "t-1", of shape (n_a, m)
    parameters -- python dictionary containing
                  Wf -- weight matrix of the forget gate, of shape (n_a, n_a + n_x)
                  bf -- bias of the forget gate, of shape (n_a, 1)
                  Wi -- weight matrix of the update gate, of shape (n_a, n_a + n_x)
                  bi -- bias of the update gate, of shape (n_a, 1)
                  Wc -- weight matrix of the first "tanh", of shape (n_a, n_a + n_x)
                  bc -- bias of the first "tanh", of shape (n_a, 1)
                  Wo -- weight matrix of the output gate, of shape (n_a, n_a + n_x)
                  bo -- bias of the output gate, of shape (n_a, 1)
                  Wy -- weight matrix relating the hidden-state to the output, of shape (n_y, n_a)
                  by -- bias relating the hidden-state to the output, of shape (n_y, 1)

    Returns:
    a_next -- next hidden state, of shape (n_a, m)
    c_next -- next memory state, of shape (n_a, m)
    yt_pred -- prediction at timestep "t",of shape (n_y, m)
    cache -- values needed for the backward pass (a_next, c_next, a_prev, c_prev, xt, parameters)
    """

    Wf = parameters["Wf"]  # Forget gate weight
    bf = parameters["bf"]
    Wi = parameters["Wi"]  # Update gate weight
    bi = parameters["bi"]
    Wc = parameters["Wc"]  # Candidate value weight
    bc = parameters["bc"]
    Wo = parameters["Wo"]  # Output gate weight
    bo = parameters["bo"]
    Wy = parameters["Wy"]  # Prediction weight
    by = parameters["by"]

    n_x, m = xt.shape
    n_y, n_a = Wy.shape

    concat = np.concatenate((a_prev, xt))
    concat[: n_a, :] = a_prev
    concat[n_a :, :] = xt

    ft = sigmoid(np.dot(Wf, concat) + bf)  # Forget gate
    it = sigmoid(np.dot(Wi, concat) + bi)  # Update gate
    cct = np.tanh(np.dot(Wc, concat) + bc)  # Candidate value
    c_next = np.multiply(ft, c_prev) + np.multiply(it, cct)  # Cell state
    ot = sigmoid(np.dot(Wo, concat) + bo)  # Output gate
    a_next = np.multiply(ot, np.tanh(c_next))  # Hidden state

    yt_pred = softmax(np.dot(Wy, a_next) + by)

    cache = (a_next, c_next, a_prev, c_prev, ft, it, cct, ot, xt, parameters)

    return a_next, c_next, yt_pred, cache</code></pre>

<pre><code class="python">def lstm_forward(x, a0, parameters):
    """
    Args:
    x -- input data for every time-step, of shape (n_x, m, T_x)
    a0 -- initial hidden state, of shape (n_a, m)
    parameters -- python dictionary containing
                  Wf -- weight matrix of the forget gate, of shape (n_a, n_a + n_x)
                  bf -- bias of the forget gate, of shape (n_a, 1)
                  Wi -- weight matrix of the update gate, of shape (n_a, n_a + n_x)
                  bi -- bias of the update gate, of shape (n_a, 1)
                  Wc -- weight matrix of the first "tanh", of shape (n_a, n_a + n_x)
                  bc -- bias of the first "tanh", of shape (n_a, 1)
                  Wo -- weight matrix of the output gate, of shape (n_a, n_a + n_x)
                  bo -- bias of the output gate, of shape (n_a, 1)
                  Wy -- weight matrix relating the hidden-state to the output, of shape (n_y, n_a)
                  by -- bias relating the hidden-state to the output, of shape (n_y, 1)

    Returns:
    a -- hidden states for every time-step, of shape (n_a, m, T_x)
    y -- predictions for every time-step, nof shape (n_y, m, T_x)
    c -- value of the cell state, of shape (n_a, m, T_x)
    caches -- values needed for the backward pass (list of all the caches, x)
    """

    caches = []

    Wy = parameters['Wy']
    n_x, m, T_x = x.shape
    n_y, n_a = parameters['Wy'].shape

    a = np.zeros((n_a, m, T_x))
    c = np.zeros((n_a, m, T_x))
    y = np.zeros((n_y, m, T_x))

    a_next = a0
    c_next = np.zeros(a_next.shape)

    for t in range(T_x):
        a_next, c_next, yt, cache = lstm_cell_forward(x[:,:,t], a[:,:,t], c[:,:,t], parameters)
        a[:,:,t] = a_next
        y[:,:,t] = yt
        c[:,:,t] = c_next
        caches.append(cache)

    caches = (caches, x)

    return a, y, c, caches</code></pre>

    <h3 class="card-title">Backward prop</h3>
    <ul>
      <li>Gate derivatives</li>
      <ul>
        <li>\( d\gamma_{o}^{\langle t \rangle} = da_{\text{next}}*\tanh(c_{\text{next}}) * \Gamma_{o}^{\langle t \rangle}*\left(1-\Gamma_{o}^{\langle t \rangle}\right) \)</li>
        <li>\( dp\widetilde{c}^{\langle t \rangle} = \left(dc_{\text{next}}*\Gamma_{u}^{\langle t \rangle} + \Gamma_{o}^{\langle t \rangle} * (1-\tanh^2(c_{\text{next}})) * \Gamma_{u}^{\langle t \rangle} * da_{\text{next}} \right) * \left(1-\left(\widetilde c^{\langle t \rangle}\right)^2\right) \)</li>
        <li>\( d\gamma_{u}^{\langle t \rangle} = \left(dc_{\text{next}}*\widetilde{c}^{\langle t \rangle} + \Gamma_{o}^{\langle t \rangle} * (1-\tanh^2(c_{\text{next}})) * \widetilde{c}^{\langle t \rangle} * da_{\text{next}}\right) * \Gamma_{u}^{\langle t \rangle} * \left(1-\Gamma_{u}^{\langle t \rangle}\right) \)</li>
        <li>\( d\gamma_{f}^{\langle t \rangle} = \left(dc_{\text{next}}* c_{\text{prev}} + \Gamma_{o}^{\langle t \rangle} * (1-\tanh^2(c_{\text{next}})) * c_{\text{prev}} * da_{\text{next}}\right) * \Gamma_{f}^{\langle t \rangle} * \left(1-\Gamma_{f}^{\langle t \rangle}\right) \)</li>
      </ul>
      <li>Parameter derivatives</li>
      <ul>
        <li>\( dW_{f} = d\gamma_{f}^{\langle t \rangle} \begin{bmatrix} a_{\text{prev}} \\ x_t\end{bmatrix}^{T} \)</li>
        <li>\( dW_{u} = d\gamma_{u}^{\langle t \rangle} \begin{bmatrix} a_{\text{prev}} \\ x_t\end{bmatrix}^{T} \)</li>
        <li>\( dW_{c} = dp\widetilde c^{\langle t \rangle} \begin{bmatrix} a_{\text{prev}} \\ x_t\end{bmatrix}^{T} \)</li>
        <li>\( dW_{o} = d\gamma_{o}^{\langle t \rangle} \begin{bmatrix} a_{\text{prev}} \\ x_t\end{bmatrix}^{T} \)</li>
        <li>\( db_{f} = \displaystyle\sum_{\text{batch}}d\gamma_{f}^{\langle t \rangle} \)</li>
        <li>\( db_{u} = \displaystyle\sum_{\text{batch}}d\gamma_{u}^{\langle t \rangle} \)</li>
        <li>\( db_{c} = \displaystyle\sum_{\text{batch}}d\gamma_{c}^{\langle t \rangle} \)</li>
        <li>\( db_{o} = \displaystyle\sum_{\text{batch}}d\gamma_{o}^{\langle t \rangle} \)</li>
        <li>\( da_{\text{prev}} = W_{f}^{T} d\gamma_{f}^{\langle t \rangle} + W_{u}^{T} d\gamma_{u}^{\langle t \rangle} + W_{c}^{T} dp\widetilde c^{\langle t \rangle} + W_{o}^{T} d\gamma_{o}^{\langle t \rangle} \)</li>
        <li>\( dc_{\text{prev}} = dc_{\text{next}} * \Gamma_{f}^{\langle t \rangle} + \Gamma_{o}^{\langle t \rangle} * (1-\tanh^2(c_{\text{next}})) * \Gamma_{f}^{\langle t \rangle} * da_{\text{next}} \)</li>
        <li>\( dx^{\langle t \rangle} = W_{f}^{T} d\gamma_{f}^{\langle t \rangle} + W_{u}^{T} d\gamma_{u}^{\langle t \rangle} + W_{c}^{T} dp\widetilde c^{\langle t \rangle} + W_{o}^{T} d\gamma_{o}^{\langle t \rangle} \)</li>
      </ul>
    </ul>

<pre><code class="python">def lstm_cell_backward(da_next, dc_next, cache):
    """
    Args:
    da_next -- gradients of next hidden state, of shape (n_a, m)
    dc_next -- gradients of next cell state, of shape (n_a, m)
    cache --

    Returns:
    gradients -- python dictionary containing
                 dxt -- gradient of input data at time-step t, of shape (n_x, m)
                 da_prev -- gradient w.r.t. the previous hidden state, of shape (n_a, m)
                 dc_prev -- gradient w.r.t. the previous memory state, of shape (n_a, m, T_x)
                 dWf -- gradient w.r.t. the weight matrix of the forget gate, of shape (n_a, n_a + n_x)
                 dWi -- gradient w.r.t. the weight matrix of the update gate, of shape (n_a, n_a + n_x)
                 dWc -- gradient w.r.t. the weight matrix of the memory gate, of shape (n_a, n_a + n_x)
                 dWo -- gradient w.r.t. the weight matrix of the output gate, of shape (n_a, n_a + n_x)
                 dbf -- gradient w.r.t. biases of the forget gate, of shape (n_a, 1)
                 dbi -- gradient w.r.t. biases of the update gate, of shape (n_a, 1)
                 dbc -- gradient w.r.t. biases of the memory gate, of shape (n_a, 1)
                 dbo -- gradient w.r.t. biases of the output gate, of shape (n_a, 1)
    """

    (a_next, c_next, a_prev, c_prev, ft, it, cct, ot, xt, parameters) = cache

    n_x, m = xt.shape
    n_a, m = a_next.shape

    dot = da_next*np.tanh(c_next)*ot*(1-ot)
    dcct = (dc_next*it+ot*(1-np.square(np.tanh(c_next)))*it*da_next)*(1-np.square(cct))
    dit = (dc_next*cct+ot*(1-np.square(np.tanh(c_next)))*cct*da_next)*it*(1-it)
    dft = (dc_next*c_prev+ot*(1-np.square(np.tanh(c_next)))*c_prev*da_next)*ft*(1-ft)

    dit = None
    dft = None
    dot = None
    dcct = None

    dWf = np.dot(dft,np.concatenate((a_prev, xt), axis=0).T)
    dWi = np.dot(dit,np.concatenate((a_prev, xt), axis=0).T)
    dWc = np.dot(dcct,np.concatenate((a_prev, xt), axis=0).T)
    dWo = np.dot(dot,np.concatenate((a_prev, xt), axis=0).T)
    dbf = np.sum(dft,axis=1,keepdims=True)
    dbi = np.sum(dit,axis=1,keepdims=True)
    dbc = np.sum(dcct,axis=1,keepdims=True)
    dbo = np.sum(dot,axis=1,keepdims=True)

    da_prev = np.dot(parameters['Wf'][:,:n_a].T,dft)+np.dot(parameters['Wi'][:,:n_a].T,dit)+np.dot(parameters['Wc'][:,:n_a].T,dcct)+np.dot(parameters['Wo'][:,:n_a].T,dot)
    dc_prev = dc_next*ft+ot*(1-np.square(np.tanh(c_next)))*ft*da_next
    dxt = np.dot(parameters['Wf'][:,n_a:].T,dft)+np.dot(parameters['Wi'][:,n_a:].T,dit)+np.dot(parameters['Wc'][:,n_a:].T,dcct)+np.dot(parameters['Wo'][:,n_a:].T,dot)

    gradients = {"dxt": dxt, "da_prev": da_prev, "dc_prev": dc_prev, "dWf": dWf, "dbf": dbf, "dWi": dWi, "dbi": dbi, "dWc": dWc, "dbc": dbc, "dWo": dWo, "dbo": dbo}

    return gradients</code></pre>

<pre><code class="python">def lstm_backward(da, caches):
    """
    Args:
    da -- gradients w.r.t the hidden states, of shape (n_a, m, T_x)
    caches --

    Returns:
    gradients -- python dictionary containing
                 dxt -- gradient of input data at time-step t, of shape (n_x, m)
                 da_prev -- gradient w.r.t. the previous hidden state, of shape (n_a, m)
                 dc_prev -- gradient w.r.t. the previous memory state, of shape (n_a, m, T_x)
                 dWf -- gradient w.r.t. the weight matrix of the forget gate, of shape (n_a, n_a + n_x)
                 dWi -- gradient w.r.t. the weight matrix of the update gate, of shape (n_a, n_a + n_x)
                 dWc -- gradient w.r.t. the weight matrix of the memory gate, of shape (n_a, n_a + n_x)
                 dWo -- gradient w.r.t. the weight matrix of the output gate, of shape (n_a, n_a + n_x)
                 dbf -- gradient w.r.t. biases of the forget gate, of shape (n_a, 1)
                 dbi -- gradient w.r.t. biases of the update gate, of shape (n_a, 1)
                 dbc -- gradient w.r.t. biases of the memory gate, of shape (n_a, 1)
                 dbo -- gradient w.r.t. biases of the output gate, of shape (n_a, 1)
    """

    (caches, x) = caches
    (a1, c1, a0, c0, f1, i1, cc1, o1, x1, parameters) = caches[0]

    n_a, m, T_x = da.shape
    n_x, m = x1.shape

    dx = np.zeros((n_x, m, T_x))
    da0 = np.zeros((n_a, m))
    da_prevt = np.zeros((n_a, m))
    dc_prevt = np.zeros((n_a, m))
    dWf = np.zeros((n_a, n_a + n_x))
    dWi = np.zeros((n_a, n_a + n_x))
    dWc = np.zeros((n_a, n_a + n_x))
    dWo = np.zeros((n_a, n_a + n_x))
    dbf = np.zeros((n_a, 1))
    dbi = np.zeros((n_a, 1))
    dbc = np.zeros((n_a, 1))
    dbo = np.zeros((n_a, 1))

    for t in reversed(range(T_x)):
        gradients = lstm_cell_backward(da[:,:,t]+da_prevt,dc_prevt,caches[t])
        dx[:, :, t] = gradients['dxt']
        dWf = dWf+gradients['dWf']
        dWi = dWi+gradients['dWi']
        dWc = dWc+gradients['dWc']
        dWo = dWo+gradients['dWo']
        dbf = dbf+gradients['dbf']
        dbi = dbi+gradients['dbi']
        dbc = dbc+gradients['dbc']
        dbo = dbo+gradients['dbo']

    da0 = gradients['da_prev']

    gradients = {"dx": dx, "da0": da0, "dWf": dWf, "dbf": dbf, "dWi": dWi, "dbi": dbi, "dWc": dWc, "dbc": dbc, "dWo": dWo, "dbo": dbo}

    return gradients</code></pre>

    <h3 class="card-title">Bidirectional RNN</h3>
    <ul>
      <li>Getting information from the future</li>
      <ul>
        <li>Ex. He said, "Teddy bears are on sale!"</li>
        <li>Ex. He said, "Teddy Roosevelt was a great President"</li>
      </ul>
      <li>\( y^{&lt;t&gt;} = g(W_{y}[\overrightarrow{a}^{&lt;t&gt;}, \overleftarrow{a}^{&lt;t&gt;}] + b_{y}) \)</li>
    </ul>

    <!-- <h3 class="card-title">Dropout</h3>
    <ul>
      <li>In RNN, dropout is applied to input/output of each cell unit.</li>
      <li>Dropout decreases the amount of cell computation, preventing overfitting the data.</li>
    </ul> -->
  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://www.deeplearning.ai/deep-learning-specialization">Deep Learning Specialization</a>
  </div>
</div>

<!-- <div class="card mb-4" id="Long-short-term-memory-">
  <div class="card-body">
    <h2 class="card-title">Example - generate music</h2>
    <ul>
      <li>Use previous values to predict the next value</li>
      <li>Initialize parameters</li>
      <li>Run loop</li>
      <ul>
        <li>Forward propagation</li>
        <li>Compute cost</li>
        <li>Backward propagation</li>
        <li>Clip the gradients</li>
        <li>Update parameters</li>
        <li>Sample predictions at each time step and pass it to the next RNN-cell unit</li>
      </ul>
      <li>Return the learned parameters</li>
    </ul>

<pre><code class="python">def djmodel(Tx, n_a, n_values):
    """
    Args:
    Tx -- length of the sequence in a corpus
    n_a -- number of activations
    n_values -- number of unique values in the music data

    Returns:
    model -- a keras model
    """

    X = Input(shape=(Tx, n_values))
    a0 = Input(shape=(n_a,), name='a0')
    c0 = Input(shape=(n_a,), name='c0')
    a = a0
    c = c0
    outputs = []

    for t in range(Tx):
        x = Lambda(lambda x: x[:,t,:])(X)
        x = reshapor(x)
        a, _, c = LSTM_cell(x, initial_state=[a, c])
        out =  densor(a)
        outputs.append(out)

    model = Model(inputs=[X, a0, c0], outputs=outputs)

    return model</code></pre>

<pre><code class="python"></code></pre>

<pre><code class="python"></code></pre>

<pre><code class="python"></code></pre>

<pre><code class="python"></code></pre>

  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://www.deeplearning.ai/deep-learning-specialization">Deep Learning Specialization</a>
  </div>
</div> -->
<!-- Long short term memory END -->

</div> <!-- /.col-md-12 -->
</div> <!-- /.row -->
</div> <!-- /.container -->

<include src="/footer.html"></include>

</body>

</html>