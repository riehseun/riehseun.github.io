<!DOCTYPE html>

<html lang="en">

<head>

<!-- Metadata -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="">
<meta name="author" content="Seungmoon Rieh">
<meta name="keywords" content="">

<!-- Title and image -->
<title>Seungmoon Rieh</title>
<link href="/img/seungmoonrieh.jpg" rel="icon">

<!-- CSS -->
<link href="/css/bootstrap.min.css" rel="stylesheet">
<link href="/css/monokai-sublime.css" rel="stylesheet">
<link href="/css/site.css" rel="stylesheet">

<!-- JavaScript -->
<script src="/js/jquery.min.js" type="text/javascript"></script>
<script src="/js/bootstrap.bundle.min.js" type="text/javascript"></script>
<script src="/js/highlight.pack.js" type="text/javascript"></script>
<script src="/js/include_html.js" type="text/javascript"></script>
<script src="/js/mathjax/tex-chtml.js" type="text/javascript"></script>
<script src="/js/site.js" type="text/javascript"></script>

</head>

<body>

<include src="/header.html"></include>

<div class="container">
<div class="row">
<div class="col-md-12">
<h1 class="my-4">Machine Learning</h1>

<!-- Word embedding BEGIN -->
<div class="card mb-4" id="subject">
  <div class="card-body">
    <h2 class="card-title">Word embedding</h2>
    <ul class="list-unstyled mb-0">
      <li><a href="#word-embedding-1">Word embedding</a></li>
    </ul>
  </div>
</div>

<div class="card mb-4" id="word-embedding-1">
  <div class="card-body">
    <h2 class="card-title">Word embedding</h2>

    <h3 class="card-title">One-hot vector</h3>
    <ul>
      <li>Simple and requires no implied ordering.</li>
      <li>Huge and encodes no meaning.</li>
      <li>Common technique in feature engineering.</li>
      <li>Converts categorical variables into one-hot numeric array.</li>
      <li>Unimportant classes can be grouped together in "other" class</li>
      <li>High dimensional feature vectors consume large memory.</li>
      <li>Not suitable for NLP. (Storing each word as vector requires too much memory)</li>
    </ul>

    <h3 class="card-title">Feature hashing (hashing trick)</h3>
    <ul>
      <li>Reduce dimension by having multiple values encoded as same value.</li>
      <li>There is a trade-off between dimensions and collisions. (Lower dimension means high collision)</li>
      <li>Collisions impact model performance.</li>
    </ul>

    <h3 class="card-title">Word embedding</h3>
    <ul>
      <li>Low dimension.</li>
      <li>Capture semantic meaning of features.</li>
      <li>Similar features will be close to each other in the embedding vector space.</li>
      <li>Need</li>
      <ul>
        <li>Corpus.</li>
        <li>Embedding method.</li>
      </ul>
      <li>Self-supervised</li>
      <ul>
        <li>Unsupervised in a sense that input data (corpus) is unlabelled.</li>
        <li>Supervised in a sense that data provides context that would make up the labels.</li>
      </ul>
    </ul>

    <h3 class="card-title">Continuous bag of words model</h3>
    <ul>
      <li>Predict a missing word based on the surrounding words.</li>
    </ul>

    <h3 class="card-title">CBOW cost function</h3>
    <ul>
      <li>\( J = -\displaystyle\sum_{k=1}^{V}y_{k}log\hat{y}_{k} \)</li>
    </ul>

    <h3 class="card-title">CBOW forward prop</h3>
    <ul>
      <li>\( Z_{1} = W_{1}X + B_{1} \)</li>
      <li>\( H = \textrm{ReLU}(Z_{1}) \)</li>
      <li>\( Z_{2} = W_{2}H + B_{2} \)</li>
      <li>\( \hat{Y} = \textrm{softmax}(Z_{2}) \)</li>
      <li>\( J_{batch} = -\dfrac{1}{m}\displaystyle\sum_{i=1}^{m}\displaystyle\sum_{j=1}^{V}y_{j}^{(i)}log\hat{y}_{j}^{(i)} \)</li>
    </ul>

    <h3 class="card-title">CBOW backward prop</h3>
    <ul>
      <li>\( W_{1} = W_{1} - \alpha\dfrac{\partial J_{batch}}{\partial W_{1}} = W_{1} - \dfrac{1}{m}\textrm{ReLU}(W_{2}^{T}(\hat{Y}-Y))X^{T} \)</li>
      <li>\( W_{2} = W_{2} - \alpha\dfrac{\partial J_{batch}}{\partial W_{2}} = W_{2} - \dfrac{1}{m}(\hat{Y}-Y))H^{T} \)</li>
      <li>\( b_{1} = b_{1} - \alpha\dfrac{\partial J_{batch}}{\partial b_{1}} = b_{1} - \dfrac{1}{m}\textrm{ReLU}(W_{2}^{T}(\hat{Y}-Y))1_{m}^{T} \)</li>
      <li>\( b_{2} = b_{2} - \alpha\dfrac{\partial J_{batch}}{\partial b_{2}} = b_{2} - \dfrac{1}{m}(\hat{Y}-Y))1_{m}^{T} \)</li>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://www.deeplearning.ai/deep-learning-specialization/">Deep Learning Specialization</a>
  </div>
</div>
<!-- Word embedding END -->

</div> <!-- /.col-md-12 -->
</div> <!-- /.row -->
</div> <!-- /.container -->

<include src="/footer.html"></include>

</body>

</html>