<!DOCTYPE html>

<html lang="en">

<head>

<!-- Metadata -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="">
<meta name="author" content="Seungmoon Rieh">
<meta name="keywords" content="">

<!-- Title and image -->
<title>Seungmoon Rieh</title>
<link href="/img/seungmoonrieh.jpg" rel="icon">

<!-- CSS -->
<link href="/css/bootstrap.min.css" rel="stylesheet">
<link href="/css/monokai-sublime.css" rel="stylesheet">
<link href="/css/site.css" rel="stylesheet">

<!-- JavaScript -->
<script src="/js/jquery.min.js"></script>
<script src="/js/bootstrap.bundle.min.js"></script>
<script src="/js/highlight.pack.js"></script>
<script src="/js/include_html.js" type="text/javascript"></script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script src="/js/site.js" type="text/javascript"></script>

</head>

<body>

<include src="/header.html"></include>

<div class="container">
<div class="row">
<div class="col-md-12">
<h1 class="my-4">Machine Learning</h1>

<!-- Gen AI BEGIN -->
<div class="card mb-4" id="genai">
  <div class="card-body">
    <h2 class="card-title">Gen AI</h2>
    <ul class="list-unstyled mb-0">
      <li><a href="#genai-1">Requirement</a></li>
      <li><a href="#genai-2">Problem</a></li>
      <li><a href="#genai-3">Data preparation</a></li>
      <li><a href="#genai-4">Model development</a></li>
      <li><a href="#genai-5">Evaluation</a></li>
      <li><a href="#genai-6">Serving</a></li>
    </ul>
  </div>
</div>

<div class="card mb-4" id="genai-1">
  <div class="card-body">
    <h2 class="card-title">Requirement</h2>
    <ul>
      <li>Business</li>
      <ul>
        <li>What should the model do? Automate chat responses that meet business purpose</li>
        <li>Who uses the model? Users who want to ask questions to business</li>
        <li>Should the system support text, image, video? Yes</li>
      </ul>
      <li>Data</li>
      <ul>
        <li>Do we have training data available? We have user feedback on chat responses</li>
      </ul>
      <li>Constraints</li>
      <ul>
        <li>How fast should response be? very fast (under 200ms)</li>
        <li>Should the model be trained continuously? Depends but chat response should contain up-to-date general information</li>
      </ul>
      <li>Estimation</li>
      <ul>
        <li>How many users should the system expect? 100M</li>
      </ul>
    </ul>
  <div class="card-footer text-muted">
    Reference:
  </div>
</div>

<div class="card mb-4" id="genai-2">
  <div class="card-body">
    <h2 class="card-title">Problem</h2>

    <h3 class="card-title">ML objective</h3>
    <ul>
      <li>Given questions, generate appropriate responses</li>
      <li>Input</li>
      <ul>
        <li>Prompt</li>
      </ul>
      <li>Output</li>
      <ul>
        <li>Answer</li>
      </ul>
    </ul>

    <h3 class="card-title">ML category</h3>
    <ul>
      <li>Option 1. pre-training</li>
      <ul>
        <li>Pros</li>
        <ul>
          <li>Accurate</li>
        </ul>
        <li>Cons</li>
        <ul>
          <li>Very expensive</li>
        </ul>
      </ul>
      <li>Option 2. fine-tuning</li>
      <ul>
        <li>Pros</li>
        <ul>
          <li>Higher quality than prompt engineering (few shot learning) since there is no need to provide big prompt that has context</li>
        </ul>
        <li>Cons</li>
        <ul>
          <li>Still expensive</li>
          <li>May replace existing knowledge with new knowledge</li>
        </ul>
      </ul>
      <li>Option 3. retrieval augmented generation</li>
      <ul>
        <li>Pros</li>
        <ul>
          <li>No training is required</li>
        </ul>
        <li>Cons</li>
        <ul>
          <li>User query goes through intermediate hops, which will increase latency</li>
        </ul>
      </ul>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: 
  </div>
</div>

<div class="card mb-4" id="genai-3">
  <div class="card-body">
    <h2 class="card-title">Data preparation</h2>

    <h3 class="card-title">Data engineering</h3>
    <ul>
      <li>Training data</li>
      <ul>
        <li>Documents that has domain specific knowledge to business</li>
      </ul>
      <li>Database</li>
      <ul>
        <li>Embedding</li>
        <ul>
          <li>chunk</li>
          <li>embedding</li>
        </ul>
      </ul>
    </ul>

    <h3 class="card-title">Feature engineering</h3>
    <ul>
      <li>In RAG, there is no feature engineering</li>
    </li>
  </div>
  <div class="card-footer text-muted">
    Reference: 
  </div>
</div>

<div class="card mb-4" id="genai-4">
  <div class="card-body">
    <h2 class="card-title">Model development</h2>

    <h3 class="card-title">Model selection</h3>
    <ul>
      <li>Option 1. pre-training</li>
      <ul>
        <li>Option 1a. Llama 3</li>
      </ul>
      <li>Option 2. fine-tuning</li>
      <ul>
        <li>Option 2a. low ranking adaption</li>
        <li>Option 2b. proximal policy optimization</li>
        <li>Option 2c. reinforcement learning with human feedback</li>
        <li></li>
      </ul>
      <li>Option 3. retrival augmented generation</li>
      <ul>
        <li>Chunking</li>
        <ul>
          <li>Option 1. fixed-size chunking</li>
          <ul>
            <li>Fixed number of tokens in each chunk</li>
            <li>Fixed number of overlapping tokens allowed in each chunk</li>
          </ul>
          <li>Option 2. sentence chunking</li>
          <ul>
            <li>Ex. Python libraries such as NLTK (Natural Language Toolkit), spaCy</li>
          </ul>
          <li>Option 3. semantic chunking</li>
          <ul>
            <li>Split texts into sentences or paragraphs</li>
            <li>Create windows with a fixed size of tokens</li>
            <li>Merge windows that are within certain cosine similarity threshold</li>
          </ul>
        </ul>
        <li>Embedding</li>
        <ul>
          <li>Option 1. late chuncking</li>
          <ul>
            <li>Embed entire document into token-level representation</li>
            <li>Token-level representations are groupd into chunks</li>
            <li>Each chunk has contextual information from entire document</li>
            <li>Pros</li>
            <ul>
              <li>Context awareness</li>
            </ul>
            <li>Cons</li>
            <ul>
              <li>Requires long context models</li>
            </ul>
          </ul>
          <li>Option 2. late interaction</li>
          <ul>
            <li>Ex. ColBERT</li>
            <li>Generate token-level embeddings for document and query independently</li>
            <li>Compare similarity between query tokens and document tokens one by one</li>
            <li>Highest similarity score gets selected</li>
            <li>Pros</li>
            <ul>
              <li>High precision</li>
            </ul>
            <li>Cons</li>
            <ul>
              <li>Compute intensive</li>
            </ul>
          </ul>
          <li>Option 3. Sentence BERT</li>
          <ul>
            <li>Feed two sentences represented by matrix \( (\text{seq_len} \times d_{\text{model}}) \) as input embedding to two BERTs</li>
            <li>Compute avarage of embeddings of all words in sentence</li>
            <li>Two BERTs produce to sentence embeddings \( (\text{seq_len} \times d_{\text{model}}) \)</li>
            <li>Calculate cosine simliarity</li>
            <li>Loss (MSE) uses target cosine simliarity and calculated cosine simliarity</li>
          </ul>
        </ul>
        <li>Retriever</li>
        <ul>
          <li>Option 1. naive</li>
          <ul>
            <li>Exact KNN</li>
            <li>Compare query embedding with all embeddings to compute cosine similarity</li>
            <li>Sort by cosine similarity and keep top k</li>
            <li>Very slow \( \mathcal{O}(ND) \)</li>
          </ul>
          <li>Option 2. hierarchical navigable small worlds (HNSW)</li>
          <ul>
            <li>ANN</li>
            <li>Focused on recall</li>
            <li>Build a graph where nodes are embeddings and edges are connections between embeddings</li>
            <li>Number of connections that each node has should be small (For example, less than 6)</li>
            <li>When a query enters the graph, entry node is randomly chosen</li>
            <li>Compare similarity between query and random node versus similairty between query and neighboring nodes of the random node</li>
            <li>If random node has higher similarity with query, algorithm stops there</li>
            <li>Else, algorithm moves to the neighoboring node that has highest similarity with query, and repeats</li>
            <li>When the algorithm stops, all visited nodes are ordered based on similarity and top k is returned</li>
            <li>Search is repeated many time with different random entry nodes</li>
            <li>How to add a new sentence to graph?</li>
            <ul>
              <li>Do the search and connect the nodes with top k nodes based on cosine similarity</li>
            </ul>
            <li>SkipList</li>
            <ul>
              <li>Lists where indices of each list are connected by pointers as if nodes in linked lists</li>
              <li>Search and insert is \( \mathcal{O}(logn) \)</li>
            </ul>
          </ul>
        </ul>
        <li>Re-ranker</li>
        <ul>
          <li>Re-orders documents based on relevancy with the query</li>
          <li>Option 1. cross-encoder</li>
          <ul>
            <li>Both sentences are fed to network</li>
            <li>Value between \( 0 \) and \( 1 \) is produced</li>
            <li>Does not produce embedding (re-ranker does not needs to produce embedding, thus this is fine)</li>
          </ul>
        </ul>
      </ul>
      <li>RAG to support images</li>
      <ul>
        <li>Option 1. embedding service</li>
        <ul>
          <li>Use embedding service (for example, Cilp) to embed images</li>
          <li>Store image embeddings in vector DB</li>
          <li>Retriever searches both chunks and images</li>
          <li>Raw images and chunks are sent to multi-modal LLM</li>
        </ul>
        <li>Option 2. image summary service</li>
        <ul>
          <li>Use multi-modal LLM (for example, GPT-4V) to produce text summary of images</li>
          <li>Store image summary embeddings in vector DB</li>
          <li>Retriever searches both chunks and image summary</li>
          <li>Raw image is retrived from lookup table using its summary</li>
          <li>Raw images and chunks are sent to multi-modal LLM</li>
        </ul>
      </ul>
      <li>Agentic RAG to provide up-to-date information</li>
      <ul>
        <li>Master agent coodinates many specialized agents</li>
        <ul>
          <li>Agent that queries internal database</li>
          <li>Agent that queries user specific information such as chat history or email</li>
          <li>Agent that queries internet for up-to-date information</li>
        </ul>
        <li>Pros</li>
        <ul>
          <li>Better accuracy</li>
        </ul>
        <li>Cons</li>
        <ul>
          <li>Higher latency</li>
          <li>Complexity in error handling</li>
        </ul>
      </ul>
    </ul>

    <h3 class="card-title">Model training</h3>
    <ul>
      <li>There is no model training in RAG</li>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: Aman Chadha
  </div>
</div>

<div class="card mb-4" id="genai-5">
  <div class="card-body">
    <h2 class="card-title">Evaluation</h2>

    <h3 class="card-title">Retrieval</h3>
    <ul>
      <li>Context precision</li>
      <ul>
        <li>For each query, determine if chunks returned by retriever is relevant or not</li>
        <li>Compute precision @ k for each k in the retrieved chunks</li>
        <ul>
          <li>\( \text{precision @ k} = \dfrac{\text{true positive @ k}}{\text{true positive @ k} + \text{false positive @ k}} \)</li>
        </ul>
        <li>Compute context precision @ K by averaging precision @ k for all relevant chunk in top k results</li>
        <ul>
          <li>\( \text{context precision @ K} = \dfrac{\displaystyle\sum_{k=1}^{K} \text{precision @ k} \times v_{k}}{\text{total number of relevant chunks in top k result}} \)</li>
          <li>\( v_{k} \) is an indicator whether chunk is relevant or not</li>
          <li>\( K \) is total number of chunks</li>
        </ul>
      </ul>
      <li>Context recall</li>
      <ul>
          <li>\( \dfrac{\text{relevant chunks in ground truth}}{\text{total chunks in ground truth}} \)</li>
        </ul>
    </ul>

    <h3 class="card-title">Generation</h3>
    <ul>
      <li>Faithfulness</li>
      <ul>
        <li>\( \dfrac{\text{total number of claims}}{\text{number of clains that can be inferred from given context}} \)</li>
      </ul>
      <li>Answer relevance</li>
      <ul>
        <li>Calculate mean cosine similarity between original question and generated (artificial) questions that are reverse engineered based on provided answers</li>
        <li>\( \dfrac{1}{N}\displaystyle\sum_{i=1}^{N} \cos(E_{g_{i}}, E_{o}) = \dfrac{1}{N}\displaystyle\sum_{i=1}^{N} \dfrac{E_{g_{i}} \cdot E_{o}}{\|E_{g_{i}}\|\|{E_{o}}\|} \)</li>
        <li>\( E_{g_{i}} \) is embedding of generated question \( i \)</li>
        <li>\( E_{o} \) is embedding of original question</li>
        <li>\( N \) is number of generated question (typically 3)</li>
      </ul>
    </ul>

    <h3 class="card-title">End-to-end</h3>
    <ul>
      <li>Summarization score</li>
      <ul>
        <li>\( \text{QA score} = \dfrac{\text{number of correctly answered questions}}{\text{total number of questions}} \)</li>
        <li>\( \text{consciousness score} = \dfrac{\text{length of summary}}{\text{length of context}} \)</li>
        <li>\( \text{summarization score} = \dfrac{\text{QA score+consciousness score}}{\text{2}} \)</li>
      </ul>
      <li>Context entities recall</li>
      <ul>
        <li>\( \dfrac{|CE \cap GE|}{|GE|} \)</li>
        <li>\( CE \) - entities present in provided answer</li>
        <li>\( GE \) - entities present in ground truth</li>
        <li>\( |CE \cap GE| \) - entities that are both in provided answer and ground truth</li>
      </ul>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: Aman Chadha
  </div>
</div>

<div class="card mb-4" id="genai-6">
  <div class="card-body">
    <h2 class="card-title">Serving</h2>

    <img class="img-fluid" class="card-img-top" src="/machine-learning/image/machine-learning-system-design-0/genai-1.png" alt="Card image cap">
    <h3>Data ingestion</h3>
    <ul>
      <li>Text splitter</li>
      <ul>
        <li>Split documents into chunks (chunks are usually single sentences)</li>
        <li>We want to pass only the relevant texts to LLM</li>
      </ul>
      <li>Embedding</li>
      <ul>
        <li>Create embedding for each chunk</li>
        <li>We want to use embedding to find the most relevant chunks</li>
      </ul>
      <li>Vector database</li>
      <ul>
        <li>Stores embeddings of chunks</li>
      </ul>
    </ul>

    <img class="img-fluid" class="card-img-top" src="/machine-learning/image/machine-learning-system-design-0/genai-2.png" alt="Card image cap">
    <h3>Query</h3>
    <ul>
      <li>Query expansion</li>
      <ul>
        <li>Produces standalone questions from chat history</li>
        <li>Can include spelling correction, adding additional keyword, etc</li>
        <li>Query is transformned into embedding</li>
      </ul>
      <li>Retriever</li>
      <ul>
        <li>Executes search on vector database using embedding of standalone prompt</li>
        <li>Uses variants of NN to search similar embeddings</li>
      </ul>
      <li>Re-ranker</li>
      <ul>
        <li>Takes \( k \) candidate chunks from retriever and come up with smaller subset</li>
        <li>Focused on optimizing precision while maintaining recall</li>
      </ul>
      <li>System prompt</li>
      <ul>
        <li>A template</li>
        <li>Query and chucks are added to system propmt, then fed to LLM</li>
      </ul>
      <li>LLM</li>
      <ul>
        <li>Original document of most relevant sections plus a system prompt are sent to LLM</li>
        <li>LLM response includes citations to the referenced documents</li>
      </ul>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: Umar Jamil
  </div>
</div>
<!-- Gen AI END -->

</div> <!-- /.col-md-12 -->
</div> <!-- /.row -->
</div> <!-- /.container -->

<include src="/footer.html"></include>

</body>

</html>