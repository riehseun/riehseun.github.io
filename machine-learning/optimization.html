<!DOCTYPE html>

<html lang="en">

<head>

<!-- Metadata -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="">
<meta name="author" content="Seungmoon Rieh">
<meta name="keywords" content="">

<!-- Title and image -->
<title>Seungmoon Rieh</title>
<link href="/img/seungmoonrieh.jpg" rel="icon">

<!-- CSS -->
<link href="/css/bootstrap.min.css" rel="stylesheet">
<link href="/css/monokai-sublime.css" rel="stylesheet">
<link href="/css/site.css" rel="stylesheet">

<!-- JavaScript -->
<script src="/js/jquery.min.js"></script>
<script src="/js/bootstrap.bundle.min.js"></script>
<script src="/js/highlight.pack.js"></script>
<script type="text/javascript" src="/js/include_html.js"></script>
<script type="text/javascript" src="/js/site.js"></script>

</head>

<body>

<include src="/header.html"></include>

<div class="container">
<div class="row">
<div class="col-md-12">
<h1 class="my-4">Machine Learning</h1>

<!-- Optimization BEGIN -->
<div class="card mb-4" id="subject">
  <div class="card-body">
    <h2 class="card-title">Optimization</h2>
    <p class="card-text"></p>
    <ul class="list-unstyled mb-0">
      <li><a href="#optimization-1">Gradient descent</a></li>
      <li><a href="#optimization-2">Momentum</a></li>
      <li><a href="#optimization-3">RMSprop</a></li>
      <li><a href="#optimization-4">Adam (Adaptive moment estimation)</a></li>
    </ul>
  </div>
</div>

<div class="card mb-4" id="optimization-1">
  <div class="card-body">
    <h2 class="card-title">Gradient descent</h2>
    <ul>
      <li>Find values of parameters of function that minimize a cost function.</li>
      <li>Used when parameters cannot be calculated analytically.</li>
      <li>Take negative of gradients of a function at a point.</li>
      <li>Repeatedly update that point until reaching an optimal point.</li>
      <li>Batch: run through all samples to do single update of parameters.</li>
      <li>Min-batch: run through subset of samples to do single update of parameters.</li>
      <li>Stochastic: run through one sample to do single update of parameters.</li>
    </ul>

<pre><code class="python"># Batch.
for t in range(steps):
    dw = gradient(loss, data, w)
    w = w - learning_rate * dw</code></pre>

<pre><code class="python"># Min-batch.
for t in range(steps):
    for mini_batch in get_batches(data, batch_size):
        dw = gradient(loss, mini_batch, w)
        w = w - learning_rate * dw</code></pre>

<pre><code class="python"># Stochastic.
for t in range(steps):
    for example in data:
        dw = gradient(loss, example, w)
        w = w - learning_rate * dw</code></pre>

    <h3 class="card-title">Learning rate decay</h3>
    <ul>
      <li>Helps gradient descent converge by taking smaller steps as it approaches the minimum.</li>
    </ul>

    <h3 class="card-title">Local optima</h3>
    <ul>
      <li>In high dimensions, when gradient is 0, it is almost always saddle points rather than local optima. (so it is unlikely for the optimization algorithm to stuck at bad local optima)</li>
      <li>Plateaus (where derivatives are close to 0) can slow down the learning.</li>
    </ul>

<pre><code class="python">import torch
import torch.nn as nn

def train():

    model = nn.Linear(4,2)

    criterion = torch.nn.MSELoss()
    optimizer = torch.optim.SGD(model.parameters(), lr=0.1)

    for epoch in range(10):
        # Convert inputs and labels to variable.
        inputs = torch.Tensor([0.8,0.4,0.4,0.2])
        labels = torch.Tensor([1,0])

        # Clear gradient buffers because we don't want any gradient from previous epoch to carry forward.
        optimizer.zero_grad()

        # Get output from the model from the inputs.
        outputs = model(inputs)

        # Get loss for the predicted output.
        loss = criterion(outputs, labels)
        print(loss)

        # Get gradients w.r.t to parameters.
        loss.backward()

        # Update parameters.
        optimizer.step()

        print('epoch {}, loss {}'.format(epoch, loss.item()))

if __name__ == "__main__":
    train()</code></pre>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://www.deeplearning.ai/deep-learning-specialization/">Deep Learning Specialization</a> | <a href="https://www.educative.io/path/become-a-deep-learning-professional">Become a Deep Learning Professional</a>
  </div>
</div>

<div class="card mb-4" id="optimization-2">
  <div class="card-body">
    <h2 class="card-title">Momentum</h2>
    <ul>
      <li>Enforce gradients to move in the same direction as previous timesteps.</li>
      <li>Velocity: running mean of gradients including the direction.</li>
      <li>Friction: a constant.</li>
      <li>At every step, update velocity, then update weights with the velocity.</li>
      <li>Can escape local minimums or saddle points because gradients keep moving downwards even though gradients are zero.</li>
      <li>Reduce oscilation by slowing learning vertially but speeding up learning horizontally.</li>
    </ul>

<pre><code class="python">for t in range(steps):
    dw = gradient(loss, w)
    v = rho * v + dw
    w = w - learning_rate * v</code></pre>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://www.deeplearning.ai/deep-learning-specialization/">Deep Learning Specialization</a> | <a href="https://www.educative.io/path/become-a-deep-learning-professional">Become a Deep Learning Professional</a>
  </div>
</div>

<div class="card mb-4" id="optimization-3">
  <div class="card-body">
    <h2 class="card-title">RMSprop</h2>
    <ul>
      <li>Adaptive learning rate: smaller updates for frequent features and bigger updates for infrequent features.</li>
      <li>Keep running sum of squares of gradients in each dimension.</li>
      <li>Decay the sum of the previous squared gradients.</li>
    </ul>

<pre><code class="python">for t in range(steps):
    dw = gradient(loss, w)
    squared_gradients = decay_rate * squared_gradients + (1-decay_rate) * dw * dw
    w = w - learning_rate * (dw/(squared_gradients.sqrt()+e)</code></pre>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://www.deeplearning.ai/deep-learning-specialization/">Deep Learning Specialization</a> | <a href="https://www.educative.io/path/become-a-deep-learning-professional">Become a Deep Learning Professional</a>
  </div>
</div>

<div class="card mb-4" id="optimization-4">
  <div class="card-body">
    <h2 class="card-title">Adam (Adaptive moment estimation)</h2>
    <ul>
      <li>Keep track of two running variables, velocity and the squared gradients average.</li>
      <li>At t=0, velocity is close to 0, resulting in very big change in gradients. Add biases in moments to take smaller steps in the beginning.</li>
    </ul>

<pre><code class="python">for t in range(steps):
    dw = gradient(loss, w)
    moment1 = delta1 * moment1 + (1-delta1) * dw
    moment2 = delta2 * moment2 + (1-delta2) * dw * dw
    moment1_unbiased = moment1 / (1-delta1**t)
    moment2_unbiased = moment2 / (1-delta2**t)
    w = w - learning_rate * moment1_unbiased / (moment2_unbiased.sqrt()+e)</code></pre>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://www.deeplearning.ai/deep-learning-specialization/">Deep Learning Specialization</a> | <a href="https://www.educative.io/path/become-a-deep-learning-professional">Become a Deep Learning Professional</a>
  </div>
</div>
<!-- Optimization END -->

</div> <!-- /.col-md-12 -->
</div> <!-- /.row -->
</div> <!-- /.container -->

<include src="/footer.html"></include>

</body>

</html>