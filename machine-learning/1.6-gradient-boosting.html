<!DOCTYPE html>

<html lang="en">

<head>

<!-- Metadata -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="">
<meta name="author" content="Seungmoon Rieh">
<meta name="keywords" content="">

<!-- Title and image -->
<title>Seungmoon Rieh</title>
<link href="/img/seungmoonrieh.jpg" rel="icon">

<!-- CSS -->
<link href="/css/bootstrap.min.css" rel="stylesheet">
<link href="/css/monokai-sublime.css" rel="stylesheet">
<link href="/css/site.css" rel="stylesheet">

<!-- JavaScript -->
<script src="/js/jquery.min.js" type="text/javascript"></script>
<script src="/js/bootstrap.bundle.min.js" type="text/javascript"></script>
<script src="/js/highlight.pack.js" type="text/javascript"></script>
<script src="/js/include_html.js" type="text/javascript"></script>
<script src="/js/mathjax/tex-chtml.js" type="text/javascript"></script>
<script src="/js/site.js" type="text/javascript"></script>

</head>

<body>

<include src="/header.html"></include>

<div class="container">
<div class="row">
<div class="col-md-12">
<h1 class="my-4">Machine Learning</h1>

<!-- Gradient boosting BEGIN -->
<div class="card mb-4" id="subject">
  <div class="card-body">
    <h2 class="card-title">Gradient boosting</h2>
    <ul class="list-unstyled mb-0">
      <li><a href="#gradient-boosting-">Gradient boosting</a></li>
      <li><a href="#gradient-boosting-">Hyperparameters</a></li>
    </ul>
  </div>
</div>

<div class="card mb-4" id="gradient-boosting-">
  <div class="card-body">
    <h2 class="card-title">Gradient boosting</h2>
    <ul>
      <li>AdaBoost</li>
      <ul>
        <li>Each new tree adjusts its weights based on errors from the previous trees</li>
      </ul>
      <li>Gradient boosting</li>
      <ul>
        <li>Fits each new tree entirely based on errors from the previous trees</li>
        <li>Computes the residuals of each tree's predictions and sums all the residuals to score the model</li>
      </ul>
    </ul>

<pre><code class="python">import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')
from sklearn.model_selection import train_test_split

df_bikes = pd.read_csv('bike_rentals_cleaned.csv')
X_bikes = df_bikes.iloc[:,:-1]
y_bikes = df_bikes.iloc[:,-1]
X_train, X_test, y_train, y_test = train_test_split(X_bikes, y_bikes, random_state=2)</code></pre>

<pre><code class="python"></code></pre>
  </div>
  <div class="card-footer text-muted">
    Reference: Hands-On Gradient Boosting with XGBoost and scikit-learn, Corey Wade
  </div>
</div>

<div class="card mb-4" id="gradient-boosting-">
  <div class="card-body">
    <h2 class="card-title">Hyperparameters</h2>
    <ul>
      <li><code>learning_rate</code></li>
      <ul>
        <li><pre><code class="python">n_estimators = [30, 300, 3000]
learning_rates = [0.001, 0.01, 0.05, 0.1, 0.15, 0.2, 0.3, 0.5, 1.0]
for i in n_estimators:
    for j in learning_rates:
        gbr = GradientBoostingRegressor(max_depth=2, n_estimators=i, random_state=2, learning_rate=i)
        gbr.fit(X_train, Y_train)
        y_pred = gbr.predict(X_test)
        rmse = MSE(y_test, y_pred) ** 0.5</code></pre></li>
      </ul>
      <li><code>max_depth</code></li>
      <li><code>subsample</code></li>
      <ul>
        <li>Each tree only select certain percentages of samples</li>
        <li>When subsample is not 1.0, it is considered as stochastic gradient descent</li>
      </ul>
      <li>RandomizedSearchCV</li>
      <ul>
        <li><pre><code class="python">from sklearn.model_selection import RandomizedSearchCV
params = {'subsample': [0.65, 0.7, 0.75], 'n_estimators': [300, 500, 1000], 'learning_rate': [0.05, 0.075, 0.1]}
gbr = GradientBoostingRegressor(max_depth=3, random_state=2)
rand_reg = RandomizedSearchCV(gbr, params, n_iter=10, scoring='neg_mean_squared_error', cv=5, n_jobs=1, random_state=2)
rand_reg.fit(X_train, Y_train)
best_model = rand_reg.best_estimator_
best_params = rand_reg.best_params_
best_score = np.sqrt(-rand_reg.best_score_)
y_pred = best_model.predict(X_test)
rmse_test = MSE(y_test, y_pred) ** 0.5</code></pre></li>
      </ul>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: Hands-On Gradient Boosting with XGBoost and scikit-learn, Corey Wade
  </div>
</div>
<!-- Gradient boosting END -->

</div> <!-- /.col-md-12 -->
</div> <!-- /.row -->
</div> <!-- /.container -->

<include src="/footer.html"></include>

</body>

</html>