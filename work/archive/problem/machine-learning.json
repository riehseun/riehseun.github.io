[
    {
      "question": "Explain how to explain machine learning to a kid"
    },
    {
      "question": "Explain difference between supervised, unsupervised, and reinforcement learning"
    },
    {
      "question": "Explain regression algorithm"
    },
    {
      "question": "Explain classification algorithm"
    },
    {
      "question": "Explain linear regression"
    },
    {
      "question": "Explain logistic regression"
    },
    {
      "question": "Explain neural network algorithm"
    },
    {
      "question": "Explain KNN"
    },
    {
      "question": "Explain ANN"
    },
    {
      "question": "Explain clustering algorithm"
    },
    {
      "question": "Explain k-means clustering"
    },
    {
      "question": "Explain convergence in k-means clustering"
    },
    {
      "question": "Explain how to choose optimul number of clusters"
    },
    {
      "question": ""
    },
    {
      "question": ""
    },
    {
      "question": "Explain why we do feature selection"
    },
    {
      "question": "Explain feature selection technique"
    },
    {
      "question": "Explain SHAP (SHapley Additive exPlanations)"
    },
    {
      "question": "Explain feature scaling"
    },
    {
      "question": "Explain how to encode categorical data"
    },
    {
      "question": "Explain how to handle missing data"
    },
    {
      "question": "Explain how to handle imbalanced dataset"
    },
    {
      "question": "Explain how to avoid data leakage"
    },
    {
      "question": ""
    },
    {
      "question": ""
    },
    {
      "question": "Explain bias-variance trade-off"
    },
    {
      "question": "Explain underfitting and how to tackle it"
    },
    {
      "question": "Explain overfitting and how to tackle it"
    },
    {
      "question": "Explain why we use validation and test set"
    },
    {
      "question": "Explain cross-validation"
    },

    {
      "question": "Explain how to debug model not performing well on test data"
    },
    {
      "question": "Explain how to perfrom error analysis"
    },
    {
      "question": ""
    },
    {
      "question": ""
    },
    {
      "question": "Explain confusion matrix"
    },
    {
      "question": "Explain difference between precision and recall"
    },
    {
      "question": "Explain difference between PR-AUC and ROC-AUC"
    },
    {
      "question": "Explain accuracy"
    },
    {
      "question": "Explain F1 score"
    },
    {
      "question": "Explain how to find threshold for classifier"
    },
    {
      "question": "Explain when to use which metric"
    },
    {
      "question": ""
    },
    {
      "question": ""
    },
    {
      "question": "Explain cost function"
    },
    {
      "question": "Explain difference between epoch, batch, and iteration"
    },
    {
      "question": "Explain difference between single-layer perception and multi-layer perception"
    },
    {
      "question": "Explain what is meant by depth in neural network"
    },
    {
      "question": "Explain significance of bias term in neural network"
    },
    {
      "question": "Explain challenges with very deep neural network"
    },
    {
      "question": "Explain why we shuffle training data after each epoch"
    },
    {
      "question": "Explain hyperparameter tuning"
    },
    {
      "question": "Explain which hyperparameter are important"
    },
    {
      "question": "Explain logits"
    },
    {
      "question": ""
    },
    {
      "question": ""
    },
    {
      "question": "Explain activation function"
    },
    {
      "question": "Explain why ReLu is used more than sigmoid in neural network"
    },
    {
      "question": ""
    },
    {
      "question": ""
    },
    {
      "question": "Explain normalization and why we use it"
    },
    {
      "question": "Explain difference between normalization and standardization"
    },
    {
      "question": "Explain difference between batch normalization and layer normalization"
    },
    {
      "question": ""
    },
    {
      "question": ""
    },
    {
      "question": "Explain weight initialization methods like Xavier, He"
    },
    {
      "question": "Explain what happens if weights are initialized to same values"
    },
    {
      "question": "Explain weight constraints and how it can benefit training"
    },
    {
      "question": ""
    },
    {
      "question": ""
    },
    {
      "question": "Explain regularization and why we use it"
    },
    {
      "question": "Explain difference between L1 and L2 regularization"
    },
    {
      "question": "Explain dropout"
    },
    {
      "question": "Explain how early stopping prevents overfitting"
    },
    {
      "question": ""
    },
    {
      "question": ""
    },
    {
      "question": "Explain batch size and how it is related to convergence"
    },
    {
      "question": "Explain vanishing and exploding gradient"
    },
    {
      "question": "Explain how to deal with vanishing gradient"
    },
    {
      "question": "Explain how to deal with exploding gradient"
    },
    {
      "question": "Explain momentum"
    },
    {
      "question": "Explain RMSProp"
    },
    {
      "question": "Explain ADAM optimizer"
    },
    {
      "question": "Explain difference between batch, min-batch, and stochastic gradient descent"
    },
    {
      "question": "Explain difference between global optima and local optima"
    },
    {
      "question": "Explain gradient clipping and how it help training"
    },
    {
      "question": ""
    },
    {
      "question": ""
    },
    {
      "question": "Explain learning rate"
    },
    {
      "question": "Explain adaptive learning rate"
    },
    {
      "question": "Explain learning rate scheduler"
    },
    {
      "question": "Explain why learning rate is considered as most important hyperparameter"
    },
    {
      "question": ""
    },
    {
      "question": ""
    },
    {
      "question": "Explain how convolution works, when inputs are gray-scale or RGB"
    },
    {
      "question": "Explain why we use convolution layer rather than just FC layers"
    },
    {
      "question": "Explain why we use many small kernels like 3 by 3 rather than few large kernels"
    },
    {
      "question": "Explain why we use max-pooling and how it is different from average pooling"
    },
    {
      "question": "Explain ResNet and skip connection"
    },
    {
      "question": "Explain non-max suppression"
    },
    {
      "question": "Explain data augmentation"
    },
    {
      "question": "Explain autoencoder"
    },
    {
      "question": ""
    },
    {
      "question": ""
    },
    {
      "question": "Explain RNN"
    },
    {
      "question": "Explain difference between feed forward network and recurrent network"
    },
    {
      "question": "Explain LSTM"
    },
    {
      "question": "Explain how LSTM handle vanishing gradient"
    },
    {
      "question": "Explain word embedding and how it is different from one-hot encoding"
    },
    {
      "question": "Explain sentence embedding"
    },
    {
      "question": "Explain tokenization"
    },
    {
      "question": "Explain Word2Vec"
    },
    {
      "question": "Explain CBOW"
    },
    {
      "question": "Explain Skip-gram"
    },
    {
      "question": ""
    },
    {
      "question": ""
    },
    {
      "question": "Explain transfer learning and its benefits"
    },
    {
      "question": "Explain pre-training and fine-tuning"
    },
    {
      "question": "Explain self-attention"
    },
    {
      "question": "Explain attention mask"
    },
    {
      "question": "Explain transformer"
    },
    {
      "question": "Explain drawbacks of transformer"
    },
    {
      "question": "Explain regularization used in transformer"
    },
    {
      "question": "Explain how to achieve fast inference on transformer model"
    },
    {
      "question": "Explain BERT"
    },
    {
      "question": "Explain GPT-3"
    },
    {
      "question": "Explain RAG"
    },
    {
      "question": "Explain LoRA"
    },
    {
      "question": "Explain PPO"
    },
    {
      "question": "Explain RLHF"
    },
    {
      "question": "Explain multi-modal models"
    },
    {
      "question": "Explain how stability diffusion model use LLM to understand complex text prompts"
    },
    {
      "question": "Explain how to train LLM with billions of parameters"
    },
    {
      "question": "Explain how to train LLM to prevent hallucinations"
    },
    {
      "question": "Explain how to prevent bias and harmful prompt"
    },
    {
      "question": "Explain how knowledge distillation benefits LLM"
    },
    {
      "question": "Explain few shot learning in LLM"
    },
    {
      "question": "Explain how to evaluate LLM performance"
    },
    {
      "question": "Explain how to improve factual accuracy of LLM"
    },
    {
      "question": "Explain how to detect drift in LLM performance over time"
    },
    {
      "question": "Explain how to curate dataset for training LLM"
    },
    {
      "question": "Explain how to fine-tune LLM for domain specific applications like finance"
    },
    {
      "question": ""
    },
    {
      "question": ""
    },
    {
      "question": "Explain decision tree"
    },
    {
      "question": "Explain bagging"
    },
    {
      "question": "Explain boosting"
    },
    {
      "question": "Explain information gain and entropy"
    },
    {
      "question": "Explain Why emsembles have higher scores than individual models"
    },
    {
      "question": "EExplain gradient boosting or GBM, and its pros and cons"
    },
    {
      "question": "Explain why XGBoost is not suitable for continual learning"
    },
    {
      "question": "Explain why XGBoost is good at dealing with sparse data"
    },
    {
      "question": ""
    },
    {
      "question": ""
    }
]
  