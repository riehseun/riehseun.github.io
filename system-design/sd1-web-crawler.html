<!DOCTYPE html>

<html lang="en">

<head>

<!-- Metadata -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="">
<meta name="author" content="Seungmoon Rieh">
<meta name="keywords" content="">

<!-- Title and image -->
<title>Seungmoon Rieh</title>
<link href="/img/seungmoonrieh.jpg" rel="icon">

<!-- CSS -->
<link href="/css/bootstrap.min.css" rel="stylesheet">
<link href="/css/monokai-sublime.css" rel="stylesheet">
<link href="/css/site.css" rel="stylesheet">

<!-- JavaScript -->
<script src="/js/jquery.min.js" type="text/javascript"></script>
<script src="/js/bootstrap.bundle.min.js" type="text/javascript"></script>
<script src="/js/highlight.pack.js" type="text/javascript"></script>
<script src="/js/include_html.js" type="text/javascript"></script>
<script src="/js/mathjax/tex-chtml.js" type="text/javascript"></script>
<script src="/js/site.js" type="text/javascript"></script>

</head>

<body>

<include src="/header.html"></include>

<div class="container">
<div class="row">
<div class="col-md-12">
<h1 class="my-4">Software Engineering</h1>

<!-- Web Crawler BEGIN -->
<div class="card mb-4" id="web-crawler">
  <div class="card-body">
    <h2 class="card-title">Web Crawler</h2>
    <ul class="list-unstyled mb-0">
      <li><a href="#web-crawler-1">Understand the problem and establish design scope</a></li>
      <li><a href="#web-crawler-2">Propose high-level design and get buy-in</a></li>
      <li><a href="#web-crawler-3">Design deep dive</a></li>
      <li><a href="#web-crawler-4">Wrap up</a></li>
    </ul>
  </div>
</div>

<div class="card mb-4" id="web-crawler-1">
  <div class="card-body">
    <h2 class="card-title">Understand the problem and establish design scope</h2>
    <ul>
      <li>What is the purpose of the application? Search engine indexing</li>
      <li>Should the system work with mutiple content types? No, HTML only</li>
      <li>Should the system handle duplicate content? Duplicate content should be ignored</li>
    </ul>

    <h3 class="card-title">Estimation</h3>
    <ul>
      <li>How many web pages to crawl? 1B pages per month</li>
      <li>What is the size of each HTML page? 100KB including metadata</li>
      <li>How long does it take to traverse one page? 100ms</li>
      <li>Do we need to store HTML page? Yes for 5 years</li>
    </ul>

    <h3 class="card-title">Storage</h3>
    <ul>
      <li>1B per month * 100KB = 100TB per month</li>
      <li>100TB per month = 6PB for 5 years</li>
    </ul>

    <h3 class="card-title">Bandwidth</h3>
    <ul>
      <li>Incoming</li>
      <ul>
        <li>100TB * 8 / (86400 * 30) = 3Gbps</li>
      </ul>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: System Design Interview, Alex Xu
  </div>
</div>

<div class="card mb-4" id="web-crawler-2">
  <div class="card-body">
    <h2 class="card-title">Propose high-level design and get buy-in</h2>

    <img class="img-fluid" class="card-img-top" src="/system-design/image/ref-sd-1/web-crawler-1.png" alt="Card image cap">

    <ul>
      <li>Seed URLs</li>
      <ul>
        <li>Base URLs</li>
      </ul>
      <li>URL frontier (priority queue)</li>
      <ul>
        <li>Contains all the remaining URLs to download</li>
      </ul>
      <li>Service host</li>
      <ul>
        <li>Each worker dequeues URL from the URL frontier</li>
        <li>Each worker uses DNS resolver to acquire web page's IP address</li>
      </ul>
      <li>DNS resolver</li>
      <ul>
        <li>Custom DNS resolver is needed because DNS lookup is time consuming process</li>
        <li>Cache frequently used IP addresses within their TTL</li>
      </ul>
      <li>Content parser</li>
      <ul>
        <li>HTML is parsed and validated</li>
      </ul>
      <li>Duplicate HTML eliminator</li>
      <ul>
        <li>Checks if document is duplicate by checking the hash values of two web pages</li>
      </ul>
      <li>URL extractor</li>
      <ul>
        <li>Extracts links from HTML pages</li>
      </ul>
      <li>URL filter</li>
      <ul>
        <li>Exclude certain content types, file extensions, and blacklisted sites</li>
      </ul>
      <li>Duplicate URL eliminator</li>
      <ul>
        <li>Bloom filter or hash table is used</li>
      </ul>
      <li>URL storage</li>
      <ul>
        <li>Stores already visited URLs</li>
      </ul>
    </ul>

    <ul>
      <li>Add seed URLs to URL frontier</li>
      <li>Service host fetches URLs from URL frontier</li>
      <li>Service host gets IP addresses of URLs from DNS resolver and starts downloading</li>
      <li>Content parser validates HTML pages</li>
      <li>Duplicate HTML eliminator checks if HTML is already in storage</li>
      <li>Duplicate URL eliminator checks if URL is already in storage</li>
      <ul>
        <li>If URL is not duplicate, it is added to URL frontier</li>
      </ul>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: System Design Interview, Alex Xu
  </div>
</div>

<div class="card mb-4" id="web-crawler-3">
  <div class="card-body">
    <h2 class="card-title">Design deep dive</h2>

    <img class="img-fluid" class="card-img-top" src="/system-design/image/ref-sd-1/web-crawler-2.png" alt="Card image cap">

    <ul>
      <li>Algorithm</li>
      <ul>
        <li>Use BFS</li>
        <li>DFS is not a good choice because depth can be very deep</li>
      </ul>
      <li>Priority</li>
      <ul>
        <li>Prioritizer computes priority of each URL</li>
        <li>Each queue is assigned a priority</li>
        <li>Queue selector has higher probability of picking queues with higher priority</li>
      </ul>
      <li>Politeness</li>
      <ul>
        <li>Download one page at a time from the same host</li>
        <li>Each thread in service host has a queue and download URLs from that queue</li>
        <li>Queue router ensures that each queue only contains URLs from the same host</li>
        <li>Mapping table maps host (ex. wikipedia.com) to queue</li>
        <li>Queue selector rotates worker threads that downloads webpages</li>
      </ul>
      <li>Freshness</li>
      <ul>
        <li>Recrawl is needed because webpages are constantly updated</li>
        <li>Recrawl URLs with higher priority first</li>
      </ul>
      <li>Storage</li>
      <ul>
        <li>Due to huge size of URLs, need to store URLs into a disk</li>
        <li>But, disk is slow, thus we need buffers in memory</li>
        <ul>
          <li>Enqueue buffer - once filled, written to disk</li>
          <li>Dequeue buffer - keeps cache of URLs to be visited. It periodically reads from disk to fill the buffer</li>
        </ul>
      </ul>
    </ul>

    <h3 class="card-title">Service host</h3>
    <ul>
      <li>Robots.txt</li>
      <ul>
        <li>Specifies pages that are not allowed to be downloaded</li>
      </ul>
      <li>Performance optimization</li>
      <ul>
        <li>Crawl jobs are distributed into multiple servers, and each server runs multiple threads</li>
        <li>Response from DNS resolver is cached in Service host</li>
        <li>Place crawl servers close to website hosts in terms of geographical region</li>
        <li>Enforce maximum wait time from the host</li>
      </ul>
      <li>Robustness</li>
      <ul>
        <li>Use consistent hashing to distribute load among crawl jobs</li>
        <li>Save crawl states into disk (When failure happens, can resume from the saved state)</li>
      </ul>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: System Design Interview, Alex Xu
  </div>
</div>

<div class="card mb-4" id="web-crawler-4">
  <div class="card-body">
    <h2 class="card-title">Wrap up</h2>
    <ul>
      <li>Explain how to handle dynamically generated links</li>
      <li>Explain how to filter out unwanted pages</li>
      <li>Explain how to perform data analytics</li>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: System Design Interview, Alex Xu
  </div>
</div>
<!-- Web Crawler END -->

</div> <!-- /.col-md-12 -->
</div> <!-- /.row -->
</div> <!-- /.container -->

<include src="/footer.html"></include>

</body>

</html>