<!DOCTYPE html>

<html lang="en">

<head>

<!-- Metadata -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="">
<meta name="author" content="Seungmoon Rieh">
<meta name="keywords" content="">

<!-- Title and image -->
<title>Seungmoon Rieh</title>
<link href="/img/seungmoonrieh.jpg" rel="icon">

<!-- CSS -->
<link href="/css/bootstrap.min.css" rel="stylesheet">
<link href="/css/monokai-sublime.css" rel="stylesheet">
<link href="/css/site.css" rel="stylesheet">

<!-- JavaScript -->
<script src="/js/jquery.min.js" type="text/javascript"></script>
<script src="/js/bootstrap.bundle.min.js" type="text/javascript"></script>
<script src="/js/highlight.pack.js" type="text/javascript"></script>
<script src="/js/include_html.js" type="text/javascript"></script>
<script src="/js/mathjax/tex-chtml.js" type="text/javascript"></script>
<script src="/js/site.js" type="text/javascript"></script>

</head>

<body>

<include src="/header.html"></include>

<div class="container">
<div class="row">
<div class="col-md-12">
<h1 class="my-4">Software Engineering</h1>

<!-- System Design 2 BEGIN -->
<div class="card mb-4" id="system-design">
  <div class="card-body">
    <h2 class="card-title">System Design 2</h2>
    <ul class="list-unstyled mb-0">
      <li><a href="#system-design-2-1">Proximity Service</a></li>
      <li><a href="#system-design-2-2">Nearby Friends</a></li>
      <li><a href="#system-design-2-3">Google Maps</a></li>
      <li><a href="#system-design-2-4">Distributed Message Queue</a></li>
      <li><a href="#system-design-2-5">Metrics Monitoring and Alerting System</a></li>
      <li><a href="#system-design-2-6">Ad Click Event Aggregation</a></li>
      <li><a href="#system-design-2-7"></a></li>
      <li><a href="#system-design-2-8"></a></li>
      <li><a href="#system-design-2-9"></a></li>
      <li><a href="#system-design-2-10"></a></li>
      <li><a href="#system-design-2-11"></a></li>
      <li><a href="#system-design-2-12"></a></li>
    </ul>
  </div>
</div>

<div class="card mb-4" id="system-design-2-1">
  <div class="card-body">
    <h2 class="card-title">Proximity Service</h2>

    <h3 class="card-title">Understand the Problem and Establish Design Scope</h3>
    <ul>
      <li>What is the radius of search? User can specify the radius and We can only care about businesses within that radius</li>
      <li>What is the maxium radius allowed? 20km</li>
      <li>Can user change the radius on UI? Yes, we should support these options (0.5km, 1km, 2km, 5km, 20km)</li>
      <li>How is business information get added or updated? Business owners can add or udpate business info</li>
      <li>Should the system update or refresh search result as users are moving and the location is being updated? No</li>
    </ul>

    <h3 class="card-title">Bandwidth</h3>
    <ul>
      <li>Assume 100M daily active users</li>
      <li>Assume each user makes 5 queries per day</li>
      <li>Incoming - (100M * 5 / 86400) queries per second</li>
    </ul>

    <h3 class="card-title">Propose High-level Diagram and Get Buy-in</h3>
    <ul>
      <img class="img-fluid" class="card-img-top" src="/system-design/image/sd-c/proximity-service-1.png" alt="Card image cap">
      <li>Location based service</li>
      <ul>
        <li>Read heavy service with no write requests</li>
        <li>High QPS</li>
        <li>Stateless, thus can easily scale horizontally</li>
      </ul>
      <li>Business service</li>
      <ul>
        <li>Write QPS is not high</li>
        <li>Viewing businesses (read QPS) can be high during peak hours</li>
      </ul>
      <li>Database cluster</li>
      <ul>
        <li>Write operation is written to primary and propagated to replicas</li>
        <li>Consistency is not a concern since businesses don't need to be udpated real time</li>
      </ul>
      <li>Algorithm</li>
      <ul>
        <li>Two dimensional search</li>
        <ul>
          <li>Draw a circle in pre-defined radius</li>
          <li>Need to scan the whole DB, thus long search time </li>
          <li>Building indices on latitude and longitude does not help because indexing works on one dimension only</li>
        </ul>
        <li>Evenly divided grid</li>
        <ul>
          <li>Divided the world into small grids</li>
          <li>Not efficient because businesses are not evenly distributed</li>
        </ul>
        <li>Geohash</li>
        <ul>
          <li>Divided the world into small grids</li>
          <li>Each grid is represented by alternating latitude bit and longitude bit</li>
          <ul>
            <li>latitude between -90 and 0 is represented by 0</li>
            <li>latitude between 0 and 90 is represented by 1</li>
            <li>longitude between -180 and 0 is represented by 0</li>
            <li>longitude between 0 and 180 is represented by 1</li>
          </ul>
          <li>Example</li>
          <ul>
            <li>1001 10110 01001 11011 11010 (binary) = 9q9hvu (base32)</li>
          </ul>
          <li>Geohash longer than 6 is too small and less than 4 is too big</li>
          <li>Boundary issue</li>
          <ul>
            <li>Geohashes sharing long common prefix is close location-wise. But two close locations may have completely different geohashes</li>
          </ul>
          <li>Not enough businesses</li>
          <ul>
            <li>Remove last digit of geohahs and use new geohash to fetch nearby businesses</li>
          </ul>
        </ul>
        <li>Quadtree</li>
        <ul>
          <li>The root node represents the world map</li>
          <li>Each node gets 4 children and this process repeats until no nodes have more than a certain number of businesses</li>
          <li>Quadtree is stored in memory</li>
          <li>Quadtree does not require much memory and can fit into a single server (But still multiple servers are needed to serve all read requests)</li>
          <li>How to get neayby businesses</li>
          <ul>
            <li>Build Quadtree in memory</li>
            <li>Start from the root until finding the node where the search origin is</li>
            <li>Either return the businesses in that node or add businesses from its neighbors until enough businesses are returned</li>
          </ul>
        </ul>
        <li>Google S2</li>
        <ul>
          <li>Maps a sphere into 1D index based on Hilbert curve</li>
        </ul>
      </ul>
    </ul>

    <h3 class="card-title">API</h3>
    <ul>
      <li>GET /v1/search/nearby - return businesses</li>
      <li>GET /v1/businesses/id - return detailed info about a business</li>
      <li>POST /v1/businesses - add a business</li>
      <li>PUT /v1/businesses/id - update a business</li>
      <li>DELETE /v1/businesses/id - delete a business</li>
    </ul>

    <h3 class="card-title">Storage schema</h3>
    <ul>
      <li>Read is heavy due to seaching businesses and viewing business information</li>
      <li>Write is low</li>
      <li>Choose MySQL</li>
      <li>Business</li>
      <ul>
        <li>business_id (int, pk)</li>
        <li>address</li>
        <li>city</li>
        <li>state</li>
        <li>country</li>
        <li>latitude</li>
        <li>longitude</li>
      </ul>
      <li>Geo index</li>
      <ul>
        <li>Geohash</li>
        <li>business_id</li>
      </ul>
    </ul>

    <h3 class="card-title">Design Deep Dive</h3>
    <ul>
      <li>Scaling DB</li>
      <ul>
        <li>Business table</li>
        <ul>
          <li>Shard based on business ID</li>
        </ul>
        <li>Geospatial index table (Geohash)</li>
        <ul>
          <li>The table is not large, thus have multiple read replicas instead of sharding</li>
        </ul>
      </ul>
      <li>Caching</li>
      <ul>
        <li>Pre-compute list of business IDs for a given geohash and store it in key-value store like Redis</li>
        <li>Cache data on Redis on different geohash precisions (Ex. 4, 5, 6)</li>
      </ul>
      <li>Region and availability zones</li>
      <ul>
        <li>For example, users from US West should be connected to data centers in US West</li>
        <li>High density regions (Ex. Japan, Korea) should be in separate regions</li>
        <li>For some regions, employing DNS routing to limit requests only in those regions is needed to respect privacy laws</li>
      </ul>
      <img class="img-fluid" class="card-img-top" src="/system-design/image/sd-c/proximity-service-2.png" alt="Card image cap">
      <li>Get nearby business</li>
      <ul>
        <li>Client sends latitude, longitude, radius to load balancer, which in turn gets sent to location based service</li>
        <li>Location based service finds geohash length that matches the search</li>
        <li>Location based service calculates neighboring geohashes and adds them to list</li>
        <li>Location based service calls Geohash Redis cache to find business IDs for each geohash</li>
        <li>Location based service pulls businees info from Business info Redis cache based on the business IDs</li>
      </ul>
      <li>View/add/update/delete business</li>
      <ul>
        <li>Business service first checks Business info Redis cache</li>
        <li>If not, Business service fetches data from DB and data is stored in Redis cache</li>
      </ul>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: System Design Interview Volume 2, Alex Xu & Sahn Lam
  </div>
</div>

<div class="card mb-4" id="system-design-2-2">
  <div class="card-body">
    <h2 class="card-title">Nearby Friends</h2>

    <h3 class="card-title">Understand the Problem and Establish Design Scope</h3>
    <ul>
      <li>How close do users need to be in order be considered as nearby? The number should be configurable</li>
      <li>Is distance between users calculated based on the straight line between them? Yes</li>
      <li>How many users are there? 1B users and 100M of them use nearby friends feature</li>
      <li>Should the system store location history? Yes</li>
      <li>Should the system how inactive users? No</li>
      <li>Should the system consider privacy laws? No</li>
    </ul>

    <h3 class="card-title">Bandwidth</h3>
    <ul>
      <li>Assume location refresh takes place every 30 seconds</li>
      <li>Assume 10M out of 100M DAU are concurrent users</li>
      <li>Incoming - (10M / 30) queries per second</li>
    </ul>

    <h3 class="card-title">Propose High-level Diagram and Get Buy-in</h3>
    <ul>
      <li>Design</li>
      <ul>
        <img class="img-fluid" class="card-img-top" src="/system-design/image/sd-c/nearby-friends-1.png" alt="Card image cap">
        <li>API servers</li>
        <ul>
          <li>Add/remove/update users</li>
        </ul>
        <li>WebSocket servers</li>
        <ul>
          <li>Real-time update of friends' location</li>
          <li>Each client maintains one persistent WebSocket connection to one WebSocket server</li>
        </ul>
        <li>Location cache</li>
        <ul>
          <li>Key-value store is used to cache the most recent location for each active user</li>
          <li>When TTL expires, location data is deleted from cache</li>
        </ul>
      </ul>
      <li>User DB</li>
      <ul>
        <li>Stores user and user friendship data</li>
        <li>Relational DB or NoSQL can be used</li>
      </ul>
      <li>Location history DB</li>
      <ul>
        <li>Stores location history data</li>
      </ul>
      <li>Redis Pub/Sub</li>
      <ul>
        <li>Very light message bus</li>
        <li>Channels are very cheap to create</li>
        <li>A user's WebSocket server publishes to that user's channel in Redis Pub/Sub</li>
        <li>Each channel of a user in Redis Pub/Sub is subscribed by WebSocket servers of that user's friends</li>
      </ul>
      <li>Workflow</li>
      <ul>
        <img class="img-fluid" class="card-img-top" src="/system-design/image/sd-c/nearby-friends-2.png" alt="Card image cap">
        <li>User sends location update to load balancer</li>
        <li>Load balancer forwards update to WebSocket server</li>
        <li>WebSocket server updates new location in location cache. The update refreshes TTL</li>
        <li>WebSocket server publishes new location to user's channel in Redis Pub/Sub</li>
        <li>Update is broadcasted to all subscribers who are user's friends</li>
        <li>WebSocket server of friends compute the distance from the user</li>
        <li>If distance is within search radius, new location is sent to friends. Otherwise, update is dropped</li>
      </ul>
      <li>API</li>
      <ul>
        <li>Periodic location update</li>
        <ul>
          <li>Request - latitude, longigude, timestamp</li>
          <li>Response - none</li>
        </ul>
        <li>Client receives location updates</li>
        <li>WebScoket initialization</li>
        <ul>
          <li>Request - latitude, longigude, timestamp</li>
          <li>Response - friends' location data</li>
        </ul>
        <li>Subscribe to a new friend</li>
        <ul>
          <li>Request - friend ID</li>
          <li>Response - friend's latest latitude, longigude, timestamp</li>
        </ul>
        <li>Unsubscribe a friend</li>
        <ul>
          <li>Request - friend ID</li>
          <li>Response - none</li>
        </ul>
      </ul>
      <li>Storage schema</li>
      <ul>
        <li>Location cache</li>
        <ul>
          <li>Redis is good choice due to fast read and write as well as TTL support</li>
          <li>Shard based on user_id and replicate each shard to improve availability</li>
          <li>Key - user_id</li>
          <li>Value - {latitude, longigude, timestamp}</li>
        </ul>
        <li>Location history DB</li>
        <ul>
          <li>Cassandra is good choice due to heavy write workload</li>
          <li>If using relational DB, sharding based on user ID is needed</li>
          <li>user_id</li>
          <li>latitude</li>
          <li>longigude</li>
          <li>timestamp</li>
        </ul>
        <li>User DB</li>
        <ul>
          <li>Relational DB with sharding based on user_id</li>
        </ul>
      </ul>
    </ul>

    <h3 class="card-title">Design Deep Dive</h3>
    <ul>
      <li>Scaling servers</li>
      <ul>
        <li>API servers</li>
        <ul>
          <li>Stateless, thus autoscale based on loads</li>
        </ul>
        <li>WebSocket servers</li>
        <ul>
          <li>Stateful, the load balancers need to gracefully open/close connections when adding/removing servers</li>
        </ul>
      </ul>
      <li>Client initialization</li>
      <ul>
        <li>User sends it's initial location</li>
        <li>Server loads all user's friends from user DB</li>
        <li>Server makes batched requests to cache to fetch locations of all friends</li>
        <li>Server computes distance between user and all friends</li>
        <li>For each friend within the search radius, server subscribes to friend's channel in Redis PubSub</li>
        <li>Server sends user's current location to user's channel in Redis PubSub</li>
      </ul>
      <li>Redis PubSub</li>
      <ul>
        <li>Design</li>
        <ul>
          <li>Uses a hash table and linked list to track the subscribers</li>
          <li>A user subscribes to each friend's channel whether the friend is online or not</li>
        </ul>
        <li>Memory</li>
        <ul>
          <li>Assume a channel is allocated for each user who uses nearby friend</li>
          <li>Then, we need 100M channels</li>
          <li>Assume each user have 100 active friends</li>
          <li>Assume 20 bytes is need to track each subscriber</li>
          <li>100M * 20 bytes * 100 = 200GB</li>
          <li>For a server with 100GB memory, 2 PubSub servers can hold all channels</li>
        </ul>
        <li>CPU</li>
        <ul>
          <li>Assume Redis PubSub sends 14 million updates per second to subscribers</li>
          <li>Assume each server can handle 100k pushes per seconds</li>
          <li>Then, 14M / 100K = 140 Redis PubSub servers are needed</li>
        </ul>
        <li>Distributed Redis PubSub</li>
        <ul>
          <li>Similar to consistent hashing, place Redis PubSub servers in a hash ring</li>
          <li>Compute hash of channels to determine which PubSub servers that those channels should go to</li>
          <li>The hash ring can be stored in service discovery like ZooKeeper and cached in WebSocket servers</li>
        </ul>
      </ul>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: System Design Interview Volume 2, Alex Xu & Sahn Lam
  </div>
</div>

<div class="card mb-4" id="system-design-2-3">
  <div class="card-body">
    <h2 class="card-title">Google Maps</h2>

    <h3 class="card-title">Understand the Problem and Establish Design Scope</h3>
    <ul>
      <li>How many total users in the system? 1B</li>
      <li>How many daily users in the system? 100M</li>
      <li>What is the main function of the system? Location update, navigation, ETA, map rendering</li>
      <li>What is the size of road data? terrabytes</li>
      <li>Should the system consider traffic conditions? Yes</li>
      <li>Should the system support different travel mode like driving, walking, public transit? Yes</li>
      <li>Should the system support multi-stop directions? No</li>
      <li>Should the system support business photos and videos? No</li>
      <li>Should the system show the route as user moves? Yes</li>
      <li>Should the system show new route when user deviates from the path? Yes</li>
    </ul>

    <h3 class="card-title">Storage</h3>
    <ul>
      <li>Map tiling</li>
      <ul>
        <li>Geo coding</li>
        <ul>
          <li>Convert each address to a pair of latitude and longitude</li>
        </ul>
        <li>Geo hashing</li>
        <ul>
          <li>Encode geographical area with digits</li>
          <li>Each geographical area is considered as a tile</li>
        </ul>
      </ul>
      <li>Map rendering</li>
      <ul>
        <li>Clients only download relevant tiles for the area</li>
      </ul>
      <li>Metadata - negligible</li>
      <li>Road info - assume terrabytes</li>
      <li>World map</li>
      <ul>
        <li>At zoom level 21, there are 2^21 = 4.3 trillion tiles</li>
        <li>Assume each 256 by 265 tile image is 100KB</li>
        <li>Then, higest zoom level would have 430PB</li>
        <li>Assume 90% of world surfaces are ocean, desert, mountains, etc</li>
        <li>Then, 430PB * 0.1 = 43PB</li>
        <li>The subsequent zoom level would have \( 43 + \dfrac{43}{4} + \dfrac{43}{16} + \dfrac{43}{64} + \dots \) = ? PB</li>
      </ul>
    </ul>

    <h3 class="card-title">Bandwidth</h3>
    <ul>
      <li>Assume</li>
      <ul>
        <li>Assume each user uses navigation 1 hour per day</li>
        <li>Then, 100M * 1hr = 100M hours per day = 360B seconds per day</li>
        <li>Assume each GPS update is sent every 10 seconds on average</li>
        <li>Then, 360B / 10 = 36B GPS updates per day</li>
        <li>Assume both request and response is 100 bytes</li>
      </ul>
      <li>Incoming/Outgoing - 36B * 100 bytes * 8 / 86400 = ? bps</li>
    </ul>

    <h3 class="card-title">Propose High-level Diagram and Get Buy-in</h3>
    <ul>
      <img class="img-fluid" class="card-img-top" src="/system-design/image/sd-c/google-maps-1.png" alt="Card image cap">
      <li>Location service</li>
      <ul>
        <li>Client sends its location every second</li>
        <li>Location updates can be buffered on the client and send to the server as a batch every 10-15 seconds</li>
      </ul>
      <li>Navigation service</li>
      <ul>
        <li>Find route between two points</li>
        <li>Runs shortest path algorithm</li>
        <li>Shows the path on the map for the user</li>
        <li>Sends updated directions when user deviates from the path</li>
      </ul>
      <li>Map rendering</li>
      <ul>
        <li>User fetches new map tile from the server when</li>
        <ul>
          <li>User is zooming on the map</li>
          <li>During navigation when user moves out from current tile and moves to nearby tile</li>
        </ul>
        <li>Map tiles are pre-computed at various zoom level</li>
        <ul>
          <li>Each map tile is represented by its geohash</li>
        </ul>
        <li>CDN serves as a cache of map tiles</li>
        <li>Map tile service takes user location and zoom level as inputs and return 9 URLs of tiles</li>
        <ul>
          <li>Return value includes the tile to render and 8 surrounding tiles</li>
        </ul>
        <li>User downloads the tiles from CDN</li>
      </ul>
      <li>API</li>
      <ul>
        <li><code>curr_location(location)</code></li>
        <ul>
          <li>Display current location of user on the map</li>
          <li><code>location</code></li>
          <ul>
            <li>Boolean to indicate if user location is turned on or off</li>
            <li>If off, the service will ask user to turn on the location</li>
          </ul>
        </ul>
        <li><code>find_route(start, end, transport_type)</code></li>
        <ul>
          <li>Computes optimal route between two points</li>
          <li><code>start</code></li>
          <ul>
            <li>Start point in text format</li>
          </ul>
          <li><code>end</code></li>
          <ul>
            <li>Start point in text format</li>
          </ul>
        </ul>
        <li><code>directions(curr_location, steps)</code></li>
        <ul>
          <li>Provides alerts of when and where to make turns</li>
        </ul>
      </ul>
      <li>Database</li>
      <ul>
        <li>Road info</li>
        <ul>
          <li>Graph data is represented as adjacency list</li>
          <li>Serialize adjacency lists into binary files</li>
          <li>Use S3 to store data and cache it on the servers</li>
        </ul>
        <li>User location data</li>
        <ul>
          <li>Use Cassandra for write-heavy workload and availability over consistency</li>
          <ul>Schema</ul>
          <ul>
            <li>Key - (user_id, timestamp)</li>
            <li>Value - (latitude, longitude)</li>
          </ul>
        </ul>
        <li>Geocoding DB</li>
        <ul>
          <li>Use Redis for read-heavy workload</li>
          <ul>Schema</ul>
          <ul>
            <li>place_id</li>
            <li>location</li>
          </ul>
        </ul>
        <li>World map</li>
        <ul>
          <li>Use S3 and cache it on CDN</li>
        </ul>
      </ul>
    </ul>

    <h3 class="card-title">Design Deep Dive</h3>
    <ul>
      <li>Location service</li>
      <ul>
        <li>Other services consume location data from Kafka</li>
      </ul>
      <li>Map rendering</li>
      <ul>
        <li>WebGL - instead of sending images over network, send vectors (paths and polygons)</li>
        <li>Compressed vector data is much smaller than compressed images</li>
        <li>Zooming experience is much better with vectors</li>
      </ul>
      <li>Navigation service</li>
      <ul>
        <li>Geocoding service</li>
        <ul>
          <li>Converts location name to lat/long pair</li>
        </ul>
        <li>Routing planner service</li>
        <ul>
          <li>Computes suggested routes using other services</li>
        </ul>
        <li>Shortest path service</li>
        <ul>
          <li>Returns top k shortest paths without considering traffic or current conditions</li>
        </ul>
        <li>ETA service</li>
        <ul>
          <li>Computes ETA for each route using ML from current traffic and historical data</li>
        </ul>
        <li>Ranker</li>
        <ul>
          <li>Applies filters defined by user like avoid tolls or highways</li>
        </ul>
        <li>Traffic update service</li>
        <ul>
          <li>Updates traffic DB so that ETA service can compute accurate estimates</li>
        </ul>
      </ul>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: System Design Interview Volume 2, Alex Xu & Sahn Lam
  </div>
</div>

<div class="card mb-4" id="system-design-2-4">
  <div class="card-body">
    <h2 class="card-title">Distributed Message Queue</h2>

    <h3 class="card-title">Understand the Problem and Establish Design Scope</h3>
    <ul>
      <li>What is format of messages? Text messages only</li>
      <li>What is size of messages? KBs</li>
      <li>Should messages be repeatedly consumed? Yes by different consumers</li>
      <li>Should messages be consumed in the same order that they arrived? Yes</li>
      <li>Should data be persisted? Yes for two weeks</li>
      <li>How many producers and consumers should the system support? As many as it can</li>
      <li>Should we support at most once, at least once, or exactly once? All of them</li>
      <li>Should the system have both high throughput and low latency? Yes</li>
    </ul>

    <h3 class="card-title">Propose High-level Diagram and Get Buy-in<</h3>
    <ul>
      <img class="img-fluid" class="card-img-top" src="/system-design/image/sd-c/distributed-message-queue-1.png" alt="Card image cap">
      <li>Producer</li>
      <ul>
        <li>Topic organizes messages</li>
        <li>Sends messages to a partition of a topic</li>
      </ul>
      <li>Consumer groups</li>
      <ul>
        <li>Each group can subscribe one or more topics</li>
      </ul>
      <li>Brokers</li>
      <ul>
        <li>Holds partitions of topics</li>
        <li>Data storage - persists messages in a form of partitions</li>
      </ul>
      <li>Zookeeper</li>
      <ul>
        <li>Metadata stroage - persists configuration of topics</li>
        <li>State storage - manages consumer states</li>
        <li>Coordination service</li>
        <ul>
          <li>Service discovery - checks which brokers are alive</li>
          <li>Leader election - only one broker serves as active controller at a time, which assigns partitions in the cluster</li>
        </ul>
      </ul>
    </ul>

    <h3 class="card-title">Design Deep Dive</h3>
    <ul>
      <li>Data storage</li>
      <ul>
        <li>Traffic pattern of message queue</li>
        <ul>
          <li>Both read-heavy and write-heavy</li>
          <li>No update or delete operation</li>
          <li>Mostly sequential read/write access</li>
        </ul>
        <li>Database is hard to be suitable for both read-heavy and write-heavy access pattern</li>
        <li>Write-ahead log (WAL) has pure sequential read/write access pattern</li>
        <ul>
          <li>Messages can be stored as WAL on disk</li>
          <li>Disk is slow for random access but it can be fast for sequential access</li>
          <li>New messages are appended to the tail of a partition</li>
          <li>Partition is divided into segments where active segments serve writes and inactive segments serve reads</li>
          <li>Old inactive segments can be truncated</li>
        </ul>
      </ul>
      <li>Message data structure</li>
      <ul>
        <li>key - used to determine the partition of message by hash(key) % number_of_partitions</li>
        <li>value - payload of message</li>
        <li>topic - topic that message belongs to</li>
        <li>partition - partition id that message belongs to</li>
        <li>offset - position of message in partition</li>
        <li>timestamp - timestamp of when message was stored</li>
        <li>size - size of message</li>
        <li>crc - cyclic redundancy check used to ensure data integrity</li>
      </ul>
      <li>Batching</li>
      <ul>
        <li>Message queue</li>
        <ul>
          <li>There is trade-off between throughput and latency</li>
          <li>Smaller batch size (thus, small throughput) leads to low latency and vice-versa</li>
        </ul>
        <li>Producer</li>
        <ul>
          <img class="img-fluid" class="card-img-top" src="/system-design/image/sd-c/distributed-message-queue-2.png" alt="Card image cap">
          <li>Batching buffers messages in memory and send out larger batches in a single request, which increases throughput</li>
          <li>Routing decides which replica/partition should the message go to</li>
          <li>Leader replica receives the message and follower replicas pull data from leader</li>
        </ul>
        <li>Consumer</li>
        <ul>
          <li>Brokers push data to consumers</li>
          <ul>
            <li>Pros</li>
            <ul>
              <li>Messages are propagated immediately, thus lower latency</li>
            </ul>
            <li>Cons</li>
            <ul>
              <li>Consumers could be overwhelmed</li>
            </ul>
          </ul>
          <li>Consumers pull data from brokers</li>
          <ul>
            <li>Pros</li>
            <ul>
              <li>Consumers can control consumption rate, which is suitable for batch processing</li>
            </ul>
            <li>Cons</li>
            <ul>
              <li>Requires long polling, that waits some time for new messages</li>
            </ul>
            <li>Workflow</li>
            <ul>
              <img class="img-fluid" class="card-img-top" src="/system-design/image/sd-c/distributed-message-queue-3.png" alt="Card image cap">
              <li>A new consumer wants to join group 1 and subscribes to topic A</li>
              <li>All consumers in the same group connect to the same broker</li>
              <li>Coordinator assigns partition 2 to consumer</li>
              <li>Consumer fetches message from the last consumed offset, which is managed by state storage</li>
              <li>Consumer processes messages and commits the offset to the broker</li>
            </ul>
          </ul>
          <li>Consumer rebalancing</li>
          <ul>
            <li>Decides which consumer is responsible for which subset of partitions</li>
            <li>Needed when consumer joins, consumer leaves, consumer crashes, paritions are adjusted</li>
            <img class="img-fluid" class="card-img-top" src="/system-design/image/sd-c/distributed-message-queue-4.png" alt="Card image cap">
            <li>Each consumer belongs to a group. Each consumer finds the coordinator by hashing the group name. All consumers from the same group are connected to the same coordinator</li>
            <li>Coordinator elects new leader for the group</li>
            <li>Leader generates new partition dispatch plan and reports it back to coordinator. Coordinator broadcasts the plan to other consumers in the group</li>
          </ul>
        </ul>
      </ul>
      <li>Replication</li>
      <ul>
        <li>For example, each partition having 3 replica means that 1 leader and 2 followers are distributed in different brokers</li>
        <li>Messages are only written to the leader, and followers sync messages from leader</li>
        <li>In-sync replica - replicas that are in-sync with the leader</li>
        <li>ACK=all</li>
        <ul>
          <li>Producers get ACK when all in-sync replicas received the message</li>
          <li>High latency but strong message durability</li>
        </ul>
        <li>ACK=1</li>
        <ul>
          <li>Producers get ACK when leader received the message</li>
          <li>Low latency but occasional data loss</li>
        </ul>
        <li>ACK=0</li>
        <ul>
          <li>Producers send messages to leader without waiting for any ACK</li>
          <li>Suitable for collecting metrics or logging data</li>
        </ul>
      </ul>
      <li>Scalability</li>
      <ul>
        <li>Each partition of each topic will have specific number of replicas that are spread across different brokers</li>
        <li>When a broker is added or removed, replicas will be re-distributed</li>
        <li>When a partition is added to a topic, messages will also be persisted in the newly added partition</li>
        <li>When a partition is removed from a topic, producers only send messages to remaining partitions but consumers still consume from removed partition during certain retention period</li>
      </ul>
      <li>Data delivery semantics</li>
      <ul>
        <li>At most once</li>
        <ul>
          <li>ACK=0</li>
          <li>Consumer fetches the message and commits the offset before data is processed</li>
          <li>If consumer crashes just after offset commit, message will not be re-consumed</li>
        </ul>
        <li>At least once</li>
        <ul>
          <li>ACK=1 or ACK=all</li>
          <li>Consumer fetches the message and commits the offset only after data is processed</li>
          <li>If consumer fails to process the message, it will re-consume the message (There is no data loss)</li>
          <li>If consumer processes the message but fails to commit offset to broker, it will re-consume the message (There will be duplicates)</li>
        </ul>
        <li>Exactly once</li>
        <ul>
          <li>High cost for system performance and complexity</li>
        </ul>
      </ul>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: System Design Interview Volume 2, Alex Xu & Sahn Lam
  </div>
</div>

<div class="card mb-4" id="system-design-2-5">
  <div class="card-body">
    <h2 class="card-title">Metrics Monitoring and Alerting System</h2>

    <h3 class="card-title">Understand the Problem and Establish Design Scope</h3>
    <ul>
      <li>What metrics should the system collect? CPU load, memory usage, disk space comsumption, request per second of a service, running server count of web pool</li>
      <li>How many servers are there? 1000 server pools, 100 machines per pool. Also 100M daily active users</li>
      <li>How long should the system keep the data? 1 year</li>
      <li>How should the system alert metrics? Email, webhooks</li>
      <li>Should the system collect logs? No</li>
      <li>Should the system support distributed system tracing? No</li>
    </ul>

    <h3 class="card-title">Propose High-level Diagram and Get Buy-in</h3>
    <ul>
      <img class="img-fluid" class="card-img-top" src="/system-design/image/sd-c/metrics-monitoring-and-alerting-system-1.png" alt="Card image cap">
      <li>Metrics source</li>
      <ul>
        <li>Ex. application servers, SQL databases, message queues, etc</li>
      </ul>
      <li>Metrics collector</li>
      <ul>
        <li>Gathers metrics data and write them into time-series DB</li>
      </ul>
      <li>Time-series DB</li>
      <ul>
        <li>Store metrics data as time series</li>
        <li>Indexes lables to allow fast lookup based on labels</li>
      </ul>
      <li>Alerting system</li>
      <ul>
        <li>Send notifications to various destinations</li>
      </ul>
      <li>Visualization system</li>
      <ul>
        <li>Shows metrics in graphs/charts</li>
      </ul>
      <li>Data storage</li>
      <ul>
        <li>Need time-series DB</li>
        <ul>
          <li>Ex. MetricsDB, Timestream, InfluxDB, Prometheus</li>
        </ul>
      </ul>
      <li>Storage schema</li>
      <ul>
        <li>Constant heavy write load and spiky read load</li>
        <li>A metric name - string</li>
        <li>A set of tags/labels - list of &lt;key:value&gt; pairs</li>
        <li>An array of values and their timestamps - an array of &lt;value:timestamp&gt; pairs</li>
      </ul>
    </ul>

    <h3 class="card-title">Design Deep Dive</h3>
    <ul>
      <li>Metrics collections</li>
      <ul>
        <li>Pull</li>
        <ul>
          <img class="img-fluid" class="card-img-top" src="/system-design/image/sd-c/metrics-monitoring-and-alerting-system-2.png" alt="Card image cap">
          <li>Metrics collector fetches configuration metadata of service endpoints (such as pulling interval, IP addresses, timeout, etc) from service discovery</li>
          <li>Metrics collector pulls metrics data via pre-defined HTTP endpoints</li>
          <li>Distribute metrics source to a scalable set of metrics collectors via consistent hashing</li>
          <li>Ex. Prometheus</li>
          <li>Pros</li>
          <ul>
            <li>Easy debugging - HTTPS endpoint can be used to view metrics at any time</li>
            <li>Health check - if application server doesn't respond, it's down</li>
          </ul>
        </ul>
        <li>Push</li>
        <ul>
          <img class="img-fluid" class="card-img-top" src="/system-design/image/ref-sd-2/metrics-monitoring-and-alerting-system-3.png" alt="Card image cap">
          <li>Collection agent is installed on every server being monitored</li>
          <li>Collection agent periodically pushes metrics to metrics collectors</li>
          <li>Ex. Amazon CloudWatch, Graphite</li>
          <li>Pros</li>
          <ul>
            <li>Networking - load balancer allows receiving data from anywhere</li>
          </ul>
        </ul>
      </ul>
      <li>Scaling metrics transmission pipeline</li>
      <ul>
        <li>Place Kafka between metrics collector and time-series DB</li>
        <li>Decouples data collection and data processing</li>
        <li>Prevents data loss when DB is unavailable by retaining data in Kafka</li>
      </ul>
      <li>Query service</li>
      <ul>
        <li>Add a cache layer to query service</li>
        <li>Prometheus and InfluxDB have their own query language</li>
      </ul>
      <li>Storage layer</li>
      <ul>
        <li>Data encoding</li>
        <ul>
          <li>Instead of storing full timestamp, store only the delta</li>
        </ul>
        <li>Downsampling</li>
        <ul>
          <li>Up to 7 days old data - no downsampling</li>
          <li>Between 7 days and 30 days - downsample to 1 minute resolution</li>
          <li>Between 30 days and 1 year - downsample to 1 hour resolution</li>
        </ul>
      </ul>
      <img class="img-fluid" class="card-img-top" src="/system-design/image/sd-c/metrics-monitoring-and-alerting-system-4.png" alt="Card image cap">
      <li>Alerting system</li>
      <ul>
        <li>Load config file (which has alert rules) to cache server</li>
        <li>Alert manager fetches alert configs from the cache</li>
        <li>Alert manager calls query service at pre-defined interval</li>
        <li>Alert store is a key-value DB (Ex. Cassandra) that keeps state of all alerts</li>
        <li>Eligible alerts are inserted into Kafka</li>
        <li>Alert consumers pull alert events from Kafka</li>
      </ul>
      <li>Visualization system</li>
      <ul>
        <li>A high-quality visualization system is hard to build</li>
        <li>Use product like Grafana</li>
      </ul>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: System Design Interview Volume 2, Alex Xu & Sahn Lam
  </div>
</div>

<div class="card mb-4" id="system-design-2-6">
  <div class="card-body">
    <h2 class="card-title">Ad Click Event Aggregation</h2>

    <h3 class="card-title">Understand the Problem and Establish Design Scope</h3>
    <ul>
      <li>How does input data look like? Log files where latest click events are appended to them</li>
      <li>How does event look like in terms of data attribute? ad_id, click_timestamp, user_id, ip, country</li>
      <li>How large is dataset? 1B clicks per day and 2M Ads where number of Ads grows 30% per year</li>
      <li>What kind of queries should the system support?</li>
      <ul>
        <li>Return the number of clicks for a particular Ad in last M minites</li>
        <li>Return the top 100 most clicked Ads in past 1 minute</li>
        <li>Support filtering by user_id, ip, country for above two queries</li>
      </ul>
      <li>What kind of edge cases are there?</li>
      <ul>
        <li>Events that arrive later than expected</li>
        <li>Duplicate events</li>
      </ul>
      <li>Should the system have low latency? Less than 1s for real-time bidding, few minutes for ad click event aggregation</li>
    </ul>

    <h3 class="card-title">Storage</h3>
    <ul>
      <li>Assume single click requires 0.1KB storage</li>
      <li>0.1KB * 1B = 100GB per day = 3TB per month</li>
    </ul>

    <h3 class="card-title">Bandwidth</h3>
    <ul>
      <li>Incoming - (1B / 24 * 86400) clicks per second on average</li>
      <li>Assuming peak is 5 times the average number, then 5 * (1B / 24 * 86400) clicks per second during peak</li>
    </ul>

    <h3 class="card-title">Propose High-level Diagram and Get Buy-in</h3>
    <ul>
      <img class="img-fluid" class="card-img-top" src="/system-design/image/sd-c/ad-click-event-aggregation-1.png" alt="Card image cap">
      <li>Message queues</li>
      <ul>
        <li>First message queue</li>
        <ul>
          <li>ad_id, click_timestamp, user_id, ip, country</li>
        </ul>
        <li>Second message queue</li>
        <ul>
          <li>ad_id, click_minute, count</li>
          <li>update_time_minute, most_clicked_ads</li>
        </ul>
        <li>Delivery guarantee</li>
        <ul>
          <li>At-least once is not good enough because difference of few percent data can result in huge discrepancies</li>
          <li>Exactly-once is needed</li>
        </ul>
      </ul>
      <li>Data aggregation service</li>
      <ul>
        <li>MapReduce</li>
        <ul>
          <li>Map node</li>
          <ul>
            <li>Read data from data source and transform it (clean and normalize data)</li>
          </ul>
          <li>Aggregate node</li>
          <ul>
            <li>Counts ad click events by ad_id every minute</li>
          </ul>
          <li>Reduce node</li>
          <ul>
            <li>Combines aggregated results from all Aggregate nodes to produce the final result</li>
          </ul>
        </ul>
      </ul>
      <li>API</li>
      <ul>
        <li>Return aggregated event count for a given <code>ad_id</code></li>
        <ul>
          <li><code>GET /v1/ads/{:ad_id}/aggregated_count</code></li>
          <li>Request</li>
          <ul>
            <li><code>from</code> - start minute</li>
            <li><code>to</code> - end minute</li>
            <li><code>filter</code> - identifier for different filtering strategies</li>
          </ul>
          <li>Response</li>
          <ul>
            <li><code>ad_id</code></li>
            <li><code>count</code> - aggregated count between start and end minutes</li>
          </ul>
        </ul>
        <li>Return top N most clicked ads in last M minutes</li>
        <ul>
          <li><code>GET /v1/ads/popular_ads</code></li>
          <li>Request</li>
          <ul>
            <li><code>count</code> - specifies N</li>
            <li><code>window</code> - specifies M</li>
            <li><code>filter</code> - identifier for different filtering strategies</li>
          </ul>
          <li>Response</li>
          <ul>
            <li><code>ad_ids</code></li>
          </ul>
        </ul>
      </ul>
      <li>Storage schema</li>
      <ul>
        <li>Raw data serves as backup data in case aggregated data is corrupted</li>
        <li>Aggregated data makes queries fast</li>
        <li>Raw data</li>
        <ul>
          <li>Little read and heavy write</li>
          <li>Use Cassandra or InfluxDB because these are good for writes and time-range queries</li>
          <ul>
            <li>ad_id</li>
            <li>click_timestamp</li>
            <li>user_id</li>
            <li>ip</li>
            <li>country</li>
          </ul>
        </ul>
        <li>Aggregated data</li>
        <ul>
          <li>Heavy read and write</li>
          <li>Use Cassandra or InfluxDB here as well</li>
        </ul>
        <ul>
          <li>Aggregated data with filters</li>
          <ul>
            <li>ad_id</li>
            <li>click_minute</li>
            <li>filter_id</li>
            <li>count</li>
          </ul>
          <li>Filter table</li>
          <ul>
            <li>filter_id</li>
            <li>region</li>
            <li>ip</li>
            <li>user_id</li>
          </ul>
          <li>Most clicked_ads</li>
          <ul>
            <li>window_size - integer</li>
            <li>update_time_minute - timestamp</li>
            <li>most_clicked_ads - array</li>
          </ul>
        </ul>
      </ul>
    </ul>

    <h3 class="card-title">Design Deep Dive</h3>
    <ul>
      <img class="img-fluid" class="card-img-top" src="/system-design/image/sd-c/ad-click-event-aggregation-2.png" alt="Card image cap">
      <li>Streaming vs batching</li>
      <ul>
        <li>Use streaming to process data as it arrives and generate aggregated result in real-time</li>
        <li>Use batch for historical data backup</li>
        <li>Lambda</li>
        <ul>
          <li>Contains batch and streaming simultaneously</li>
          <li>Need to maintain two codebases</li>
        </ul>
        <li>Kappa</li>
        <ul>
          <li>Combines batch and streaming in one processing path</li>
          <li>Handle both real-time data processing and continuous data reprocessing using a single stream processing engine</li>
        </ul>
        <li>Data recalculation</li>
        <ul>
          <li>Recalculation service retrieves data from raw data storage, which is done by batch job</li>
          <li>Data is sent to a dedicated aggregation service (Real-time processing is not impacted)</li>
          <li>Aggregated results are sent to second message queue, then updated in aggregation DB</li>
        </ul>
        <li>Time</li>
        <ul>
          <li>Event time</li>
          <ul>
            <li>When an Ad click happens</li>
            <li>Pros - aggregation results are more accurate</li>
            <li>Cons - depends on timestamp generated on client side</li>
            <li>Watermark</li>
            <ul>
              <li>Extend aggregation window</li>
              <li>Long watermark increases accuracy but adds more latency to the system</li>
              <li>Alternative is to use reconcilation</li>
              <ul>
                <li>Sort Ad click events by event time in every partition at the end of each day (using a batch job and reconciling with real-time aggregation result)</li>
                <li>Result from batch job might not match with real-time aggregation result, since some event might arrive late</li>
              </ul>
            </ul>
          </ul>
          <li>Processing time</li>
          <ul>
            <li>System time of aggregation server that processes the click event</li>
            <li>Pros - server timestamps are more reliable</li>
            <li>Cons - timestamp is not accurate if event reaches the system at much later time</li>
          </ul>
        </ul>
      </ul>
      <li>Aggregation window</li>
      <ul>
        <li>Tumbling window</li>
        <ul>
          <li>Time is partitioned into same-length and non-overlapping chunks</li>
          <li>Good for aggregating ad click events every minute</li>
        </ul>
        <li>Sliding window</li>
        <ul>
          <li>Events are grouped within a window that slides across the data stream</li>
          <li>Good for getting top N most clicked Ads during last M minutes</li>
        </ul>
      </ul>
      <li>Scaling</li>
      <ul>
        <li>Message queues</li>
        <ul>
          <li>Producers</li>
          <ul>
            <li>There is no limit on number of producers</li>
          </ul>
          <li>Consumers</li>
          <ul>
            <li>Rebalancing</li>
          </ul>
          <li>Brokers</li>
          <ul>
            <li>Use ad_id as hashing key for Kafka partition</li>
            <li>Then, aggregation service can subscribe to all events of an ad_id from one single partition</li>
            <li>Pre-allocate enough partitions to avoid increasing the number of partitions</li>
          </ul>
        </ul>
        <li>Data aggregation service</li>
        <ul>
          <li>Option 1. allocate events with different ad_ids to different thread</li>
          <li>Option 2. Deploy aggregation service nodes on resource providers like Apache Hadoop YARN</li>
          <li>Handling hotspot</li>
          <ul>
            <li>Assume each node can handle only 100 events</li>
            <li>Assume there are 300 events in a node</li>
            <li>Node applies for extra resources to resource manager</li>
            <li>Resource manager allocates more resources</li>
            <li>Node split event into 3 groups and each node handles 100 events</li>
            <li>Result is written back to original node</li>
          </ul>
        </ul>
        <li>DB</li>
        <ul>
          <li>Cassandra natively supports horizontal scaling</li>
        </ul>
      </ul>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: System Design Interview Volume 2, Alex Xu & Sahn Lam
  </div>
</div>

<div class="card mb-4" id="system-design-2-7">
  <div class="card-body">
    <h2 class="card-title"></h2>
  </div>
  <div class="card-footer text-muted">
    Reference: System Design Interview Volume 2, Alex Xu & Sahn Lam
  </div>
</div>

<div class="card mb-4" id="system-design-2-8">
  <div class="card-body">
    <h2 class="card-title"></h2>
  </div>
  <div class="card-footer text-muted">
    Reference: System Design Interview Volume 2, Alex Xu & Sahn Lam
  </div>
</div>

<div class="card mb-4" id="system-design-2-9">
  <div class="card-body">
    <h2 class="card-title"></h2>
  </div>
  <div class="card-footer text-muted">
    Reference: System Design Interview Volume 2, Alex Xu & Sahn Lam
  </div>
</div>

<div class="card mb-4" id="system-design-2-10">
  <div class="card-body">
    <h2 class="card-title"></h2>
  </div>
  <div class="card-footer text-muted">
    Reference: System Design Interview Volume 2, Alex Xu & Sahn Lam
  </div>
</div>

<div class="card mb-4" id="system-design-2-11">
  <div class="card-body">
    <h2 class="card-title"></h2>
  </div>
  <div class="card-footer text-muted">
    Reference: System Design Interview Volume 2, Alex Xu & Sahn Lam
  </div>
</div>

<div class="card mb-4" id="system-design-2-12">
  <div class="card-body">
    <h2 class="card-title"></h2>
  </div>
  <div class="card-footer text-muted">
    Reference: System Design Interview Volume 2, Alex Xu & Sahn Lam
  </div>
</div>
<!-- System Design 2 END -->

</div> <!-- /.col-md-12 -->
</div> <!-- /.row -->
</div> <!-- /.container -->

<include src="/footer.html"></include>

</body>

</html>