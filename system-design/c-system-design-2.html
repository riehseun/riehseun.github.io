<!DOCTYPE html>

<html lang="en">

<head>

<!-- Metadata -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="">
<meta name="author" content="Seungmoon Rieh">
<meta name="keywords" content="">

<!-- Title and image -->
<title>Seungmoon Rieh</title>
<link href="/img/seungmoonrieh.jpg" rel="icon">

<!-- CSS -->
<link href="/css/bootstrap.min.css" rel="stylesheet">
<link href="/css/monokai-sublime.css" rel="stylesheet">
<link href="/css/site.css" rel="stylesheet">

<!-- JavaScript -->
<script src="/js/jquery.min.js" type="text/javascript"></script>
<script src="/js/bootstrap.bundle.min.js" type="text/javascript"></script>
<script src="/js/highlight.pack.js" type="text/javascript"></script>
<script src="/js/include_html.js" type="text/javascript"></script>
<script src="/js/mathjax/tex-chtml.js" type="text/javascript"></script>
<script src="/js/site.js" type="text/javascript"></script>

</head>

<body>

<include src="/header.html"></include>

<div class="container">
<div class="row">
<div class="col-md-12">
<h1 class="my-4">Software Engineering</h1>

<!-- System Design 2 BEGIN -->
<div class="card mb-4" id="system-design">
  <div class="card-body">
    <h2 class="card-title">System Design 2</h2>
    <ul class="list-unstyled mb-0">
      <li><a href="#system-design-2-1">Proximity Service</a></li>
      <li><a href="#system-design-2-2">Nearby Friends</a></li>
      <li><a href="#system-design-2-3">Google Maps</a></li>
      <li><a href="#system-design-2-4">Distributed Message Queue</a></li>
      <li><a href="#system-design-2-5">Metrics Monitoring and Alerting System</a></li>
      <li><a href="#system-design-2-6">Ad Click Event Aggregation</a></li>
      <li><a href="#system-design-2-7">Hotel Reservation System</a></li>
      <li><a href="#system-design-2-8">Distributed Email Service</a></li>
      <li><a href="#system-design-2-9">S3-like Object Storage</a></li>
      <li><a href="#system-design-2-10">Real-time Gaming Leaderboard</a></li>
      <li><a href="#system-design-2-11">Payment System</a></li>
      <li><a href="#system-design-2-12">Digital Wallet</a></li>
      <li><a href="#system-design-2-13">Stock Exchange</a></li>
    </ul>
  </div>
</div>

<div class="card mb-4" id="system-design-2-1">
  <div class="card-body">
    <h2 class="card-title">Proximity Service</h2>

    <h3 class="card-title">Understand the Problem and Establish Design Scope</h3>
    <ul>
      <li>What is the radius of search? User can specify the radius and We can only care about businesses within that radius</li>
      <li>What is the maxium radius allowed? 20km</li>
      <li>Can user change the radius on UI? Yes, we should support these options (0.5km, 1km, 2km, 5km, 20km)</li>
      <li>How is business information get added or updated? Business owners can add or udpate business info</li>
      <li>Should the system update or refresh search result as users are moving and the location is being updated? No</li>
    </ul>

    <h3 class="card-title">Bandwidth</h3>
    <ul>
      <li>Assume 100M daily active users</li>
      <li>Assume each user makes 5 queries per day</li>
      <li>Incoming - (100M * 5 / 86400) queries per second</li>
    </ul>

    <h3 class="card-title">Propose High-level Diagram and Get Buy-in</h3>
    <ul>
      <img class="img-fluid" class="card-img-top" src="/system-design/image/sd-c/proximity-service-1.png" alt="Card image cap">
      <li>Location based service</li>
      <ul>
        <li>Read heavy service with no write requests</li>
        <li>High QPS</li>
        <li>Stateless, thus can easily scale horizontally</li>
      </ul>
      <li>Business service</li>
      <ul>
        <li>Write QPS is not high</li>
        <li>Viewing businesses (read QPS) can be high during peak hours</li>
      </ul>
      <li>Database cluster</li>
      <ul>
        <li>Write operation is written to primary and propagated to replicas</li>
        <li>Consistency is not a concern since businesses don't need to be udpated real time</li>
      </ul>
      <li>Algorithm</li>
      <ul>
        <li>Two dimensional search</li>
        <ul>
          <li>Draw a circle in pre-defined radius</li>
          <li>Need to scan the whole DB, thus long search time </li>
          <li>Building indices on latitude and longitude does not help because indexing works on one dimension only</li>
        </ul>
        <li>Evenly divided grid</li>
        <ul>
          <li>Divided the world into small grids</li>
          <li>Not efficient because businesses are not evenly distributed</li>
        </ul>
        <li>Geohash</li>
        <ul>
          <li>Divided the world into small grids</li>
          <li>Each grid is represented by alternating latitude bit and longitude bit</li>
          <ul>
            <li>latitude between -90 and 0 is represented by 0</li>
            <li>latitude between 0 and 90 is represented by 1</li>
            <li>longitude between -180 and 0 is represented by 0</li>
            <li>longitude between 0 and 180 is represented by 1</li>
          </ul>
          <li>Example</li>
          <ul>
            <li>1001 10110 01001 11011 11010 (binary) = 9q9hvu (base32)</li>
          </ul>
          <li>Geohash longer than 6 is too small and less than 4 is too big</li>
          <li>Boundary issue</li>
          <ul>
            <li>Geohashes sharing long common prefix is close location-wise. But two close locations may have completely different geohashes</li>
          </ul>
          <li>Not enough businesses</li>
          <ul>
            <li>Remove last digit of geohahs and use new geohash to fetch nearby businesses</li>
          </ul>
        </ul>
        <li>Quadtree</li>
        <ul>
          <li>The root node represents the world map</li>
          <li>Each node gets 4 children and this process repeats until no nodes have more than a certain number of businesses</li>
          <li>Quadtree is stored in memory</li>
          <li>Quadtree does not require much memory and can fit into a single server (But still multiple servers are needed to serve all read requests)</li>
          <li>How to get neayby businesses</li>
          <ul>
            <li>Build Quadtree in memory</li>
            <li>Start from the root until finding the node where the search origin is</li>
            <li>Either return the businesses in that node or add businesses from its neighbors until enough businesses are returned</li>
          </ul>
        </ul>
        <li>Google S2</li>
        <ul>
          <li>Maps a sphere into 1D index based on Hilbert curve</li>
        </ul>
      </ul>
    </ul>

    <h3 class="card-title">API</h3>
    <ul>
      <li>GET /v1/search/nearby - return businesses</li>
      <li>GET /v1/businesses/id - return detailed info about a business</li>
      <li>POST /v1/businesses - add a business</li>
      <li>PUT /v1/businesses/id - update a business</li>
      <li>DELETE /v1/businesses/id - delete a business</li>
    </ul>

    <h3 class="card-title">Storage schema</h3>
    <ul>
      <li>Read is heavy due to seaching businesses and viewing business information</li>
      <li>Write is low</li>
      <li>Choose MySQL</li>
      <li>Business</li>
      <ul>
        <li>business_id (int, pk)</li>
        <li>address</li>
        <li>city</li>
        <li>state</li>
        <li>country</li>
        <li>latitude</li>
        <li>longitude</li>
      </ul>
      <li>Geo index</li>
      <ul>
        <li>Geohash</li>
        <li>business_id</li>
      </ul>
    </ul>

    <h3 class="card-title">Design Deep Dive</h3>
    <ul>
      <li>Scaling DB</li>
      <ul>
        <li>Business table</li>
        <ul>
          <li>Shard based on business ID</li>
        </ul>
        <li>Geospatial index table (Geohash)</li>
        <ul>
          <li>The table is not large, thus have multiple read replicas instead of sharding</li>
        </ul>
      </ul>
      <li>Caching</li>
      <ul>
        <li>Pre-compute list of business IDs for a given geohash and store it in key-value store like Redis</li>
        <li>Cache data on Redis on different geohash precisions (Ex. 4, 5, 6)</li>
      </ul>
      <li>Region and availability zones</li>
      <ul>
        <li>For example, users from US West should be connected to data centers in US West</li>
        <li>High density regions (Ex. Japan, Korea) should be in separate regions</li>
        <li>For some regions, employing DNS routing to limit requests only in those regions is needed to respect privacy laws</li>
      </ul>
      <img class="img-fluid" class="card-img-top" src="/system-design/image/sd-c/proximity-service-2.png" alt="Card image cap">
      <li>Get nearby business</li>
      <ul>
        <li>Client sends latitude, longitude, radius to load balancer, which in turn gets sent to location based service</li>
        <li>Location based service finds geohash length that matches the search</li>
        <li>Location based service calculates neighboring geohashes and adds them to list</li>
        <li>Location based service calls Geohash Redis cache to find business IDs for each geohash</li>
        <li>Location based service pulls businees info from Business info Redis cache based on the business IDs</li>
      </ul>
      <li>View/add/update/delete business</li>
      <ul>
        <li>Business service first checks Business info Redis cache</li>
        <li>If not, Business service fetches data from DB and data is stored in Redis cache</li>
      </ul>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: System Design Interview Volume 2, Alex Xu & Sahn Lam
  </div>
</div>

<div class="card mb-4" id="system-design-2-2">
  <div class="card-body">
    <h2 class="card-title">Nearby Friends</h2>

    <h3 class="card-title">Understand the Problem and Establish Design Scope</h3>
    <ul>
      <li>How close do users need to be in order be considered as nearby? The number should be configurable</li>
      <li>Is distance between users calculated based on the straight line between them? Yes</li>
      <li>How many users are there? 1B users and 100M of them use nearby friends feature</li>
      <li>Should the system store location history? Yes</li>
      <li>Should the system how inactive users? No</li>
      <li>Should the system consider privacy laws? No</li>
    </ul>

    <h3 class="card-title">Bandwidth</h3>
    <ul>
      <li>Assume location refresh takes place every 30 seconds</li>
      <li>Assume 10M out of 100M DAU are concurrent users</li>
      <li>Incoming - (10M / 30) queries per second</li>
    </ul>

    <h3 class="card-title">Propose High-level Diagram and Get Buy-in</h3>
    <ul>
      <li>Design</li>
      <ul>
        <img class="img-fluid" class="card-img-top" src="/system-design/image/sd-c/nearby-friends-1.png" alt="Card image cap">
        <li>API servers</li>
        <ul>
          <li>Add/remove/update users</li>
        </ul>
        <li>WebSocket servers</li>
        <ul>
          <li>Real-time update of friends' location</li>
          <li>Each client maintains one persistent WebSocket connection to one WebSocket server</li>
        </ul>
        <li>Location cache</li>
        <ul>
          <li>Key-value store is used to cache the most recent location for each active user</li>
          <li>When TTL expires, location data is deleted from cache</li>
        </ul>
      </ul>
      <li>User DB</li>
      <ul>
        <li>Stores user and user friendship data</li>
        <li>Relational DB or NoSQL can be used</li>
      </ul>
      <li>Location history DB</li>
      <ul>
        <li>Stores location history data</li>
      </ul>
      <li>Redis Pub/Sub</li>
      <ul>
        <li>Very light message bus</li>
        <li>Channels are very cheap to create</li>
        <li>A user's WebSocket server publishes to that user's channel in Redis Pub/Sub</li>
        <li>Each channel of a user in Redis Pub/Sub is subscribed by WebSocket servers of that user's friends</li>
      </ul>
      <li>Workflow</li>
      <ul>
        <img class="img-fluid" class="card-img-top" src="/system-design/image/sd-c/nearby-friends-2.png" alt="Card image cap">
        <li>User sends location update to load balancer</li>
        <li>Load balancer forwards update to WebSocket server</li>
        <li>WebSocket server updates new location in location cache. The update refreshes TTL</li>
        <li>WebSocket server publishes new location to user's channel in Redis Pub/Sub</li>
        <li>Update is broadcasted to all subscribers who are user's friends</li>
        <li>WebSocket server of friends compute the distance from the user</li>
        <li>If distance is within search radius, new location is sent to friends. Otherwise, update is dropped</li>
      </ul>
      <li>API</li>
      <ul>
        <li>Periodic location update</li>
        <ul>
          <li>Request - latitude, longigude, timestamp</li>
          <li>Response - none</li>
        </ul>
        <li>Client receives location updates</li>
        <li>WebScoket initialization</li>
        <ul>
          <li>Request - latitude, longigude, timestamp</li>
          <li>Response - friends' location data</li>
        </ul>
        <li>Subscribe to a new friend</li>
        <ul>
          <li>Request - friend ID</li>
          <li>Response - friend's latest latitude, longigude, timestamp</li>
        </ul>
        <li>Unsubscribe a friend</li>
        <ul>
          <li>Request - friend ID</li>
          <li>Response - none</li>
        </ul>
      </ul>
      <li>Storage schema</li>
      <ul>
        <li>Location cache</li>
        <ul>
          <li>Redis is good choice due to fast read and write as well as TTL support</li>
          <li>Shard based on user_id and replicate each shard to improve availability</li>
          <li>Key - user_id</li>
          <li>Value - {latitude, longigude, timestamp}</li>
        </ul>
        <li>Location history DB</li>
        <ul>
          <li>Cassandra is good choice due to heavy write workload</li>
          <li>If using relational DB, sharding based on user ID is needed</li>
          <li>user_id</li>
          <li>latitude</li>
          <li>longigude</li>
          <li>timestamp</li>
        </ul>
        <li>User DB</li>
        <ul>
          <li>Relational DB with sharding based on user_id</li>
        </ul>
      </ul>
    </ul>

    <h3 class="card-title">Design Deep Dive</h3>
    <ul>
      <li>Scaling servers</li>
      <ul>
        <li>API servers</li>
        <ul>
          <li>Stateless, thus autoscale based on loads</li>
        </ul>
        <li>WebSocket servers</li>
        <ul>
          <li>Stateful, the load balancers need to gracefully open/close connections when adding/removing servers</li>
        </ul>
      </ul>
      <li>Client initialization</li>
      <ul>
        <li>User sends it's initial location</li>
        <li>Server loads all user's friends from user DB</li>
        <li>Server makes batched requests to cache to fetch locations of all friends</li>
        <li>Server computes distance between user and all friends</li>
        <li>For each friend within the search radius, server subscribes to friend's channel in Redis PubSub</li>
        <li>Server sends user's current location to user's channel in Redis PubSub</li>
      </ul>
      <li>Redis PubSub</li>
      <ul>
        <li>Design</li>
        <ul>
          <li>Uses a hash table and linked list to track the subscribers</li>
          <li>A user subscribes to each friend's channel whether the friend is online or not</li>
        </ul>
        <li>Memory</li>
        <ul>
          <li>Assume a channel is allocated for each user who uses nearby friend</li>
          <li>Then, we need 100M channels</li>
          <li>Assume each user have 100 active friends</li>
          <li>Assume 20 bytes is need to track each subscriber</li>
          <li>100M * 20 bytes * 100 = 200GB</li>
          <li>For a server with 100GB memory, 2 PubSub servers can hold all channels</li>
        </ul>
        <li>CPU</li>
        <ul>
          <li>Assume Redis PubSub sends 14 million updates per second to subscribers</li>
          <li>Assume each server can handle 100k pushes per seconds</li>
          <li>Then, 14M / 100K = 140 Redis PubSub servers are needed</li>
        </ul>
        <li>Distributed Redis PubSub</li>
        <ul>
          <li>Similar to consistent hashing, place Redis PubSub servers in a hash ring</li>
          <li>Compute hash of channels to determine which PubSub servers that those channels should go to</li>
          <li>The hash ring can be stored in service discovery like ZooKeeper and cached in WebSocket servers</li>
        </ul>
      </ul>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: System Design Interview Volume 2, Alex Xu & Sahn Lam
  </div>
</div>

<div class="card mb-4" id="system-design-2-3">
  <div class="card-body">
    <h2 class="card-title">Google Maps</h2>

    <h3 class="card-title">Understand the Problem and Establish Design Scope</h3>
    <ul>
      <li>How many total users in the system? 1B</li>
      <li>How many daily users in the system? 100M</li>
      <li>What is the main function of the system? Location update, navigation, ETA, map rendering</li>
      <li>What is the size of road data? terrabytes</li>
      <li>Should the system consider traffic conditions? Yes</li>
      <li>Should the system support different travel mode like driving, walking, public transit? Yes</li>
      <li>Should the system support multi-stop directions? No</li>
      <li>Should the system support business photos and videos? No</li>
      <li>Should the system show the route as user moves? Yes</li>
      <li>Should the system show new route when user deviates from the path? Yes</li>
    </ul>

    <h3 class="card-title">Storage</h3>
    <ul>
      <li>Map tiling</li>
      <ul>
        <li>Geo coding</li>
        <ul>
          <li>Convert each address to a pair of latitude and longitude</li>
        </ul>
        <li>Geo hashing</li>
        <ul>
          <li>Encode geographical area with digits</li>
          <li>Each geographical area is considered as a tile</li>
        </ul>
      </ul>
      <li>Map rendering</li>
      <ul>
        <li>Clients only download relevant tiles for the area</li>
      </ul>
      <li>Metadata - negligible</li>
      <li>Road info - assume terrabytes</li>
      <li>World map</li>
      <ul>
        <li>At zoom level 21, there are 2^21 = 4.3 trillion tiles</li>
        <li>Assume each 256 by 265 tile image is 100KB</li>
        <li>Then, higest zoom level would have 430PB</li>
        <li>Assume 90% of world surfaces are ocean, desert, mountains, etc</li>
        <li>Then, 430PB * 0.1 = 43PB</li>
        <li>The subsequent zoom level would have \( 43 + \dfrac{43}{4} + \dfrac{43}{16} + \dfrac{43}{64} + \dots \) = ? PB</li>
      </ul>
    </ul>

    <h3 class="card-title">Bandwidth</h3>
    <ul>
      <li>Assume</li>
      <ul>
        <li>Assume each user uses navigation 1 hour per day</li>
        <li>Then, 100M * 1hr = 100M hours per day = 360B seconds per day</li>
        <li>Assume each GPS update is sent every 10 seconds on average</li>
        <li>Then, 360B / 10 = 36B GPS updates per day</li>
        <li>Assume both request and response is 100 bytes</li>
      </ul>
      <li>Incoming/Outgoing - 36B * 100 bytes * 8 / 86400 = ? bps</li>
    </ul>

    <h3 class="card-title">Propose High-level Diagram and Get Buy-in</h3>
    <ul>
      <img class="img-fluid" class="card-img-top" src="/system-design/image/sd-c/google-maps-1.png" alt="Card image cap">
      <li>Location service</li>
      <ul>
        <li>Client sends its location every second</li>
        <li>Location updates can be buffered on the client and send to the server as a batch every 10-15 seconds</li>
      </ul>
      <li>Navigation service</li>
      <ul>
        <li>Find route between two points</li>
        <li>Runs shortest path algorithm</li>
        <li>Shows the path on the map for the user</li>
        <li>Sends updated directions when user deviates from the path</li>
      </ul>
      <li>Map rendering</li>
      <ul>
        <li>User fetches new map tile from the server when</li>
        <ul>
          <li>User is zooming on the map</li>
          <li>During navigation when user moves out from current tile and moves to nearby tile</li>
        </ul>
        <li>Map tiles are pre-computed at various zoom level</li>
        <ul>
          <li>Each map tile is represented by its geohash</li>
        </ul>
        <li>CDN serves as a cache of map tiles</li>
        <li>Map tile service takes user location and zoom level as inputs and return 9 URLs of tiles</li>
        <ul>
          <li>Return value includes the tile to render and 8 surrounding tiles</li>
        </ul>
        <li>User downloads the tiles from CDN</li>
      </ul>
      <li>API</li>
      <ul>
        <li><code>curr_location(location)</code></li>
        <ul>
          <li>Display current location of user on the map</li>
          <li><code>location</code></li>
          <ul>
            <li>Boolean to indicate if user location is turned on or off</li>
            <li>If off, the service will ask user to turn on the location</li>
          </ul>
        </ul>
        <li><code>find_route(start, end, transport_type)</code></li>
        <ul>
          <li>Computes optimal route between two points</li>
          <li><code>start</code></li>
          <ul>
            <li>Start point in text format</li>
          </ul>
          <li><code>end</code></li>
          <ul>
            <li>Start point in text format</li>
          </ul>
        </ul>
        <li><code>directions(curr_location, steps)</code></li>
        <ul>
          <li>Provides alerts of when and where to make turns</li>
        </ul>
      </ul>
      <li>Database</li>
      <ul>
        <li>Road info</li>
        <ul>
          <li>Graph data is represented as adjacency list</li>
          <li>Serialize adjacency lists into binary files</li>
          <li>Use S3 to store data and cache it on the servers</li>
        </ul>
        <li>User location data</li>
        <ul>
          <li>Use Cassandra for write-heavy workload and availability over consistency</li>
          <ul>Schema</ul>
          <ul>
            <li>Key - (user_id, timestamp)</li>
            <li>Value - (latitude, longitude)</li>
          </ul>
        </ul>
        <li>Geocoding DB</li>
        <ul>
          <li>Use Redis for read-heavy workload</li>
          <ul>Schema</ul>
          <ul>
            <li>place_id</li>
            <li>location</li>
          </ul>
        </ul>
        <li>World map</li>
        <ul>
          <li>Use S3 and cache it on CDN</li>
        </ul>
      </ul>
    </ul>

    <h3 class="card-title">Design Deep Dive</h3>
    <ul>
      <li>Location service</li>
      <ul>
        <li>Other services consume location data from Kafka</li>
      </ul>
      <li>Map rendering</li>
      <ul>
        <li>WebGL - instead of sending images over network, send vectors (paths and polygons)</li>
        <li>Compressed vector data is much smaller than compressed images</li>
        <li>Zooming experience is much better with vectors</li>
      </ul>
      <li>Navigation service</li>
      <ul>
        <li>Geocoding service</li>
        <ul>
          <li>Converts location name to lat/long pair</li>
        </ul>
        <li>Routing planner service</li>
        <ul>
          <li>Computes suggested routes using other services</li>
        </ul>
        <li>Shortest path service</li>
        <ul>
          <li>Returns top k shortest paths without considering traffic or current conditions</li>
        </ul>
        <li>ETA service</li>
        <ul>
          <li>Computes ETA for each route using ML from current traffic and historical data</li>
        </ul>
        <li>Ranker</li>
        <ul>
          <li>Applies filters defined by user like avoid tolls or highways</li>
        </ul>
        <li>Traffic update service</li>
        <ul>
          <li>Updates traffic DB so that ETA service can compute accurate estimates</li>
        </ul>
      </ul>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: System Design Interview Volume 2, Alex Xu & Sahn Lam
  </div>
</div>

<div class="card mb-4" id="system-design-2-4">
  <div class="card-body">
    <h2 class="card-title">Distributed Message Queue</h2>

    <h3 class="card-title">Understand the Problem and Establish Design Scope</h3>
    <ul>
      <li>What is format of messages? Text messages only</li>
      <li>What is size of messages? KBs</li>
      <li>Should messages be repeatedly consumed? Yes by different consumers</li>
      <li>Should messages be consumed in the same order that they arrived? Yes</li>
      <li>Should data be persisted? Yes for two weeks</li>
      <li>How many producers and consumers should the system support? As many as it can</li>
      <li>Should we support at most once, at least once, or exactly once? All of them</li>
      <li>Should the system have both high throughput and low latency? Yes</li>
    </ul>

    <h3 class="card-title">Propose High-level Diagram and Get Buy-in<</h3>
    <ul>
      <img class="img-fluid" class="card-img-top" src="/system-design/image/sd-c/distributed-message-queue-1.png" alt="Card image cap">
      <li>Producer</li>
      <ul>
        <li>Topic organizes messages</li>
        <li>Sends messages to a partition of a topic</li>
      </ul>
      <li>Consumer groups</li>
      <ul>
        <li>Each group can subscribe one or more topics</li>
      </ul>
      <li>Brokers</li>
      <ul>
        <li>Holds partitions of topics</li>
        <li>Data storage - persists messages in a form of partitions</li>
      </ul>
      <li>Zookeeper</li>
      <ul>
        <li>Metadata stroage - persists configuration of topics</li>
        <li>State storage - manages consumer states</li>
        <li>Coordination service</li>
        <ul>
          <li>Service discovery - checks which brokers are alive</li>
          <li>Leader election - only one broker serves as active controller at a time, which assigns partitions in the cluster</li>
        </ul>
      </ul>
    </ul>

    <h3 class="card-title">Design Deep Dive</h3>
    <ul>
      <li>Data storage</li>
      <ul>
        <li>Traffic pattern of message queue</li>
        <ul>
          <li>Both read-heavy and write-heavy</li>
          <li>No update or delete operation</li>
          <li>Mostly sequential read/write access</li>
        </ul>
        <li>Database is hard to be suitable for both read-heavy and write-heavy access pattern</li>
        <li>Write-ahead log (WAL) has pure sequential read/write access pattern</li>
        <ul>
          <li>Messages can be stored as WAL on disk</li>
          <li>Disk is slow for random access but it can be fast for sequential access</li>
          <li>New messages are appended to the tail of a partition</li>
          <li>Partition is divided into segments where active segments serve writes and inactive segments serve reads</li>
          <li>Old inactive segments can be truncated</li>
        </ul>
      </ul>
      <li>Message data structure</li>
      <ul>
        <li>key - used to determine the partition of message by hash(key) % number_of_partitions</li>
        <li>value - payload of message</li>
        <li>topic - topic that message belongs to</li>
        <li>partition - partition id that message belongs to</li>
        <li>offset - position of message in partition</li>
        <li>timestamp - timestamp of when message was stored</li>
        <li>size - size of message</li>
        <li>crc - cyclic redundancy check used to ensure data integrity</li>
      </ul>
      <li>Batching</li>
      <ul>
        <li>Message queue</li>
        <ul>
          <li>There is trade-off between throughput and latency</li>
          <li>Smaller batch size (thus, small throughput) leads to low latency and vice-versa</li>
        </ul>
        <li>Producer</li>
        <ul>
          <img class="img-fluid" class="card-img-top" src="/system-design/image/sd-c/distributed-message-queue-2.png" alt="Card image cap">
          <li>Batching buffers messages in memory and send out larger batches in a single request, which increases throughput</li>
          <li>Routing decides which replica/partition should the message go to</li>
          <li>Leader replica receives the message and follower replicas pull data from leader</li>
        </ul>
        <li>Consumer</li>
        <ul>
          <li>Brokers push data to consumers</li>
          <ul>
            <li>Pros</li>
            <ul>
              <li>Messages are propagated immediately, thus lower latency</li>
            </ul>
            <li>Cons</li>
            <ul>
              <li>Consumers could be overwhelmed</li>
            </ul>
          </ul>
          <li>Consumers pull data from brokers</li>
          <ul>
            <li>Pros</li>
            <ul>
              <li>Consumers can control consumption rate, which is suitable for batch processing</li>
            </ul>
            <li>Cons</li>
            <ul>
              <li>Requires long polling, that waits some time for new messages</li>
            </ul>
            <li>Workflow</li>
            <ul>
              <img class="img-fluid" class="card-img-top" src="/system-design/image/sd-c/distributed-message-queue-3.png" alt="Card image cap">
              <li>A new consumer wants to join group 1 and subscribes to topic A</li>
              <li>All consumers in the same group connect to the same broker</li>
              <li>Coordinator assigns partition 2 to consumer</li>
              <li>Consumer fetches message from the last consumed offset, which is managed by state storage</li>
              <li>Consumer processes messages and commits the offset to the broker</li>
            </ul>
          </ul>
          <li>Consumer rebalancing</li>
          <ul>
            <li>Decides which consumer is responsible for which subset of partitions</li>
            <li>Needed when consumer joins, consumer leaves, consumer crashes, paritions are adjusted</li>
            <img class="img-fluid" class="card-img-top" src="/system-design/image/sd-c/distributed-message-queue-4.png" alt="Card image cap">
            <li>Each consumer belongs to a group. Each consumer finds the coordinator by hashing the group name. All consumers from the same group are connected to the same coordinator</li>
            <li>Coordinator elects new leader for the group</li>
            <li>Leader generates new partition dispatch plan and reports it back to coordinator. Coordinator broadcasts the plan to other consumers in the group</li>
          </ul>
        </ul>
      </ul>
      <li>Replication</li>
      <ul>
        <li>For example, each partition having 3 replica means that 1 leader and 2 followers are distributed in different brokers</li>
        <li>Messages are only written to the leader, and followers sync messages from leader</li>
        <li>In-sync replica - replicas that are in-sync with the leader</li>
        <li>ACK=all</li>
        <ul>
          <li>Producers get ACK when all in-sync replicas received the message</li>
          <li>High latency but strong message durability</li>
        </ul>
        <li>ACK=1</li>
        <ul>
          <li>Producers get ACK when leader received the message</li>
          <li>Low latency but occasional data loss</li>
        </ul>
        <li>ACK=0</li>
        <ul>
          <li>Producers send messages to leader without waiting for any ACK</li>
          <li>Suitable for collecting metrics or logging data</li>
        </ul>
      </ul>
      <li>Scalability</li>
      <ul>
        <li>Each partition of each topic will have specific number of replicas that are spread across different brokers</li>
        <li>When a broker is added or removed, replicas will be re-distributed</li>
        <li>When a partition is added to a topic, messages will also be persisted in the newly added partition</li>
        <li>When a partition is removed from a topic, producers only send messages to remaining partitions but consumers still consume from removed partition during certain retention period</li>
      </ul>
      <li>Data delivery semantics</li>
      <ul>
        <li>At most once</li>
        <ul>
          <li>ACK=0</li>
          <li>Consumer fetches the message and commits the offset before data is processed</li>
          <li>If consumer crashes just after offset commit, message will not be re-consumed</li>
        </ul>
        <li>At least once</li>
        <ul>
          <li>ACK=1 or ACK=all</li>
          <li>Consumer fetches the message and commits the offset only after data is processed</li>
          <li>If consumer fails to process the message, it will re-consume the message (There is no data loss)</li>
          <li>If consumer processes the message but fails to commit offset to broker, it will re-consume the message (There will be duplicates)</li>
        </ul>
        <li>Exactly once</li>
        <ul>
          <li>High cost for system performance and complexity</li>
        </ul>
      </ul>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: System Design Interview Volume 2, Alex Xu & Sahn Lam
  </div>
</div>

<div class="card mb-4" id="system-design-2-5">
  <div class="card-body">
    <h2 class="card-title">Metrics Monitoring and Alerting System</h2>

    <h3 class="card-title">Understand the Problem and Establish Design Scope</h3>
    <ul>
      <li>What metrics should the system collect? CPU load, memory usage, disk space comsumption, request per second of a service, running server count of web pool</li>
      <li>How many servers are there? 1000 server pools, 100 machines per pool. Also 100M daily active users</li>
      <li>How long should the system keep the data? 1 year</li>
      <li>How should the system alert metrics? Email, webhooks</li>
      <li>Should the system collect logs? No</li>
      <li>Should the system support distributed system tracing? No</li>
    </ul>

    <h3 class="card-title">Propose High-level Diagram and Get Buy-in</h3>
    <ul>
      <img class="img-fluid" class="card-img-top" src="/system-design/image/sd-c/metrics-monitoring-and-alerting-system-1.png" alt="Card image cap">
      <li>Metrics source</li>
      <ul>
        <li>Ex. application servers, SQL databases, message queues, etc</li>
      </ul>
      <li>Metrics collector</li>
      <ul>
        <li>Gathers metrics data and write them into time-series DB</li>
      </ul>
      <li>Time-series DB</li>
      <ul>
        <li>Store metrics data as time series</li>
        <li>Indexes lables to allow fast lookup based on labels</li>
      </ul>
      <li>Alerting system</li>
      <ul>
        <li>Send notifications to various destinations</li>
      </ul>
      <li>Visualization system</li>
      <ul>
        <li>Shows metrics in graphs/charts</li>
      </ul>
      <li>Data storage</li>
      <ul>
        <li>Need time-series DB</li>
        <ul>
          <li>Ex. MetricsDB, Timestream, InfluxDB, Prometheus</li>
        </ul>
      </ul>
      <li>Storage schema</li>
      <ul>
        <li>Constant heavy write load and spiky read load</li>
        <li>A metric name - string</li>
        <li>A set of tags/labels - list of &lt;key:value&gt; pairs</li>
        <li>An array of values and their timestamps - an array of &lt;value:timestamp&gt; pairs</li>
      </ul>
    </ul>

    <h3 class="card-title">Design Deep Dive</h3>
    <ul>
      <li>Metrics collections</li>
      <ul>
        <li>Pull</li>
        <ul>
          <img class="img-fluid" class="card-img-top" src="/system-design/image/sd-c/metrics-monitoring-and-alerting-system-2.png" alt="Card image cap">
          <li>Metrics collector fetches configuration metadata of service endpoints (such as pulling interval, IP addresses, timeout, etc) from service discovery</li>
          <li>Metrics collector pulls metrics data via pre-defined HTTP endpoints</li>
          <li>Distribute metrics source to a scalable set of metrics collectors via consistent hashing</li>
          <li>Ex. Prometheus</li>
          <li>Pros</li>
          <ul>
            <li>Easy debugging - HTTPS endpoint can be used to view metrics at any time</li>
            <li>Health check - if application server doesn't respond, it's down</li>
          </ul>
        </ul>
        <li>Push</li>
        <ul>
          <img class="img-fluid" class="card-img-top" src="/system-design/image/ref-sd-2/metrics-monitoring-and-alerting-system-3.png" alt="Card image cap">
          <li>Collection agent is installed on every server being monitored</li>
          <li>Collection agent periodically pushes metrics to metrics collectors</li>
          <li>Ex. Amazon CloudWatch, Graphite</li>
          <li>Pros</li>
          <ul>
            <li>Networking - load balancer allows receiving data from anywhere</li>
          </ul>
        </ul>
      </ul>
      <li>Scaling metrics transmission pipeline</li>
      <ul>
        <li>Place Kafka between metrics collector and time-series DB</li>
        <li>Decouples data collection and data processing</li>
        <li>Prevents data loss when DB is unavailable by retaining data in Kafka</li>
      </ul>
      <li>Query service</li>
      <ul>
        <li>Add a cache layer to query service</li>
        <li>Prometheus and InfluxDB have their own query language</li>
      </ul>
      <li>Storage layer</li>
      <ul>
        <li>Data encoding</li>
        <ul>
          <li>Instead of storing full timestamp, store only the delta</li>
        </ul>
        <li>Downsampling</li>
        <ul>
          <li>Up to 7 days old data - no downsampling</li>
          <li>Between 7 days and 30 days - downsample to 1 minute resolution</li>
          <li>Between 30 days and 1 year - downsample to 1 hour resolution</li>
        </ul>
      </ul>
      <img class="img-fluid" class="card-img-top" src="/system-design/image/sd-c/metrics-monitoring-and-alerting-system-4.png" alt="Card image cap">
      <li>Alerting system</li>
      <ul>
        <li>Load config file (which has alert rules) to cache server</li>
        <li>Alert manager fetches alert configs from the cache</li>
        <li>Alert manager calls query service at pre-defined interval</li>
        <li>Alert store is a key-value DB (Ex. Cassandra) that keeps state of all alerts</li>
        <li>Eligible alerts are inserted into Kafka</li>
        <li>Alert consumers pull alert events from Kafka</li>
      </ul>
      <li>Visualization system</li>
      <ul>
        <li>A high-quality visualization system is hard to build</li>
        <li>Use product like Grafana</li>
      </ul>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: System Design Interview Volume 2, Alex Xu & Sahn Lam
  </div>
</div>

<div class="card mb-4" id="system-design-2-6">
  <div class="card-body">
    <h2 class="card-title">Ad Click Event Aggregation</h2>

    <h3 class="card-title">Understand the Problem and Establish Design Scope</h3>
    <ul>
      <li>How does input data look like? Log files where latest click events are appended to them</li>
      <li>How does event look like in terms of data attribute? ad_id, click_timestamp, user_id, ip, country</li>
      <li>How large is dataset? 1B clicks per day and 2M Ads where number of Ads grows 30% per year</li>
      <li>What kind of queries should the system support?</li>
      <ul>
        <li>Return the number of clicks for a particular Ad in last M minites</li>
        <li>Return the top 100 most clicked Ads in past 1 minute</li>
        <li>Support filtering by user_id, ip, country for above two queries</li>
      </ul>
      <li>What kind of edge cases are there?</li>
      <ul>
        <li>Events that arrive later than expected</li>
        <li>Duplicate events</li>
      </ul>
      <li>Should the system have low latency? Less than 1s for real-time bidding, few minutes for ad click event aggregation</li>
    </ul>

    <h3 class="card-title">Storage</h3>
    <ul>
      <li>Assume single click requires 0.1KB storage</li>
      <li>0.1KB * 1B = 100GB per day = 3TB per month</li>
    </ul>

    <h3 class="card-title">Bandwidth</h3>
    <ul>
      <li>Incoming - (1B / 24 * 86400) clicks per second on average</li>
      <li>Assuming peak is 5 times the average number, then 5 * (1B / 24 * 86400) clicks per second during peak</li>
    </ul>

    <h3 class="card-title">Propose High-level Diagram and Get Buy-in</h3>
    <ul>
      <img class="img-fluid" class="card-img-top" src="/system-design/image/sd-c/ad-click-event-aggregation-1.png" alt="Card image cap">
      <li>Message queues</li>
      <ul>
        <li>First message queue</li>
        <ul>
          <li>ad_id, click_timestamp, user_id, ip, country</li>
        </ul>
        <li>Second message queue</li>
        <ul>
          <li>ad_id, click_minute, count</li>
          <li>update_time_minute, most_clicked_ads</li>
        </ul>
        <li>Delivery guarantee</li>
        <ul>
          <li>At-least once is not good enough because difference of few percent data can result in huge discrepancies</li>
          <li>Exactly-once is needed</li>
        </ul>
      </ul>
      <li>Data aggregation service</li>
      <ul>
        <li>MapReduce</li>
        <ul>
          <li>Map node</li>
          <ul>
            <li>Read data from data source and transform it (clean and normalize data)</li>
          </ul>
          <li>Aggregate node</li>
          <ul>
            <li>Counts ad click events by ad_id every minute</li>
          </ul>
          <li>Reduce node</li>
          <ul>
            <li>Combines aggregated results from all Aggregate nodes to produce the final result</li>
          </ul>
        </ul>
      </ul>
      <li>API</li>
      <ul>
        <li>Return aggregated event count for a given <code>ad_id</code></li>
        <ul>
          <li><code>GET /v1/ads/{:ad_id}/aggregated_count</code></li>
          <li>Request</li>
          <ul>
            <li><code>from</code> - start minute</li>
            <li><code>to</code> - end minute</li>
            <li><code>filter</code> - identifier for different filtering strategies</li>
          </ul>
          <li>Response</li>
          <ul>
            <li><code>ad_id</code></li>
            <li><code>count</code> - aggregated count between start and end minutes</li>
          </ul>
        </ul>
        <li>Return top N most clicked ads in last M minutes</li>
        <ul>
          <li><code>GET /v1/ads/popular_ads</code></li>
          <li>Request</li>
          <ul>
            <li><code>count</code> - specifies N</li>
            <li><code>window</code> - specifies M</li>
            <li><code>filter</code> - identifier for different filtering strategies</li>
          </ul>
          <li>Response</li>
          <ul>
            <li><code>ad_ids</code></li>
          </ul>
        </ul>
      </ul>
      <li>Storage schema</li>
      <ul>
        <li>Raw data serves as backup data in case aggregated data is corrupted</li>
        <li>Aggregated data makes queries fast</li>
        <li>Raw data</li>
        <ul>
          <li>Little read and heavy write</li>
          <li>Use Cassandra or InfluxDB because these are good for writes and time-range queries</li>
          <ul>
            <li>ad_id</li>
            <li>click_timestamp</li>
            <li>user_id</li>
            <li>ip</li>
            <li>country</li>
          </ul>
        </ul>
        <li>Aggregated data</li>
        <ul>
          <li>Heavy read and write</li>
          <li>Use Cassandra or InfluxDB here as well</li>
        </ul>
        <ul>
          <li>Aggregated data with filters</li>
          <ul>
            <li>ad_id</li>
            <li>click_minute</li>
            <li>filter_id</li>
            <li>count</li>
          </ul>
          <li>Filter table</li>
          <ul>
            <li>filter_id</li>
            <li>region</li>
            <li>ip</li>
            <li>user_id</li>
          </ul>
          <li>Most clicked_ads</li>
          <ul>
            <li>window_size - integer</li>
            <li>update_time_minute - timestamp</li>
            <li>most_clicked_ads - array</li>
          </ul>
        </ul>
      </ul>
    </ul>

    <h3 class="card-title">Design Deep Dive</h3>
    <ul>
      <img class="img-fluid" class="card-img-top" src="/system-design/image/sd-c/ad-click-event-aggregation-2.png" alt="Card image cap">
      <li>Streaming vs batching</li>
      <ul>
        <li>Use streaming to process data as it arrives and generate aggregated result in real-time</li>
        <li>Use batch for historical data backup</li>
        <li>Lambda</li>
        <ul>
          <li>Contains batch and streaming simultaneously</li>
          <li>Need to maintain two codebases</li>
        </ul>
        <li>Kappa</li>
        <ul>
          <li>Combines batch and streaming in one processing path</li>
          <li>Handle both real-time data processing and continuous data reprocessing using a single stream processing engine</li>
        </ul>
        <li>Data recalculation</li>
        <ul>
          <li>Recalculation service retrieves data from raw data storage, which is done by batch job</li>
          <li>Data is sent to a dedicated aggregation service (Real-time processing is not impacted)</li>
          <li>Aggregated results are sent to second message queue, then updated in aggregation DB</li>
        </ul>
        <li>Time</li>
        <ul>
          <li>Event time</li>
          <ul>
            <li>When an Ad click happens</li>
            <li>Pros - aggregation results are more accurate</li>
            <li>Cons - depends on timestamp generated on client side</li>
            <li>Watermark</li>
            <ul>
              <li>Extend aggregation window</li>
              <li>Long watermark increases accuracy but adds more latency to the system</li>
              <li>Alternative is to use reconcilation</li>
              <ul>
                <li>Sort Ad click events by event time in every partition at the end of each day (using a batch job and reconciling with real-time aggregation result)</li>
                <li>Result from batch job might not match with real-time aggregation result, since some event might arrive late</li>
              </ul>
            </ul>
          </ul>
          <li>Processing time</li>
          <ul>
            <li>System time of aggregation server that processes the click event</li>
            <li>Pros - server timestamps are more reliable</li>
            <li>Cons - timestamp is not accurate if event reaches the system at much later time</li>
          </ul>
        </ul>
      </ul>
      <li>Aggregation window</li>
      <ul>
        <li>Tumbling window</li>
        <ul>
          <li>Time is partitioned into same-length and non-overlapping chunks</li>
          <li>Good for aggregating ad click events every minute</li>
        </ul>
        <li>Sliding window</li>
        <ul>
          <li>Events are grouped within a window that slides across the data stream</li>
          <li>Good for getting top N most clicked Ads during last M minutes</li>
        </ul>
      </ul>
      <li>Scaling</li>
      <ul>
        <li>Message queues</li>
        <ul>
          <li>Producers</li>
          <ul>
            <li>There is no limit on number of producers</li>
          </ul>
          <li>Consumers</li>
          <ul>
            <li>Rebalancing</li>
          </ul>
          <li>Brokers</li>
          <ul>
            <li>Use ad_id as hashing key for Kafka partition</li>
            <li>Then, aggregation service can subscribe to all events of an ad_id from one single partition</li>
            <li>Pre-allocate enough partitions to avoid increasing the number of partitions</li>
          </ul>
        </ul>
        <li>Data aggregation service</li>
        <ul>
          <li>Option 1. allocate events with different ad_ids to different thread</li>
          <li>Option 2. Deploy aggregation service nodes on resource providers like Apache Hadoop YARN</li>
          <li>Handling hotspot</li>
          <ul>
            <li>Assume each node can handle only 100 events</li>
            <li>Assume there are 300 events in a node</li>
            <li>Node applies for extra resources to resource manager</li>
            <li>Resource manager allocates more resources</li>
            <li>Node split event into 3 groups and each node handles 100 events</li>
            <li>Result is written back to original node</li>
          </ul>
        </ul>
        <li>DB</li>
        <ul>
          <li>Cassandra natively supports horizontal scaling</li>
        </ul>
      </ul>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: System Design Interview Volume 2, Alex Xu & Sahn Lam
  </div>
</div>

<div class="card mb-4" id="system-design-2-7">
  <div class="card-body">
    <h2 class="card-title">Hotel Reservation System</h2>

    <h3 class="card-title">Understand the Problem and Establish Design Scope</h3>
    <ul>
      <li>How many hotels? 5000 hotels and 1M rooms</li>
      <li>When do customers pay? They pay full when they make reservation</li>
      <li>Where do users book hotel? Hotel website or app</li>
      <li>Can users cancel reservations? Yes</li>
      <li>Should the system support overbooking? Yes, 10%</li>
    </ul>

    <h3 class="card-title">Bandwidth</h3>
    <ul>
      <li>Assume 70% of rooms are booked and stay duration is 3 days on average</li>
      <li>(1M * 0.7) / (3 * 86400) = ? reservations per second</li>
    </ul>

    <h3 class="card-title">High level design</h3>
    <ul>
      <img class="img-fluid" class="card-img-top" src="/system-design/image/sd-c/hotel-reservation-1.png" alt="Card image cap">
      <li>Public API gateway - rate limiting, authentication</li>
      <li>Internal API - available to staff and protected via VPN</li>
      <li>Hotel service - hotel and room information</li>
      <li>Rate service - provides room state for different future dates</li>
      <li>Reservation service - makes reservations and tracks room inventory</li>
      <li>Payment service - manages transactions</li>
    </ul>
    <li>API</li>
    <ul>
      <li>Hotel related API</li>
      <ul>
        <li>Get detailed information about a hotel</li>
        <ul>
          <li><code>GET /v1/hotels/ID</code></li>
        </ul>
        <li>Add a new hotel</li>
        <ul>
          <li><code>POST /v1/hotels</code></li>
        </ul>
        <li>Update hotel information</li>
        <ul>
          <li><code>PUT /v1/hotels/ID</code></li>
        </ul>
        <li>Delete a hotel</li>
        <ul>
          <li><code>DELETE /v1/hotels/ID</code></li>
        </ul>
      </ul>
      <li>Room related API</li>
      <ul>
        <li>Get detailed information about a room</li>
        <ul>
          <li><code>GET /v1/hotels/ID/rooms/ID</code></li>
        </ul>
        <li>Add a new room</li>
        <ul>
          <li><code>POST /v1/hotels/ID/rooms</code></li>
        </ul>
        <li>Update room information</li>
        <ul>
          <li><code>PUT /v1/hotels/ID/rooms/ID</code></li>
        </ul>
        <li>Delete a room</li>
        <ul>
          <li><code>DELETE /v1/hotels/ID/rooms/ID</code></li>
        </ul>
      </ul>
      <li>Reservation related API</li>
      <ul>
        <li>Get reservation history of logged-in user</li>
        <ul>
          <li><code>GET /v1/reservations</code></li>
        </ul>
        <li>Get detailed information about a reservation</li>
        <ul>
          <li><code>GET /v1/reservations/ID</code></li>
        </ul>
        <li>Make a new reservation</li>
        <ul>
          <li><code>POST /v1/reservations</code></li>
        </ul>
        <li>Cancel a reservation</li>
        <ul>
          <li><code>DELETE /v1/reservations/ID</code></li>
        </ul>
      </ul>
    </ul>
    <li>Storage schema</li>
    <ul>
      <li>Heavy read and little write</li>
      <li>Thus, choose relational</li>
      <li>ACID property prevents double charge, double booking, etc</li>
      <li>Shard DB by hash(hotel_id) % number_of_servers</li>
      <li>Hotel</li>
      <ul>
        <li>hotel_id (pk)</li>
        <li>name</li>
        <li>address</li>
        <li>location</li>
      </ul>
      <li>Room</li>
      <ul>
        <li>room_id (pk)</li>
        <li>room_type_id</li>
        <li>floor</li>
        <li>number</li>
        <li>hotel_id</li>
        <li>name</li>
        <li>is_available</li>
      </ul>
      <li>room_type_rate</li>
      <ul>
        <li>hotel_id (pk)</li>
        <li>date (pk)</li>
        <li>rate</li>
      </ul>
      <li>guest</li>
      <ul>
        <li>guest_id (pk)</li>
        <li>first_name</li>
        <li>last_name</li>
        <li>email</li>
      </ul>
      <li>room_type_inventory</li>
      <ul>
        <li>hotel_id (pk)</li>
        <li>room_type_id (pk)</li>
        <li>date (pk)</li>
        <li>total_inventory - total number of rooms minus those that are temporarily taken off the inventory</li>
        <li>total_reserved - total number of rooms booked for specific hotel_id, room_type_id, date</li>
      </ul>
      <li>reservation</li>
      <ul>
        <li>reservation_id (pk)</li>
        <li>hotel_id</li>
        <li>room_type_id</li>
        <li>start_date</li>
        <li>end_date</li>
        <li>status</li>
        <li>guest_id</li>
      </ul>
    </ul>

    <h3 class="card-title">Detailed design</h3>
    <ul>
      <li>Concurrecny</li>
      <ul>
        <li>Same user clicks submit button multiple times</li>
        <ul>
          <li>Option 1. Gray out submit button once request is sent. Not reliable because users can disable JavaScript</li>
          <li>Option 2. Idempotent API produces the same result no mater how many times it is called. Uses globally unique ID generator</li>
        </ul>
        <li>Mutiple users try the book the same room at the same time</li>
        <ul>
          <li>Option 1. persistent locking</li>
          <ul>
            <li>Place a lock on record as soon as one user starts to update it</li>
            <li>Pros</li>
            <ul>
              <li>Useful when data contention is heavy</li>
            </ul>
            <li>Cons</li>
            <ul>
              <li>Deadlock may occur</li>
              <li>Not scalable because when a transaction is locked for too long, other transactions cannot access the resouce</li>
            </ul>
          </ul>
          <li>Option 2. optimistic locking</li>
          <ul>
            <li>Use either version number or timestamp to allow multiple users to update the same resource</li>
            <li>Pros</li>
            <ul>
              <li>No need to lock the resouce</li>
            </ul>
            <li>Cons</li>
            <ul>
              <li>Poor performance when data contention is heavy</li>
            </ul>
          </ul>
        </ul>
      </ul>
      <li>Scaling</li>
      <ul>
        <img class="img-fluid" class="card-img-top" src="/system-design/image/sd-c/hotel-reservation-2.png" alt="Card image cap">
        <li>Reservation cache</li>
        <ul>
          <li>Query the number of available rooms for a given hotel, room type, date range</li>
          <li>Reserve a room by executing total_reserved+1</li>
          <li>Update inventory when a user cancels a reservation</li>
        </ul>
        <li>Inventory cache</li>
        <ul>
          <li>Most read operations are served by cache</li>
        </ul>
        <li>Inventory DB</li>
        <ul>
          <li>Stores inventory data as source of truth</li>
        </ul>
      </ul>
      <li>Consistency</li>
      <ul>
        <li>Microservice architecture (where each service has their own DB) brings great challenge in data consistency</li>
      </ul>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: System Design Interview Volume 2, Alex Xu & Sahn Lam
  </div>
</div>

<div class="card mb-4" id="system-design-2-8">
  <div class="card-body">
    <h2 class="card-title">Distributed Email Service</h2>

    <h3 class="card-title">Understand the Problem and Establish Design Scope</h3>
    <ul>
      <li>How many users are there? 1B</li>
      <li>What kind of features should the system support? Send email, search emails, spam filtering</li>
      <li>Should the system suport email attachment? Yes</li>
      <li>How do users connect with mail servers? HTTP</li>
    </ul>

    <h3 class="card-title">Bandwidth</h3>
    <ul>
      <li>Assume each user sends 10 emails per day</li>
      <li>Assume each user receives 50 emails per day, size of email is 10KB on average, size of attachment is 1MB on average, 20% of emails have attachment</li>
      <li>Incoming - (1B * 50 * (10KB + 1MB * 0.2)) per day</li>
    </ul>
    
    <h3 class="card-title">Propose High-level Diagram and Get Buy-in</h3>
    <ul>
      <img class="img-fluid" class="card-img-top" src="/system-design/image/sd-c/distributed-email-service-1.png" alt="Card image cap">
      <li>Webmail</li>
      <ul>
        <li>Users use web browsers to send and receive emails</li>
      </ul>
      <li>Real-time servers</li>
      <ul>
        <li>Push new email updates to clients in real-time</li>
        <li>Stateful servers that maintain persistent connections</li>
        <li>Use WebSocket whenever possible and fall back to long-polling in case WebSocket has browser compatibility issue</li>
      </ul>
      <li>Metadata DB</li>
      <ul>
        <li>Stores email metadata</li>
      </ul>
      <li>Attachment store</li>
      <ul>
        <li>Use object storage like S3</li>
      </ul>
      <li>Distributed cache</li>
      <ul>
        <li>Cache recent emails since they are loaded frequently</li>
        <li>Use Redis</li>
      </ul>
      <li>Search store</li>
      <ul>
        <li>Distributed document store that uses inverted index</li>
      </ul>
      <li>Email sending flow</li>
      <ul>
        <img class="img-fluid" class="card-img-top" src="/system-design/image/sd-c/distributed-email-service-2.png" alt="Card image cap">
        <li>User sneds an email. Request is sent to load balancer</li>
        <li>Load balancer performs rate limiting and routes request to web servers</li>
        <li>Web servers performs email validation</li>
        <li>If validation succeeds, email is sent to outgoing queue</li>
        <li>If validation fails, email is sent to error queue</li>
        <li>SMPT outgoing workers pull messages and check for spam and virus</li>
        <li>Outgoing email is stored in "Sent" folder in storage</li>
        <li>SMPT outgoing workers sends email to recipient mail server</li>
      </ul>
      <li>Email receiving flow</li>
      <ul>
        <img class="img-fluid" class="card-img-top" src="/software-engineering/image/sd-c/distributed-email-service-3.png" alt="Card image cap">
        <li>Load balancer distributes emails across SMTP servers</li>
        <li>Mail processing workers checks spam and virus</li>
        <li>Email is stored in email storage, cache, and object storage</li>
        <li>If receiver is currently online, email is pushed to real-time servers</li>
        <li>If receiver is offline, webmail client calls web servers and web servers pull new emails from storage layer</li>
      </ul>
      <li>Storage schema</li>
      <ul>
        <li>A sinlge column can be MBs</li>
        <li>Strong consistency needed</li>
        <li>Disk I/O should be small</li>
        <li>Highly available and fault tolerant</li>
        <li>Should be easy to create incremental backup</li>
        <li>folders_by_user</li>
        <ul>
          <li>user_id - uuid, partition key</li>
          <li>folder_id - uuid</li>
          <li>folder_name - text</li>
        </ul>
        <li>emails_by_folder</li>
        <ul>
          <li>user_id - uuid, partition key</li>
          <li>folder_id - uuid, partition key</li>
          <li>email_id - timeuuid, clustering key</li>
          <li>from - text</li>
          <li>subject - text</li>
          <li>preview - text</li>
          <li>is_read - boolean</li>
        </ul>
        <li>emails_by_user</li>
        <ul>
          <li>user_id - uuid, partition key</li>
          <li>email_id - timeuuid, clustering key</li>
          <li>from - text</li>
          <li>to - list&lt;text&gt;</li>
          <li>subject - text</li>
          <li>body - text</li>
          <li>attachments - list&lt;filename|size&gt;</li>
        </ul>
        <li>attachments</li>
        <ul>
          <li>email_id - timeuuid, clustering key</li>
          <li>filename - uuid, partition key</li>
          <li>url - text</li>
        </ul>
        <li>read_emails</li>
        <ul>
          <li>user_id - uuid, partition key</li>
          <li>folder_id - uuid, partition key</li>
          <li>email_id - timeuuid, clustering key</li>
          <li>from - text</li>
          <li>subject - text</li>
          <li>preview - text</li>
        </ul>
        <li>unread_emails</li>
        <ul>
          <li>user_id - uuid, partition key</li>
          <li>folder_id - uuid, partition key</li>
          <li>email_id - timeuuid, clustering key</li>
          <li>from - text</li>
          <li>subject - text</li>
          <li>preview - text</li>
        </ul>
      </ul>
    </ul>

    <h3 class="card-title">Design Deep Dive</h3>
    <ul>
      <li>Email deliverability</li>
      <ul>
        <li>Use dedicated IP addresses for sending emails</li>
        <li>Send different categories of emails from different IP addresses</li>
        <li>Warm up new email server IP addresses slowly to build reputation</li>
        <li>Ban spammers quickly</li>
      </ul>
      <li>Search</li>
      <ul>
        <li>Option 1. Elasticsearch</li>
        <ul>
          <li>Pros</li>
          <ul>
            <li>Scalable to some extent</li>
            <li>Easy to integrate</li>
          </ul>
          <li>Cons</li>
          <ul>
            <li>Need to maintain two different systems</li>
            <li>Data consistency is hard to maintain</li>
          </ul>
        </ul>
        <li>Option 2. custom search solution</li>
        <ul>
          <li>Pros</li>
          <ul>
            <li>Easy to scale</li>
            <li>There is only one system and no need to worry about data consistency</li>
          </ul>
          <li>Cons</li>
          <ul>
            <li>Significant engineering needed</li>
          </ul>
        </ul>
      </ul>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: System Design Interview Volume 2, Alex Xu & Sahn Lam
  </div>
</div>

<div class="card mb-4" id="system-design-2-9">
  <div class="card-body">
    <h2 class="card-title">S3-like Object Storage</h2>

    <h3 class="card-title">Understand the Problem and Establish Design Scope</h3>
    <ul>
      <li>What is typical data size? massive objects (GBs) and large number of small objects (KBs)</li>
      <li>How much data should the system store per year? 100PB</li>
    </ul>

    <h3 class="card-title">Bandwidth</h3>
    <ul>
      <li>Assume 20% of objects are small (0.5MB)</li>
      <li>Assume 60% of objects are medium (32MB)</li>
      <li>Assume 20% of objects are large (200MB)</li>
      <li>Assume we utilize 40% of 100PB</li>
      <li>\( \dfrac{10\text{PB} \times 0.4}{0.2 \times 0.5\text{MB} + 0.6 \times 32\text{MB} + 0.2 \times 200\text{MB}} = 0.68B \) objects</li>
    </ul>

    <h3 class="card-title">Propose High-level Diagram and Get Buy-in</h3>
    <ul>
      <img class="img-fluid" class="card-img-top" src="/system-design/image/sd-c/s3-like-object-storage-1.png" alt="Card image cap">
      <ul>
        <li>IAM</li>
        <ul>
          <li>Authentication and authorization</li>
        </ul>
        <li>Data store</li>
        <ul>
          <li>Stores actual data</li>
        </ul>
      </ul>
      <li>Workflow (uploading an object)</li>
      <ul>
        <li>Client sends HTTP PUT request to API service to create a bucket</li>
        <li>API service calls IAM to ensure user has write permission</li>
        <li>API service calls metadata store to create an entry</li>
        <li>Client sends HTTP PUT request to API service to create an object</li>
        <li>API service calls IAM to ensure user has write permission</li>
        <li>API service calls data store ti persist the payload as object</li>
        <li>API service calls metadata store to create an entry</li>
      </ul>
      <li>Workflow (downloading an object)</li>
      <ul>
        <li>Client sends HTTP GET request to API service</li>
        <li>API service calls IAM to ensure user has read permission</li>
        <li>API service fetches corresponding object's UUID from metadata store</li>
        <li>API service fetches object data from data store by UUID</li>
        <li>API service returns object data to client</li>
      </ul>
      <li>Storage schema</li>
      <ul>
        <li>Bucket</li>
        <ul>
          <li>bucket_name</li>
          <li>bucket_id</li>
          <li>owner_id</li>
          <li>enable_versioning</li>
        </ul>
        <li>Object</li>
        <ul>
          <li>bucket_name</li>
          <li>object_name</li>
          <li>object_version</li>
          <li>object_id</li>
        </ul>
      </ul>
    </ul>

    <h3 class="card-title">Design Deep Dive</h3>
    <ul>
      <li>Data store</li>
      <img class="img-fluid" class="card-img-top" src="/system-design/image/sd-c/s3-like-object-storage-2.png" alt="Card image cap">
      <ul>
        <li>Data routing service</li>
        <ul>
          <li>Query the placement service to get the best data node to store data</li>
          <li>Read data from data nodes and return it to API service</li>
          <li>Write data to data nodes</li>
        </ul>
        <li>Placement service</li>
        <ul>
          <li>Monitors all data nodes via heartbeats</li>
        </ul>
        <li>Data nodes</li>
        <ul>
          <li>Stores actual data object</li>
          <li>Data service daemon sends heartbeats to placement service</li>
        </ul>
      </ul>
      <li>Workflow</li>
      <ul>
        <li>API service forwards object data to data store</li>
        <li>Data routing service generates UUID for this object and queries the placement service for the data node to store this object</li>
        <li>Placement service checks the virtual cluster map and returns the primary node</li>
        <li>Data routing service sends data directly to primary data node</li>
        <li>Primary data node saves data locally and replicates it to two secondary data nodes</li>
        <li>Primary node responds to data routing service, then UUID of object is returned to API service</li>
      </ul>
      <li>Durability</li>
      <ul>
        <li>Replication</li>
        <ul>
          <li>6 nines (data is copied 3 times)</li>
          <li>200% storage overhead</li>
          <li>No computation</li>
          <li>No write operation needed</li>
          <li>Reads are served from the same replica</li>
        </ul>
        <li>Erasure coding</li>
        <ul>
          <li>11 nines (8+4 erasure coding)</li>
          <li>50% storage overhead</li>
          <li>Computation needed to calculate parities</li>
          <li>Write latency having to calculate parities before data is written to disk</li>
          <li>Every read must come from multiple nodes</li>
        </ul>
      </ul>
      <li>Scalability</li>
      <ul>
        <li>Size of bucket table will be small</li>
        <li>Size of object table will be large, thus sharding is needed</li>
        <ul>
          <li>Sharding by bucket_id can create hotspots</li>
          <li>Sharding by object_id makes queries that are based on object names inefficient</li>
          <li>Thus, shard by a combination of bucket_name and object_name (use the hash of &lt;bucket_name, object_name&gt;)</li>
        </ul>
      </ul>
      <li>Versioning</li>
      <ul>
        <li>A new record is inserted with the same bucket_id and object name but with a new object_id and object_version</li>
      </ul>
      <li>Uploading large files</li>
      <ul>
        <li>Workflow</li>
        <ul>
          <li>Client calls the object storage to initial a multipart upload</li>
          <li>Data store returns an upload_id</li>
          <li>Client splits the large files into smaller objects and start uploading them with upload_id</li>
          <li>When a part is uploaded, data store returns a ETag, which is md5 checksum of that part, which then is used to verify multipart upload</li>
          <li>After all parts are uploaded, client sends a complete multipart upload request, which includes upload_id, part numbers, ETags</li>
          <li>Data store reassembles the object from its parts pased on part number</li>
        </ul>
        <li>Garbage collection</li>
        <ul>
          <li>Old parts are no longer useful after the object has been reassembled from them</li>
        </ul>
      </ul>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: System Design Interview Volume 2, Alex Xu & Sahn Lam
  </div>
</div>

<div class="card mb-4" id="system-design-2-10">
  <div class="card-body">
    <h2 class="card-title">Real-time Gaming Leaderboard</h2>

    <h3 class="card-title">Understand the Problem and Establish Design Scope</h3>
    <ul>
      <li>How is score calculated? Users get points when they win matches</li>
      <li>Is everyone included in the leaderboard? Yes</li>
      <li>Is there a time segment associated with leaderboard? A new tornament kicks off each month</li>
      <li>How many users are there? 5M daily active users and 25M monthly active users</li>
      <li>How many matches are played on average? Each user plays 10 matches per day on average</li>
    </ul>

    <h3 class="card-title">Bandwidth</h3>
    <ul>
      <li>(5M / 86400) users per second</li>
      <li>Assume peak load is 5 times the above number</li>
      <li>Multiple above number by 10 to get query per second</li>
    </ul>

    <h3 class="card-title">Propose High-level Diagram and Get Buy-in</h3>
    <ul>
      <img class="img-fluid" class="card-img-top" src="/system-design/image/sd-c/real-time-gaming-leaderboard-1.png" alt="Card image cap">
      <li>When a player wins a game, user sends a request to game service</li>
      <li>Game service ensures the win is valid and calls leaderboard service to update the score</li>
      <li>Leaderboard service updates user's score in leaderboard store</li>
      <li>User calls leaderboard service directly to fetch leaderboard data</li>
      <li>API</li>
      <ul>
        <li>Update user's position on leaderboard when a user wins a game</li>
        <ul>
          <li><code>POST /v1/scores</code></li>
          <li>Request</li>
          <ul>
            <li><code>user_id</code> - user who wins the game</li>
            <li><code>points</code> - points that user gained by winning a game</li>
          </ul>
          <li>Response</li>
          <ul>
            <li><code>200 OK</code></li>
            <li><code>400 Bad Request</code></li>
          </ul>
        </ul>
        <li>Get top 10 plays from leaderboard</li>
        <ul>
          <li><code>GET /v1/scores</code></li>
        </ul>
        <li>Find the rank of a specific user</li>
        <ul>
          <li><code>GET /v1/scores/{:user_id}</code></li>
        </ul>
      </ul>
      <li>Storage schema</li>
      <ul>
        <li>Relational DB</li>
        <ul>
          <li>Not performant when processing large amount of continuously changing information</li>
          <li>Not suitable very high load of read queries</li>
        </ul>
        <li>Redis</li>
        <ul>
          <li>Sorted sets data type works well with leaderboard problem</li>
          <li>Redis supports persistence</li>
          <li>When main replica fails, read replica is promoted and a new read replica is attached</li>
          <li>ZADD - insert user if not exist O(logn)</li>
          <li>ZINCBY - increase the score of a user by specified amount O(logn)</li>
          <li>ZRANGE/ZREVRANGE - fetch a range of users sorted by the score O(logn+m)</li>
          <li>ZRANK/ZREVRANK - fetch the position of any user</li>
        </ul>
      </ul>
    </ul>

    <h3 class="card-title">Design Deep Dive</h3>
    <ul>
      <li>Serverless</li>
      <ul>
        <li>Allows auto-scaling as needed with DAU growth</li>
      </ul>
      <img class="img-fluid" class="card-img-top" src="/system-design/image/sd-c/real-time-gaming-leaderboard-2.png" alt="Card image cap">
      <img class="img-fluid" class="card-img-top" src="/system-design/image/sd-c/real-time-gaming-leaderboard-3.png" alt="Card image cap">
      <li>Data sharding</li>
      <ul>
        <li>Fixed partition</li>
        <ul>
          <li>Looks at overall range of scores and distribute data into different shards based on the score</li>
          <li>Adjust the score range in each shard to make sure that data is relatively evenly distributed between shards</li>
          <li>Return top 10 users from the shard with the highest scores</li>
        </ul>
        <li>Hash partition</li>
        <ul>
          <li>Returning top 10 users requires gathering data from different shards, which will be slow</li>
        </ul>
      </ul>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: System Design Interview Volume 2, Alex Xu & Sahn Lam
  </div>
</div>

<div class="card mb-4" id="system-design-2-11">
  <div class="card-body">
    <h2 class="card-title">Payment System</h2>

    <h3 class="card-title">Understand the Problem and Establish Design Scope</h3>
    <ul>
      <li>What kind of payment system? E-commerce</li>
      <li>What payment options should the system support? Credit card, debit card, PayPal</li>
      <li>Should the system handle credit card processing? No, the system can use Stripe, Square, etc</li>
      <li>Should the system store credit card data in DB? No, the system should rely on third party</li>
      <li>Should the system support multiple currencies? No</li>
      <li>How many transactions per day? 1M</li>
      <li>Should the system pay the sellers based on some cadence? Yes</li>
    </ul>

    <h3 class="card-title">Bandwidth</h3>
    <ul>
      <li>1M transactions per day = 10 transactions per second</li>
    </ul>

    <h3 class="card-title">Propose High-level Diagram and Get Buy-in</h3>
    <ul>
      <img class="img-fluid" class="card-img-top" src="/system-design/image/sd-c/payment-system-1.png" alt="Card image cap">
      <li>Payment service</li>
      <ul>
        <li>Accepts payment events from users</li>
        <li>Performs risk checks using third party provider</li>
      </ul>
      <li>Payment executor</li>
      <ul>
        <li>Execute a single payment order using payment service provider (PSP)</li>
        <li>A payment may contain several payment orders</li>
      </ul>
      <li>Payment service provider</li>
      <ul>
        <li>Ex. Stripe, Square</li>
      </ul>
      <li>Card schemes</li>
      <ul>
        <li>Ex. Visa, MasterCard</li>
      </ul>
      <li>Ledger</li>
      <ul>
        <li>Keeps transaction information for accouting and post-payment analysis</li>
      </ul>
      <li>Wallet</li>
      <ul>
        <li>Keeps account balance of the merchants</li>
      </ul>
      <li>Workflow</li>
      <ul>
        <li>When user clicks order button, payment event is generated and sent to payment service</li>
        <li>Payment service stores payment event in DB</li>
        <li>Payment service calls payment executor for each payment order</li>
        <li>Payment executor stores payment order in DB</li>
        <li>Payment executor calls PSP to process credit card payment</li>
        <li>Payment service updates wallet</li>
        <li>Wallet stores updated balance in DB</li>
        <li>Payment service updates ledger</li>
        <li>Ledgers stores new transaction information in DB</li>
      </ul>
      <li>API</li>
      <ul>
        <li>Executes a payment event</li>
        <ul>
          <li><code>POST /v1/payments</code></li>
          <ul>
            <li><code>buyer_info</code> - information of buyer (JSON)</li>
            <li><code>checkout_id</code> - globally unique ID of this checkout (string)</li>
            <li><code>credit_card_info</code> - encrypted credit card information, which is PSP specific (JSON)</li>
            <li><code>payment_orders</code> - list of payment orders (list)</li>
            <ul>
              <li><code>seller_account</code> - which seller will receive the monry (string)</li>
              <li><code>amount</code> - transaction amount of the order (string)</li>
              <ul>
                <li>Double is not a good choice because the number could be extremely big or small</li>
              </ul>
              <li><code>currency</code> - currency of the order (string)</li>
              <li><code>payment_order_id</code> - globally unique ID of this payment (string)</li>
              <ul>
                <li>This ID is used by PSP as idempotency key, thus must be globally unique</li>
              </ul>
            </ul>
          </ul>
        </ul>
        <li>Returns the execution status of a single payment order</li>
        <ul>
          <li><code>GET /v1/payments/{:id}</code></li>
        </ul>
      </ul>
      <li>Storage schema</li>
      <ul>
        <li>Performance is not the most important factor for payment service</li>
        <li>Choose relational DB for ACID property</li>
        <li>Payment event</li>
        <ul>
          <li>checkout_id (string, pk)</li>
          <li>buyer_info (string)</li>
          <li>seller_info (string)</li>
          <li>credit_card_info (depends on PSP)</li>
          <li>is_payment_done (bool)</li>
        </ul>
        <li>Payment order</li>
        <ul>
          <li>payment_order_id (string, pk)</li>
          <li>buyer_account (string)</li>
          <li>amount (string)</li>
          <li>currency (string)</li>
          <li>checkout_id (string, FK)</li>
          <li>payment_order_status (string)</li>
          <li>ledger_updated (bool)</li>
          <li>wallet_updated (bool)</li>
        </ul>
      </ul>
    </ul>

    <h3 class="card-title">Design Deep Dive</h3>
    <ul>
      <li>PSP integration</li>
      <ul>
        <li>Choose not to store sensitive payment information due to complex regulations</li>
        <img class="img-fluid" class="card-img-top" src="/system-design/image/sd-c/payment-system-2.png" alt="Card image cap">
        <ul>
          <li>User clicks checkout button and client calls payment service with payment order information</li>
          <li>Payment service calls PSP with amount, currency, expiration date, redirect URL</li>
          <li>PSP returns a token (UUID) to payment service</li>
          <li>Payment service stores token in DB</li>
          <li>Client displays PSP-hosted payment page</li>
          <li>User fills PSP-hosted payment page and PSP starts payment processing</li>
          <li>PSP returns payment status</li>
          <li>Web page is directed to the redirect URL</li>
          <li>PSP calls payment service with payment status</li>
        </ul>
      </ul>
      <li>Reconciliation</li>
      <ul>
        <li>Every night, PSP or bank sends a settlement file to clients</li>
        <li>Reconciliation system compares settlement file with ledger</li>
        <li>Finance team can manually adjust mismatches</li>
      </ul>
      <li>Communication among internal services</li>
      <ul>
        <li>Message queue - a message can only be consumed by one subscriber</li>
        <li>Kafka - a message can be consumed by multiple subscribers</li>
        <li>Payment events are published to Kafka and consumed by payment system, analytics, billing</li>
      </ul>
      <li>Handling failed payments</li>
      <ul>
        <li>Retry queue - transient errors are routed here</li>
        <li>Dead letter queue - when messages fail repeatedly, they end up here</li>
      </ul>
      <li>Exactly-once delivery</li>
      <ul>
        <li>Use both at-least-once and at-most-once</li>
        <li>At-least-once - retrying failed payment guarantees this</li>
        <li>At-most-once - idempotency check using UUID</li>
        <ul>
          <li>Subsequent requests with the UUID that is already seen will result in 429 too many request</li>
        </ul>
      </ul>
      <li>Security</li>
      <ul>
        <li>Request/response eavesdropping - use HTTPS</li>
        <li>Data tampering - use encryption</li>
        <li>Man-in-the-middle-attack - use SSL</li>
        <li>Data loss - DB replication across multiple zones/regions</li>
        <li>DDoS - rate limiting and firewall</li>
        <li>Card theft - tokenization of credit card number</li>
      </ul>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: System Design Interview Volume 2, Alex Xu & Sahn Lam
  </div>
</div>

<div class="card mb-4" id="system-design-2-12">
  <div class="card-body">
    <h2 class="card-title">Digital Wallet</h2>

    <h3 class="card-title">Understand the Problem and Establish Design Scope</h3>
    <ul>
      <li>What is the purpose of the system? Balance transfer between two digital wallets</li>
      <li>How many transactions should the system support? 1M per second</li>
      <li>Should the system handle foreign currency exchange rate? No</li>
    </ul>

    <h3 class="card-title">Propose High-level Diagram and Get Buy-in</h3>
    <ul>
      <li>In-memory sharding</li>
      <ul>
        <li>A cluster of Redis nodes to support 1M transactions per second</li>
        <li>Distribute user accounts across the cluster</li>
        <li>Store &lt;user,balance&gt; relationship in a map</li>
        <li>Use ZooKeeper to store the number of partitions and addresses of all Redis nodes</li>
        <li>Problem - when wallet service updates two Redis nodes for each transaction, there is no guarantee that both update would succeed</li>
      </ul>
      <li>Two-phase commit</li>
      <ul>
        <li>In the first phase, wallet service asks both databases to be locked</li>
        <li>In the second phase, wallet service collects replies from databases</li>
        <ul>
          <li>If reply is yes, wallet service asks databases to commit transaction</li>
          <li>If reply is no, wallet service asks databases to abort transaction</li>
        </ul>
        <li>Problem - not performant because locks can be held for long time waiting for replies from other nodes</li>
      </ul>
      <li>Try-Confirm/Cancel</li>
      <ul>
        <li>Ex. in the first phase, -1 for account A and do nothing for account B (Try)</li>
        <li>Ex. in the second phase</li>
        <ul>
          <li>If reply is yes, do nothing for account A and +1 for account B (Confirm)</li>
          <li>If reply is no, +1 for account A and do nothing for account B (Cancel)</li>
        </ul>
        <li>Transactional DB can store status in case wallet service gets interrupted during transaction</li>
        <ul>
          <li>ID and content of distributed transaction</li>
          <li>Status of Try phase for each DB (not sent yet, has been sent, response received)</li>
          <li>Name of second phase</li>
          <li>Status of second phase</li>
          <li>Out-of-order flag</li>
          <ul>
            <li>Seen a Cancel but not a Try yet</li>
            <li>Try returns failure if there is out-of-order flag</li>
          </ul>
        </ul>
        <li>Problem - business logics need to be managed at application layer</li>
      </ul>
      <li>Saga</li>
      <ul>
        <li>All operations are ordered in a sequence</li>
        <li>Each operation is independent transaction in its own DB</li>
        <li>Operations are executed from first to last</li>
        <li>When an operation fails, the entire process rolls back</li>
        <li>Problem - not suitable for latency-sensitive system</li>
      </ul>
      <li>API</li>
      <ul>
        <li>Transfer balance from one wallet to another</li>
        <ul>
          <li><code>POST /v1/wallet/balance_transfer</code></li>
          <ul>
            <li><code>from_account</code> - debit account (string)</li>
            <li><code>to_account</code> - credit account (string)</li>
            <li><code>amount</code> - money (string)</li>
            <li><code>currency</code> - currency type (string)</li>
            <li><code>transaction_id</code> - ID used for deduplication (UUID)</li>
          </ul>
        </ul>
      </ul>
    </ul>

    <h3 class="card-title">Design Deep Dive</h3>
    <ul>
      <li>Event sourcing</li>
      <ul>
        <li>Command</li>
        <ul>
          <li>Ex. transfer 1 from account A to B</li>
          <li>Commands are put into a queue (Ex. Kafka)</li>
        </ul>
        <li>Event</li>
        <ul>
          <li>One command may generate any number of events</li>
          <li>Events are put into a queue</li>
        </ul>
        <li>State</li>
        <ul>
          <li>Ex. balance of accounts</li>
        </ul>
        <li>State machine</li>
        <ul>
          <li>Ex. if command is "transfer 1 from A to B", the events are "-1 to A" and "+1 to B"</li>
          <li>Read commands from the command queue</li>
          <li>Read balance state from DB</li>
          <li>Validate the command. If valid, generate two events</li>
          <li>Read the next event</li>
          <li>Apply the event by updating the balance in DB</li>
        </ul>
      </ul>
      <li>Questions</li>
      <ul>
        <li>Do we know account balance at any given time? We can reply events from the start up to a point</li>
        <li>How do we know the historical and current account balance are correct? We can recalculate from event list</li>
        <li>How do we prove the system logic is correct after a code change? We can run different versions of code against the events and check that results are the same</li>
      </ul>
      <li>Command-query responsibility segregation (CORS)</li>
      <ul>
        <li>One write state machine</li>
        <li>Many read-only state machine</li>
        <li>How do we prove the system logic is correct after a code change? We can run different versions of code against the events and check that results are the same</li>
      </ul>
      <li>Optimization</li>
      <ul>
        <li>Save commands and events to a local disk rather than Kafka</li>
        <li>Cache recent commands and events in memory</li>
        <li>Use RocksDB to store the state (balance information) and cache recent data</li>
        <li>Periodically stop the state machine and save the current state into a snapshot for faster reproducibility. Store snapshots into HDFS</li>
      </ul>
      <li>Reliability</li>
      <ul>
        <li>State and snapshot can be reproduced from events</li>
        <li>Commands may generate different events, but events are immutable. Thus, we need reliability for events</li>
        <ul>
          <li>Raft algorithm guarantees that, as long as there are more than half of the nodes online, append-only lists have the same data</li>
        </ul>
      </ul>
      <li>Workflow (distributed event sourcing)</li>
      <ul>
        <li>User A sends a transaction to Saga coordinator (two operations, "-1 to A" and "+1 to B")</li>
        <li>Saga coordinator creates a record in phase status table</li>
        <li>Saga coordinator sends "-1 to A" as a command to partition 1</li>
        <li>Partition 1's Raft leader recevies "-1 to A" command and stores it in command list</li>
        <li>Partition 1's Raft leader validates the command, then converts it to an event</li>
        <li>Raft algorithm synchronizes data across different nodes, then the event is executed</li>
        <li>Event sourcing framework of partition 1 synchronizes the data to the read path using CORS</li>
        <li>The read path of partition 1 pushes status back to Saga coordinator</li>
        <li>Saga coordinator receives the success status from partition 1</li>
        <li>Saga coordinator creates a record in phase status table</li>
        <li>Saga coordinator sends "+1 to B" as a command to partition 2</li>
        <li>Partition 2's Raft leader recevies "+1 to B" command and stores it in command list</li>
        <li>Partition 2's Raft leader validates the command, then converts it to an event</li>
        <li>Raft algorithm synchronizes data across different nodes, then the event is executed</li>
        <li>Event sourcing framework of partition 2 synchronizes the data to the read path using CORS</li>
        <li>The read path of partition 2 pushes status back to Saga coordinator</li>
        <li>Saga coordinator receives the success status from partition 2</li>
        <li>Saga coordinator creates a record in phase status table</li>
        <li>Sage coordinator responds to its caller with the result</li>
      </ul>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: System Design Interview Volume 2, Alex Xu & Sahn Lam
  </div>
</div>

<div class="card mb-4" id="system-design-2-13">
  <div class="card-body">
    <h2 class="card-title">Stock Exchange</h2>

    <h3 class="card-title">Understand the Problem and Establish Design Scope</h3>
    <ul>
      <li>What kind of goods are we trading? stocks only</li>
      <li>What kind of operations should the system support? placing an order, canceling an order. Only limit order is in scope</li>
      <li>What are the basic functions of stock exchange? Clients place new orders or cancel them, receive matched trades in real-time, view real-time order book</li>
      <li>What are the scale of the system? 10K users trading at the same time, 100 symbols, 1M orders per day</li>
      <li>What are some regulartions that the system need to concern? Users can trade maximum of 1M shares of particular stock</li>
    </ul>

    <h3 class="card-title">Bandwidth</h3>
    <ul>
      <li>1B orders per day</li>
      <li>Assume stock exchange opens 8 hours per day</li>
      <li>Assume peak is 10 times the average</li>
      <li>Incoming - 1B / (8 * 3600) = 35K</li>
      <li>Peak incoming - 350K</li>
    </ul>

    <h3 class="card-title">Propose High-level Diagram and Get Buy-in</h3>
    <ul>
      <img class="img-fluid" class="card-img-top" src="/system-design/image/sd-c/stock-exchange-1.png" alt="Card image cap">
      <ul>
        <li>Matching engine</li>
        <ul>
          <li>Maintain order book for each symbol</li>
          <li>Match buy and sell orders</li>
          <li>Distribute the execution stream as market data</li>
        </ul>
        <li>Sequencer</li>
        <ul>
          <li>Stamps every incoming order and pair of executions completed by matching engine with a sequence ID</li>
          <li>Stamping is for fast recovery, exactly-once guarantee</li>
          <li>Serves as a message queue as well</li>
        </ul>
        <li>Order manager</li>
        <ul>
          <li>Performs risk checks</li>
          <li>Verifies that wallet has enough funds</li>
          <li>Sends the order to sequencer</li>
        </ul>
        <li>Client gateway</li>
        <ul>
          <li>Should be lightweight and complicated functions should be done by risk check adn matching engine</li>
      </ul>
      <li>Workflow</li>
      <ul>
        <li>Trading flow</li>
        <ul>
          <li>Client places a new order</li>
          <li>Broker sends the order to the exchange</li>
          <li>Client gateway performs input validation, rate limiting, authentication, etc</li>
          <li>Order manager performs risk checks and verifies that there are sufficient funds in the wallet</li>
          <li>Matching engine executes two transactions (buy and sell)</li>
        </ul>
        <li>Market data flow</li>
        <ul>
          <li>Matching engine generates a stream of executions as matches are made</li>
          <li>Market data publisher constructs candlestick charts and order books</li>
          <li>Market data is saved to specialized storage for real-time analysis</li>
          <li>Brokers connect to data service to obtain timely market data</li>
        </ul>
        <li>Reporting flow</li>
        <ul>
          <li>Report collects information from orders and executions, then write them to DB</li>
        </ul>
      </ul>
      <li>API</li>
      <ul>
        <li>Place an order</li>
        <ul>
          <li><code>POST /v1/order</code></li>
          <ul>
            <li><code>symbol</code> - stock symbol (string)</li>
            <li><code>side</code> - buy or sell (string)</li>
            <li><code>price</code> - price of limit order (long)</li>
            <li><code>orderType</code> - limit or market (string)</li>
            <li><code>quantity</code> - quanitiy of order (long)</li>
          </ul>
        </ul>
        <li>Get execution info</li>
        <ul>
          <li><code>GET /v1/execution?symbol={:symbol}&orderId={:orderId}&startTime={:startTime}&endTime/{:endTime}</code></li>
          <ul>
            <li><code>symbol</code> - stock symbol (string)</li>
            <li><code>orderId</code> - ID of order (string)</li>
            <li><code>startTime</code> - query start time in epoch (long)</li>
            <li><code>endTime</code> - query end time in epoch (long)</li>
          </ul>
        </ul>
        <li>Get order book info</li>
        <ul>
          <li><code>GET /v1/marketdata/orderBook/L2?symbol={:symbol}&depth={:depth}</code></li>
          <ul>
            <li><code>symbol</code> - stock symbol (string)</li>
            <li><code>depth</code> - order book depth per side (int)</li>
            <li><code>startTime</code> - query start time in epoch (long)</li>
            <li><code>endTime</code> - query end time in epoch (long)</li>
          </ul>
        </ul>
        <li>Get candlestick chart data</li>
        <ul>
          <li><code>GET /v1/marketdata/candles?symbol={:symbol}&resolution={:resolution}&startTime={:startTime}&endTime/{:endTime}</code></li>
          <ul>
            <li><code>symbol</code> - stock symbol (string)</li>
            <li><code>resolution</code> - window length of candlestick chart in seconds (long)</li>
            <li><code>startTime</code> - query start time in epoch (long)</li>
            <li><code>endTime</code> - query end time in epoch (long)</li>
          </ul>
        </ul>
      </ul>
      <li>Storage schema</li>
      <ul>
        <li>Order</li>
        <ul>
          <li>order_id (uuid)</li>
          <li>product_id (int)</li>
          <li>price (long)</li>
          <li>quantity (long)</li>
          <li>side (Side)</li>
          <li>order_status (OrderStatus)</li>
          <li>order_type (OrderType)</li>
        </ul>
        <li>Execution</li>
        <ul>
          <li>execution_id (uuid)</li>
          <li>order_id (uuid)</li>
          <li>price (long)</li>
          <li>quantity (long)</li>
          <li>side (Side)</li>
          <li>order_status (OrderStatus)</li>
          <li>order_type (OrderType)</li>
        </ul>
        <li>Product</li>
        <ul>
          <li>product_id (int)</li>
          <li>symbol (long)</li>
          <li>description (string)</li>
        </ul>
      </ul>
    </ul>

    <h3 class="card-title">Design Deep Dive</h3>
    <ul>
      <li>Performance</li>
      <ul>
        <img class="img-fluid" class="card-img-top" src="/system-design/image/sd-c/stock-exchange-1.png" alt="Card image cap">
        <li>Put everything on the same server to eliminate network hops</li>
        <li>Components, in this case processes on the server, communicate via mmap</li>
        <li>Application loop polls for tasks to execute in a while loop</li>
        <li>Each application loop is single-threaded, and the thread is pinned to a fixed CPU core</li>
        <ul>
          <li>No context switch because CPU is fully allocated to application loop</li>
          <li>No locks, thus no lock contention</li>
        </ul>
        <li>mmap communication has no network or disk access</li>
      </ul>
      <li>Availability</li>
      <ul>
        <li>Client gateway is stateless, thus can easily be horizontally scaled</li>
        <li>Order manager and matching engine are stateful, thus state data must be copied across replicas</li>
        <ul>
          <li>If primary goes down, how to decide which backup instance to fail over?</li>
          <ul>
            <li>Ex. Raft cluster with \( 5 \) servers</li>
            <li>Current leader sends data to all other instances (followers)</li>
            <li>Minimum number of votes required to perform operation \( \frac{n}{2}+1 = 3 \)</li>
          </ul>
          <li>How to choose leader among backup instances?</li>
          <ul>
            <li>Leader sends heartbeats to followers</li>
            <li>If followers don't receive heartbeat for some time, it triggers an election timeout</li>
            <li>First follower that reaches election timeout become a candidate</li>
            <li>If first follower receives a majority of votes, it becomes a new leader</li>
          </ul>
          <li>What is recovery time objective?</li>
          <ul>
            <li>Categorize services based on priority and define a degradation strategy to maintain a minimum service level</li>
          </ul>
          <li>What is recovery point objective?</li>
          <ul>
            <li>With Raft, we have many copies of data</li>
            <li>If current leader crashes, new leader can function immediately</li>
          </ul>
        </ul>
      </ul>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: System Design Interview Volume 2, Alex Xu & Sahn Lam
  </div>
</div>
<!-- System Design 2 END -->

</div> <!-- /.col-md-12 -->
</div> <!-- /.row -->
</div> <!-- /.container -->

<include src="/footer.html"></include>

</body>

</html>