<!DOCTYPE html>

<html lang="en">

<head>

<!-- Metadata -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="">
<meta name="author" content="Seungmoon Rieh">
<meta name="keywords" content="">

<!-- Title and image -->
<title>Seungmoon Rieh</title>
<link href="/img/seungmoonrieh.jpg" rel="icon">

<!-- CSS -->
<link href="/css/bootstrap.min.css" rel="stylesheet">
<link href="/css/monokai-sublime.css" rel="stylesheet">
<link href="/css/site.css" rel="stylesheet">

<!-- JavaScript -->
<script src="/js/jquery.min.js" type="text/javascript"></script>
<script src="/js/bootstrap.bundle.min.js" type="text/javascript"></script>
<script src="/js/highlight.pack.js" type="text/javascript"></script>
<script src="/js/include_html.js" type="text/javascript"></script>
<script src="/js/mathjax/tex-chtml.js" type="text/javascript"></script>
<script src="/js/site.js" type="text/javascript"></script>

</head>

<body>

<include src="/header.html"></include>

<div class="container">
<div class="row">
<div class="col-md-12">
<h1 class="my-4">Software Engineering</h1>

<!-- System Design 1 BEGIN -->
<div class="card mb-4" id="system-design-1">
  <div class="card-body">
    <h2 class="card-title">System Design 1</h2>
    <ul class="list-unstyled mb-0">
      <li><a href="#system-design-1-1">Scale from Zero to Millions of Users</a></li>
      <li><a href="#system-design-1-2">Back-of-the-Envelop Estimation</a></li>
      <li><a href="#system-design-1-3">Rate Limiter</a></li>
      <li><a href="#system-design-1-4">Consistent Hashing</a></li>
      <li><a href="#system-design-1-5">Key-Value Store</a></li>
      <li><a href="#system-design-1-6">UUID Generator</a></li>
      <li><a href="#system-design-1-7">URL Shortener</a></li>
      <li><a href="#system-design-1-8">Web Crawler</a></li>
      <li><a href="#system-design-1-9">Notification System</a></li>
      <li><a href="#system-design-1-10">Newsfeed System</a></li>
      <li><a href="#system-design-1-11">Chat System</a></li>
      <li><a href="#system-design-1-12">Search Autocomplete System</a></li>
      <li><a href="#system-design-1-13">Youtube</a></li>
      <li><a href="#system-design-1-14">Google Drive</a></li>
    </ul>
  </div>
</div>

<div class="card mb-4" id="system-design-1-1">
  <div class="card-body">
    <h2 class="card-title">Scale from Zero to Millions of Users</h2>
    <ul>
      <li><img class="img-fluid" class="card-img-top" src="/system-design/image/sd-b/building-blocks-1.png" alt="Card image cap"></li>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: System Design Interview, Alex Xu
  </div>
</div>

<div class="card mb-4" id="system-design-1-2">
  <div class="card-body">
    <h2 class="card-title">Back-of-the-Envelop Estimation</h2>

    <h3 class="card-title">Important latencies (in nanoseconds)</h3>
    <table>
      <tr>
        <td>L1 cache reference</td>
        <td>0.9</td>
      </tr>
      <tr>
        <td>L2 cache reference</td>
        <td>2.8</td>
      </tr>
      <tr>
        <td>L3 cache reference</td>
        <td>12.9</td>
      </tr>
      <tr>
        <td>Main memory reference</td>
        <td>100</td>
      </tr>
      <tr>
        <td>Compress 1KB with Snzip</td>
        <td>3,000 (3 microseconds)</td>
      </tr>
      <tr>
        <td>Read 1 MB sequentially from memory</td>
        <td>9,000 (9 microseconds)</td>
      </tr>
      <tr>
        <td>Read 1 MB sequentially from SSD</td>
        <td>200,000 (200 microseconds)</td>
      </tr>
      <tr>
        <td>Round trip within same datacenter</td>
        <td>500,000 (500 microseconds)</td>
      </tr>
      <tr>
        <td>Read 1 MB sequentially from SSD with speed ~1GB/sec SSD</td>
        <td>1,000,000 (1 milliseconds)</td>
      </tr>
      <tr>
        <td>Disk seek</td>
        <td>4,000,000 (4 milliseconds)</td>
      </tr>
      <tr>
        <td>Read 1 MB sequentially from disk</td>
        <td>2,000,000 (2 milliseconds)</td>
      </tr>
      <tr>
        <td>Send packet SF->NYC</td>
        <td>71,000,000 (71 milliseconds)</td>
      </tr>
    </table>

    <h3 class="card-title">Important rates</h3>
    <table>
      <tr>
        <td>QPS handled by MySQL</td>
        <td>1000</td>
      </tr>
      <tr>
        <td>QPS handled by key-value store</td>
        <td>10,000</td>
      </tr>
      <tr>
        <td>QPS handled by cache server</td>
        <td>100,000â€“1 M</td>
      </tr>
    </table>

    <h3 class="card-title">Availability numbers</h3>
    <table>
      <tr>
        <th>Availability</th>
        <th>Downtime per day</th>
        <th>Downtime per year</th>
      </tr>
      <tr>
        <td>99%</td>
        <td>14.40 minutes</td>
        <td>3.65 days</td>
      </tr>
      <tr>
        <td>99.9%</td>
        <td>1.44 minutes</td>
        <td>8.77 hours</td>
      </tr>
      <tr>
        <td>99.99%</td>
        <td>8.64 seconds</td>
        <td>52.6 minutes</td>
      </tr>
      <tr>
        <td>99.99%</td>
        <td>0.864 seconds</td>
        <td>5.26 minutes</td>
      </tr>
      <tr>
        <td>99.99%</td>
        <td>0.0864 seconds</td>
        <td>31.56 seconds</td>
      </tr>
    </table>
  </div>
  <div class="card-footer text-muted">
    Reference: System Design Interview, Alex Xu
  </div>
</div>

<div class="card mb-4" id="system-design-1-3">
  <div class="card-body">
    <h2 class="card-title">Rate Limiter</h2>

    <h3 class="card-title">Understand the problem and establish design scope</h3>
    <ul>
      <li>What kind of rate limiterm, client-side or server-side rate limiter? server side</li>
      <li>Based on what (like user_id, IP, etc) does the system trottle the requests? It should be flexible</li>
      <li>What is scale of system? Large number of requests</li>
      <li>Should the system work in distributed environment? Yes</li>
      <li>Should rate limiter be a separate service or implemented in application code? Up to design</li>
      <li>Do we need to inform users when their requests are throttled? Yes</li>
    </ul>

    <img class="img-fluid" class="card-img-top" src="/system-design/image/sd-b/rate-limiter-1.png" alt="Card image cap">

    <ul>
      <li>Where to place rate limiter</li>
      <ul>
        <li>Client - not safe and configuration can be difficult</li>
        <li>Server - a viable option</li>
        <li>Middleware - between client and server</li>
        <ul>
          <li>For example, API gateway performs rate limiting, SSL termination, authentication, IP whitelisting, service static content, etc</li>
        </ul>
      </ul>
      <li>Redis</li>
      <ul>
        <li>Cache is chosen because DB is too slow</li>
        <li>INCR - increments stored counter by 1</li>
        <li>EXPIRE - sets a timeout for the counter. If timeout expires, counter is deleted</li>
      </ul>
      <li>Throttling</li>
      <ul>
        <li>Hard - when exceed 500 (for example) the request is discarded</li>
        <li>Soft - allow 5% beyong the limit (for example) such as 525</li>
        <li>Dynamic - allow additional requests as long as there is free resource</li>
      </ul>
    </ul>

    <ul>
      <li>Option 1. token bucket algorithm</li>
      <ul>
        <li>Tokens are added to bucket periodically at pre-defined rate</li>
        <li>If bucket is full, extra tokens will overflow</li>
        <li>Each request consumes one token</li>
        <li>If not enough tokens, request is dropped</li>
        <li>Different buckets for differet API endpoints are needed</li>
        <li>If throttle based on IP addresses, each IP needs a bucket</li>
        <li>Global bucket may be needed, which is shared by all requests, if system allows only certain number of requests per second</li>
        <li>Pros</li>
        <ul>
          <li>Memory efficient</li>
          <li>Can handle burst of traffic</li>
        </ul>
        <li>Cons</li>
        <ul>
          <li>Two parameters (bucket size and token refill rate) are hard to tune</li>
        </ul>
        <li>Ex. Amazon, Stripe</li>
      </ul>
      <li>Option 2. leaking bucket algorithm</li>
      <ul>
        <li>Similar to token bucket algorithm, but uses queues instead</li>
        <li>Pre-defined rate of how many request can be processed at a time</li>
        <li>Pros</li>
        <ul>
          <li>Memory efficient</li>
          <li>Stable outflow</li>
        </ul>
        <li>Cons</li>
        <ul>
          <li>Burst of traffic will cause later requests to be rate limited</li>
          <li>Two parameters (queue size and outflow rate) are hard to tune</li>
        </ul>
        <li>Ex. Shopify</li>
      </ul>
      <li>Option 3. fixed window counter algorithm</li>
      <ul>
        <li>Each time window get a counter</li>
        <li>Each requests increments counter by one</li>
        <li>When reaching threshold, new requests are dropped until new time window starts</li>
        <li>Pros</li>
        <ul>
          <li>Memory efficient</li>
        </ul>
        <li>Cons</li>
        <ul>
          <li>Burst of traffic at the edge of window can cause more requests than allowed to go through</li>
        </ul>
      </ul>
      <li>Option 4. sliding window log algorithm</li>
      <ul>
        <li>Request timestamps are kept in a cache like Redis and added to a log</li>
        <li>When a request comes in, remove all timestamps older than the start of current time window</li>
        <li>If log size is equal or smaller than allowed acount, request is accpeted. Otherwise, it is dropped</li>
        <li>Pros</li>
        <ul>
          <li>Accurate</li>
        </ul>
        <li>Cons</li>
        <ul>
          <li>Memory inefficient because timestamps of rejected request may still be in memory</li>
        </ul>
      </ul>
      <li>Option 5. sliding window counter algorithm</li>
      <ul>
        <li>Mixes previous two approches</li>
        <li>Request in current window + requests in previous window * overlap percentage of rolling window and previous window</li>
        <li>Pros</li>
        <ul>
          <li>Memory efficient</li>
          <li>Smooths out spikes in traffic because rate is based on the average rate of previous window</li>
        </ul>
        <li>Cons</li>
        <ul>
          <li>Assumes that requests in previous window are evenly distributed</li>
        </ul>
      </ul>
    </ul>

    <h3 class="card-title">Design deep dive</h3>

    <img class="img-fluid" class="card-img-top" src="/system-design/image/sd-b/rate-limiter-2.png" alt="Card image cap">
    <ul>
      <li>Workers frequently pull rules from disk and store them in cache</li>
      <li>Rate limiter fetches counter and last request timestamp from Redis</li>
      <li>Multiple rate limiters are connnected to the same Redis for the synchronization purpose</li>
    </ul>

    <h3 class="card-title">Wrap up</h3>
    <ul>
      <li>Explain hard and soft rate limiting</li>
      <li>Explain rate limiting at layers other than the application layer </li>
      <li>Explain how to help client do best practices to avoid being rate limited</li>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: System Design Interview, Alex Xu
  </div>
</div>

<div class="card mb-4" id="system-design-1-4">
  <div class="card-body">
    <h2 class="card-title">Consistent Hashing</h2>

    <ul>
      <li>Basic method</li>
      <ul>
        <li>Server index = hash % N, where N = number of servers</li>
        <li>Client must contact server with particular index to fetch the key</li>
        <li>When adding/removing servers, most keys need to be redistributed</li>
      </ul>
      <li>Consistent hashing</li>
      <ul>
        <li>When adding/removing servers, k/n keys need to be redistributed on average where k = number of keys and n = number of slots</li>
        <li>To find the server where a key is stored, go clockwise from the position of the key until a server is found</li>
        <li>Each server may contain different number of keys</li>
      </ul>
      <li>Consistent hashing with virtual nodes</li>
      <ul>
        <li>To find the server where a key is stored, go clockwise from the position of the key until a virtual node is found</li>
        <li>As number of virtual nodes increases, the key distribution becomes more balanced</li>
      </ul>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: System Design Interview, Alex Xu
  </div>
</div>

<div class="card mb-4" id="system-design-1-5">
  <div class="card-body">
    <h2 class="card-title">Key-Value Store</h2>

    <h3 class="card-title">Components</h3>
    <ul>
      <li>Single server</li>
      <ul>
        <li>Use a hash table, which saves frequently accessd data in memory and the rest on disk</li>
        <li>Will reach its capacity quickly</li>
      </ul>
      <li>Distributed</li>
      <ul>
        <li>Consistency - when network partition occurs, block writes and return error to user until inconsistency is resolved</li>
        <li>Availability - when network partition occurs, return stale data. Data is synced when network partition is resolved</li>
      </ul>
    </ul>

    <h3 class="card-title">Data partition</h3>
    <ul>
      <li>Use consistent hasing to partition data into multiple servers</li>
    </ul>

    <h3 class="card-title">Data replication</h3>
    <ul>
      <li>Replicate data into multiple (N) servers for high availability and reliability</li>
      <li>From consistent hashing, first place a key in the ring</li>
      <li>Then, walk clock-wise and store data in the first N physical servers that are located in different data center</li>
      <ul>
        <li>Not the first N nodes or servers within the same data center</li>
      </ul>
    </ul>

    <h3 class="card-title">Consistency</h3>
    <ul>
      <li>N = number of replicas</li>
      <li>W = 1 means that coordinator must receive at least one acknowledgement from all other nodes to consider a write to be successful</li>
      <ul>
        <li>It does not mean data is written on one server</li>
      </ul>
      <li>W = 1 or R = 1 makes operations to return quickly</li>
      <li>W > 1 or R > 1 offers better consistency but operation is slower</li>
      <li>W + R > N guarantees strong consistency because there is at least one overlapping node that has the latest data</li>
      <ul>
        <li>Ex. N = 3, W = 2, R = 2</li>
      </ul>
      <li>W = N and R = 1 is optimized for fast read</li>
      <li>W = 1 and R = N is optimized for fast write</li>
    </ul>

    <h3 class="card-title">Versioning</h3>
    <ul>
      <li>Key-value store can adopt eventual consistency</li>
      <li>Vector clock, which is pairs of &lt;server Id, version number&gt;, associated with each data item can be used to resolve inconsistency between data replica</li>
      <li>Example</li>
      <ul>
        <li>A client writes data1, which is handled by server1 - D1[(s1, 1)]</li>
        <li>Another client reads data1 and updates it to data2, which is handled by server1 - D2[(s1, 2)]</li>
        <li>These two events happen at the same time</li>
        <ul>
          <li>Another client reads data2 and updates it to data3, which is handled by server2 - D3[(s1, 2), (s2, 1)]</li>
          <li>Another client reads data2 and updates it to data4, which is handled by server3 - D4[(s1, 2), (s3, 1)]</li>
        </ul>
        <li>When another client reads D3 and D4, it finds a conflict and resolve it, which is handled by server1 - D5[(s1, 3), (s2, 1), (s3, 1)]</li>
      </ul>
      <li>Set a threshold on the length of vector clock</li>
      <ul>
        <li>When it exceeds the threshold, remove oldest pairs</li>
      </ul>
    </ul>

    <h3 class="card-title">Failure</h3>
    <ul>
      <li>Failure detection - gossip protocol</li>
      <ul>
        <li>Each node maintains node membership list &lt;member_id, heartbeat_counter&gt;</li>
        <li>Each node periodically increments its heartbeat counter</li>
        <li>Each node periodically sends heartbeats to a set of random nodes, which in turn propagate to another set of nodes</li>
        <li>Once nodes receive heartbeats, membership list is updated to the latest</li>
        <li>If heartbeat has not been increased for more than pre-defined period, node is considered offline</li>
        <li>Example</li>
        <ul>
          <li>Node 0 maintains a node membership</li>
          <li>Node 0 notices that node 2's heartbeat counter has not increased for some time</li>
          <li>Node 0 sends hearbeats (which include node 2's info) to a set of random ndoes</li>
          <li>Other nodes confirm that node 2's heartbeat has not increased for some time</li>
          <li>Node 2 is marked as down and this info is propagated to all other nodes</li>
        </ul>
      </ul>
      <li>Temporary failure - sloppy quorum</li>
      <ul>
        <li>System chooses first W servers for writes and first R servers for reads on hash ring</li>
        <li>Offline servers are ignored</li>
        <li>When a server is unavailable, another server processes request temporarily. When server is back up, changes is pushed back to achieve data consistency</li>
      </ul>
      <li>Permanent failure</li>
      <ul>
        <li>Merkle tree</li>
        <ul>
          <li>Distribute keys into buckets in each server</li>
          <li>Hash keys in all bucketss in each server</li>
          <li>Create a single hash node in each bucket</li>
          <li>Build tree upwards using all buckets in each server</li>
        </ul>
        <li>Compare two Merkle trees</li>
        <ul>
          <li>If root hashes match, both servers have the same data</li>
          <li>If not, compare left, then right, then so on</li>
        </ul>
        <li>Only synchronize buckets where hashes are different</li>
        <li>Limit keys in each bucket to something like 1000</li>
      </ul>
    </ul>

    <h3 class="card-title">Architecture</h3>
    <ul>
      <li>Nodes are distributed on hash ring</li>
      <li>One of the nodes serves as a coordinator</li>
      <li>Client communicates with the coordinator</li>
    </ul>

    <h3 class="card-title">Write path</h3>

    <img class="img-fluid" class="card-img-top" src="/system-design/image/sd-b/key-value-store-1.png" alt="Card image cap">

    <ul>
      <li>Write request is saved on commit log file</li>
      <li>Data is saved in memory cache</li>
      <li>When memory cache is full, data is flushed to SSTable</li>
    </ul>

    <h3 class="card-title">Read path</h3>

    <img class="img-fluid" class="card-img-top" src="/system-design/image/sd-b/key-value-store-2.png" alt="Card image cap">

    <ul>
      <li>Check if data is in memory cache</li>
      <li>If not, check bloom filter to find SSTable that contains the key</li>
      <li>SSTable returns the data</li>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: System Design Interview, Alex Xu
  </div>
</div>

<div class="card mb-4" id="system-design-1-6">
  <div class="card-body">
    <h2 class="card-title">UUID Generator</h2>

    <h3 class="card-title">Understand the problem and establish design scope</h3>
    <ul>
      <li>How does Id look like? Must be unique and sortable</li>
      <li>Does Id contain only numbers? Yes</li>
      <li>Does Id need to increment by 1? Not necessarily</li>
      <li>How long can Id be? Should fit into 64 bits</li>
      <li>How many Ids does the system need? 10k per second</li>
    </ul>

    <h3 class="card-title">Propose high-level design and get buy-in</h3>
    <ul>
      <li>Multi-master replication</li>
      <ul>
        <li>Use DB's auto-increment feature</li>
        <li>Instead of increment by 1, increment by k, where k is number of DB servers in use</li>
        <ul>
          <li>Server 1 increment like 1, 3, 5, etc</li>
          <li>Server 2 increment like 2, 4, 6, etc</li>
        </ul>
        <li>Does not scale well when servers are added or removed</li>
      </ul>
      <li>UUID</li>
      <ul>
        <li>No coordication between servers needed, thus no synchronization issue</li>
        <li>UUID is 128 bits number, which is greater than 64 bits requirement</li>
        <li>UUID contains non-numeric characters</li>
      </ul>
      <li>Ticket server</li>
      <ul>
        <li>Single DB (ticket server) generates primary keys</li>
        <li>Ticket server is SPOF</li>
      </ul>
    </ul>

    <h3 class="card-title">Design deep dive</h3>
    <ul>
      <li>Divide an Id into different sections</li>
      <ul>
        <li>1 bit - 0</li>
        <li>41 bits - timestamp</li>
        <ul>
          <li>Miliseconds since the epoch or custom epoch</li>
        </ul>
        <li>5 bits - datacenter Id</li>
        <ul>
          <li>2^5 = 32 data centers</li>
          <li>Chosen at start up time and is fixed</li>
        </ul>
        <li>5 bits - machine Id</li>
        <ul>
          <li>2^5 = 32 machines per data center</li>
          <li>Chosen at start up time and is fixed</li>
        </ul>
        <li>12 bits - sequence number</li>
        <ul>
          <li>Every machine increments sequence number by 1 for each Id generated on that machine</li>
          <li>The number is reset to 0 every milisecond</li>
        </ul>
      </ul>
    </ul>

    <h3 class="card-title">Wrap up</h3>
    <ul>
      <li>Explain how to handle clock synchronization</li>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: System Design Interview, Alex Xu
  </div>
</div>
<!-- System Design 1 END -->

</div> <!-- /.col-md-12 -->
</div> <!-- /.row -->
</div> <!-- /.container -->

<include src="/footer.html"></include>

</body>

</html>