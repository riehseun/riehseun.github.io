<!DOCTYPE html>

<html lang="en">

<head>

<!-- Metadata -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="">
<meta name="author" content="Seungmoon Rieh">
<meta name="keywords" content="">

<!-- Title and image -->
<title>Seungmoon Rieh</title>
<link href="/img/seungmoonrieh.jpg" rel="icon">

<!-- CSS -->
<link href="/css/bootstrap.min.css" rel="stylesheet">
<link href="/css/monokai-sublime.css" rel="stylesheet">
<link href="/css/site.css" rel="stylesheet">

<!-- JavaScript -->
<script src="/js/jquery.min.js"></script>
<script src="/js/bootstrap.bundle.min.js"></script>
<script src="/js/highlight.pack.js"></script>
<script type="text/javascript" src="/js/include_html.js"></script>
<script type="text/javascript" src="/js/site.js"></script>

</head>

<body>

<include src="/header.html"></include>

<div class="container">
<div class="row">
<div class="col-md-12">
<h1 class="my-4">Data Engineering</h1>

<!-- Scikit-learn BEGIN -->
<div class="card mb-4" id="scikit-learn">
  <div class="card-body">
    <h2 class="card-title">Scikit-learn</h2>
    <p class="card-text"></p>
    <ul class="list-unstyled mb-0">
      <li><a href="#scikit-learn-1">Scikit-learn</a></li>
      <li><a href="#scikit-learn-2">Linear regression</a></li>
      <li><a href="#scikit-learn-3">Logistic regression</a></li>
      <li><a href="#scikit-learn-4">Support vector machine</a></li>
      <li><a href="#scikit-learn-5">K-Nearest Neighbors</a></li>
    </ul>
  </div>
</div>

<div class="card mb-4" id="scikit-learn-1">
  <div class="card-body">
    <h2 class="card-title">Scikit-learn</h2>

    <h3 class="card-title">Standard data format</h3>
    <ul>
      <li>Transform data such that mean is 0 and variance is 1.</li>
      <li>Stardardize across the column: <code>axis=0</code></li>
      <li>Stardardize across the row: <code>axis=1</code></li>
    </ul>

<pre><code class="python">from sklearn.preprocessing import scale

# Standardizing each column of pizza_data
col_standardized = scale(pizza_data)

# Column means (rounded to nearest thousandth)
col_means = col_standardized.mean(axis=0).round(decimals=3)

# Column standard deviations
col_stds = col_standardized.std(axis=0)</code></pre>

    <h3 class="card-title">Range scaling</h3>
    <ul>
      <li>Often need to compress data into [0,1] range.</li>
    </ul>

<pre><code class="python">from sklearn.preprocessing import MinMaxScaler

default_scaler = MinMaxScaler() # the default range is [0,1]
transformed = default_scaler.fit_transform(data)

custom_scaler = MinMaxScaler(feature_range=(-2, 3))
transformed = custom_scaler.fit_transform(data)</code></pre>

    <h3 class="card-title">Robust scaling</h3>
    <ul>
      <li>Avoid being affected by outliers by using the data's median and Interquartile Range. (IQR)</li>
    </ul>

<pre><code class="python">from sklearn.preprocessing import RobustScaler

robust_scaler = RobustScaler()
transformed = robust_scaler.fit_transform(data)</code></pre>

    <h3 class="card-title">L2 normalization</h3>
    <ul>
      <li>Other scaling techniques are applied to data features (columns), but L2 norm is applied to a particular row.</li>
    </ul>

<pre><code class="python">from sklearn.preprocessing import Normalizer

normalizer = Normalizer()
transformed = normalizer.fit_transform(data)</code></pre>

    <h3 class="card-title">Data imputation</h3>
    <ul>
      <li>Handles missing value.</li>
    </ul>

<pre><code class="python">from sklearn.impute import SimpleImputer

imp_mean = SimpleImputer()
transformed = imp_mean.fit_transform(data)

imp_median = SimpleImputer(strategy='median')
transformed = imp_median.fit_transform(data)

imp_frequent = SimpleImputer(strategy='most_frequent')
transformed = imp_frequent.fit_transform(data)

imp_constant = SimpleImputer(strategy='constant',
                             fill_value=-1)
transformed = imp_constant.fit_transform(data)</code></pre>

    <h3 class="card-title">PCA</h3>

<pre><code class="python">from sklearn.decomposition import PCA

pca_obj = PCA() # The value of n_component will be by default m-1
pc = pca_obj.fit_transform(data).round(3)

pca_obj = PCA(n_components=3)
pc = pca_obj.fit_transform(data).round(3)</code></pre>

    <h3 class="card-title">Class labels</h3>

<pre><code class="python">from sklearn.datasets import load_breast_cancer

bc = load_breast_cancer()
bc.data  # All the dataset values.
bc.target  # Class ID labels for each row in bc.data.
malignant = bc.data[bc.target == 0]
benign = bc.data[bc.target == 1]</code></pre>

    <h3 class="card-title">Linear regression</h3>
    <ul>
      <li><code>LinearRegression</code> implements the least squares model.</li>
    </ul>

<pre><code class="python">from sklearn import linear_model

pizza_data  # 5x2 vector
pizza_prices  # 5x1 vector
new_pizzas  # 2x2 vector

reg = linear_model.LinearRegression()
reg.fit(pizza_data, pizza_prices)
reg.predict(new_pizzas)
reg.coef_
reg.intercept_
reg.score(pizza_data, pizza_prices)  # R2 score.</code></pre>

    <h3 class="card-title">Ridge regression</h3>
    <ul>
      <li>L2 regulariztion.</li>
    </ul>

<pre><code class="python">from sklearn import linear_model

reg = linear_model.Ridge(alpha=0.1)
reg.fit(pizza_data, pizza_prices)
reg.score(pizza_data, pizza_prices)

alphas = [0.1, 0.2, 0.3]
reg = linear_model.RidgeCV(alphas=alphas)
reg.fit(pizza_data, pizza_prices)</code></pre>

    <h3 class="card-title">LASSO regression</h3>
    <ul>
      <li>L1 regulariztion.</li>
    </ul>

<pre><code class="python">from sklearn import linear_model

reg = linear_model.Lasso(alpha=0.1)
reg.fit(data, labels)</code></pre>

    <h3 class="card-title">Bayesian regression</h3>
    <ul>
      <li>Hyperparameter optimization of regularized regression.</li>
    </ul>

<pre><code class="python">from sklearn import linear_model

# data.shape: (150, 4)
# labels.shape: (150,)

reg = linear_model.BayesianRidge()
reg.fit(data, labels)
reg.coef_
reg.intercept_
reg.score(data, labels)
reg.alpha_
reg.lambda_</code></pre>

    <h3 class="card-title">Logistic regression</h3>
    <ul>
      <li>Classification technique..</li>
    </ul>

<pre><code class="python">from sklearn import linear_model

# data.shape: (569, 4)
# labels.shape: (569,)
# new_data.shape: (2, 4)

reg = linear_model.LogisticRegression()
reg.fit(data, labels)
reg.predict(new_data)

# Multiclass
reg = linear_model.LogisticRegression(
    solver='lbfgs',
    multi_class='multinomial',
    max_iter=200)
reg.fit(data, labels)
reg.predict(new_data)

# Cross-validated model
reg = linear_model.LogisticRegressionCV(
    solver='multinomial', max_iter=1000)</code></pre>

    <h3 class="card-title">Decision tree</h3>
    <ul>
      <li>Binary tree where each node represents a feature.</li>
      <li>Decent left or right based on feature value.</li>
      <li>Choose feature value that best split_dataset the remaining dataset in each node.</li>
    </ul>

<pre><code class="python">from sklearn import tree

clf_tree1 = tree.DecisionTreeClassifier()
reg_tree1 = tree.DecisionTreeRegressor()
clf_tree2 = tree.DecisionTreeClassifier(max_depth=8)  # max depth of 8.
reg_tree2 = tree.DecisionTreeRegressor(max_depth=5)  # max depth of 5.

# data.shape: (569, 4)
# labels.shape: (569,)
clf_tree1.fit(data, labels)</code></pre>

    <h3 class="card-title">Data split</h3>

<pre><code class="python">from sklearn.model_selection import train_test_split

# data.shape: (8, 2)
# labels.shape: (2,)

split_dataset = train_test_split(data, labels, test_size=0.375)
train_data = split_dataset[0]
test_data = split_dataset[1]
train_labels = split_dataset[2]
test_labels = split_dataset[3]

# train_data.shape: (5, 2)
# train_labels.shape: (5,)
# test_data.shape: (3, 2)
# test_labels.shape: (3,)</code></pre>

    <h3 class="card-title">Cross validation</h3>
    <ul>
      <li>Partition training set into smaller subsets.</li>
    </ul>

<pre><code class="python">from sklearn import linear_model
from sklearn.model_selection import cross_val_score

clf = linear_model.LogisticRegression(max_iter=3000)
cv_score = cross_val_score(clf, data, labels, cv=3)  # k = 3.

reg = linear_model.LinearRegression()
cv_score = cross_val_score(reg, data, labels, cv=4)  # k = 4.

# Apply K-Fold CV to tune a decision tree's maximum depth.
is_clf = True  # for classification
for depth in range(3, 8):
    # Predefined data and labels
    scores = cv_decision_tree(is_clf, data, labels, depth, 5)  # k = 5.
    mean = scores.mean()  # Mean acc across folds.
    std_2 = 2 * scores.std()  # 2 std devs.</code></pre>

    <h3 class="card-title">Model evaluation</h3>

<pre><code class="python">from sklearn import metrics

reg = tree.DecisionTreeRegressor()
reg.fit(train_data, train_labels)
predictions = reg.predict(test_data)

r2 = metrics.r2_score(test_labels, predictions)
mse = metrics.mean_squared_error(test_labels, predictions)
mae = metrics.mean_absolute_error(test_labels, predictions)

clf = tree.DecisionTreeClassifier()
clf.fit(train_data, train_labels)
predictions = clf.predict(test_data)

acc = metrics.accuracy_score(test_labels, predictions)</code></pre>

    <h3 class="card-title">Grid search</h3>
    <ul>
      <li>It can be incredibly slow for larger datasets.</li>
    </ul>

<pre><code class="python">reg = linear_model.BayesianRidge()
params = {
  'alpha_1':[0.1,0.2,0.3],
  'alpha_2':[0.1,0.2,0.3]
}
reg_cv = GridSearchCV(reg, params, cv=5)
reg_cv.fit(train_data, train_labels)</code></pre>

    <h3 class="card-title">Cosine similarity</h3>
    <ul>
      <li>Range: [-1, 1].</li>
      <li>Proportional similarity of feature values between two data observations.</li>
    </ul>

<pre><code class="python">from sklearn.metrics.pairwise import cosine_similarity

data = np.array([
  [ 1.1,  0.3],
  [ 2.1,  0.6],
  [-1.1, -0.4],
  [ 0. , -3.2]])
cos_sims = cosine_similarity(data)
# array([[ 1.        ,  0.99992743, -0.99659724, -0.26311741],
#        [ 0.99992743,  1.        , -0.99751792, -0.27472113],
#        [-0.99659724, -0.99751792,  1.        ,  0.34174306],
#        [-0.26311741, -0.27472113,  0.34174306,  1.        ]])

data = np.array([
  [ 1.1,  0.3],
  [ 2.1,  0.6],
  [-1.1, -0.4],
  [ 0. , -3.2]])
data2 = np.array([
  [ 1.7,  0.4],
  [ 4.2, 1.25],
  [-8.1,  1.2]])
cos_sims = cosine_similarity(data, data2)
# array([[ 0.9993819 ,  0.99973508, -0.91578821],
#        [ 0.99888586,  0.99993982, -0.9108828 ],
#        [-0.99308366, -0.9982304 ,  0.87956492],
#        [-0.22903933, -0.28525359, -0.14654866]])</code></pre>

    <h3 class="card-title">Nearest neighbors</h3>
    <ul>
      <li>Find k most similar data observations for a given data observation.</li>
    </ul>

<pre><code class="python">data = np.array([
  [5.1, 3.5, 1.4, 0.2],
  [4.9, 3. , 1.4, 0.2],
  [4.7, 3.2, 1.3, 0.2],
  [4.6, 3.1, 1.5, 0.2],
  [5. , 3.6, 1.4, 0.2],
  [5.4, 3.9, 1.7, 0.4],
  [4.6, 3.4, 1.4, 0.3],
  [5. , 3.4, 1.5, 0.2],
  [4.4, 2.9, 1.4, 0.2],
  [4.9, 3.1, 1.5, 0.1]])

from sklearn.neighbors import NearestNeighbors
nbrs = NearestNeighbors(n_neighbors=5)
nbrs.fit(data)
new_obs = np.array([[5. , 3.5, 1.6, 0.3]])
dists, knbrs = nbrs.kneighbors(new_obs)  # array([[0.17320508, 0.24494897, 0.24494897, 0.45825757, 0.46904158]]), array([[7, 4, 0, 6, 9]])
only_nbrs = nbrs.kneighbors(new_obs, return_distance=False)  # array([[7, 4, 0, 6, 9]])</code></pre>

    <h3 class="card-title">K-means clustering</h3>
    <ul>
      <li>Separate data into K clusters using cluster means/centroids.</li>
      <li>Assumes that the dataset consists of spherical clusters.</li>
      <li>Number of clusters needs to be passed in.</li>
    </ul>

<pre><code class="python">from sklearn.cluster import KMeans
kmeans = KMeans(n_clusters=3)  # kmeans = MiniBatchKMeans(n_clusters=3, batch_size=10) can be used for min-batch clustering.
# predefined data
kmeans.fit(data)

kmeans.cluster_centers_
# array([[6.85      , 3.07368421, 5.74210526, 2.07105263],
#        [5.006     , 3.428     , 1.462     , 0.246     ],
#        [5.9016129 , 2.7483871 , 4.39354839, 1.43387097]])

new_obs = np.array([
  [5.1, 3.2, 1.7, 1.9],
  [6.9, 3.2, 5.3, 2.2]])
kmeans.predict(new_obs)
# array([1, 0], dtype=int32)</code></pre>

    <h3 class="card-title">Hierarchical clustering</h3>
    <ul>
      <li>Initially treats each data observation as its own cluster, then repeatedly merges the two most similar clusters until reaching the desired number of clusters.</li>
      <li>Number of clusters needs to be passed in.</li>
    </ul>

<pre><code class="python">from sklearn.cluster import AgglomerativeClustering
agg = AgglomerativeClustering(n_clusters=3)
# predefined data
agg.fit(data)</code></pre>

    <h3 class="card-title">Mean shift clustering</h3>
    <ul>
      <li>Finds a number of candidate centroids.</li>
      <li>Removes candidates that are duplicates of others.</li>
      <li>The final set of centroids determines the number of clusters as well as the dataset cluster assignments.</li>
      <li>Assumes that clusters have a "blob"-like shape.</li>
    </ul>

<pre><code class="python">from sklearn.cluster import MeanShift
mean_shift = MeanShift()
# predefined data
mean_shift.fit(data)

mean_shift.cluster_centers_
# array([[6.21142857, 2.89285714, 4.85285714, 1.67285714],
#        [5.01632653, 3.45102041, 1.46530612, 0.24489796]])

new_obs = np.array([
  [5.1, 3.2, 1.7, 1.9],
  [6.9, 3.2, 5.3, 2.2]])
mean_shift.predict(new_obs)
# array([1, 0])</code></pre>

    <h3 class="card-title">DBSCAN</h3>
    <ul>
      <li>Find dense regions in the dataset.</li>
      <li>Treats high-density regions as clusters in the dataset, and low-density regions as area between clusters.</li>
    </ul>

<pre><code class="python">from sklearn.cluster import DBSCAN
dbscan = DBSCAN(eps=1.2, min_samples=30)
# predefined data
dbscan.fit(data)

num_core_samples = len(dbscan.core_sample_indices_)  # 133</code></pre>

    <h3 class="card-title">Evaluating clusters</h3>
    <ul>
      <li>Score -1: bad labeling.</li>
      <li>Score 0: random labeling.</li>
      <li>Score 1: perfect labeling.</li>
    </ul>

<pre><code class="python">from sklearn.metrics import adjusted_rand_score
true_labels = np.array([0, 0, 0, 1, 1, 1])
pred_labels = np.array([0, 0, 1, 1, 2, 2])

ari = adjusted_rand_score(true_labels, pred_labels)  # 0.24242424242424243
ari = adjusted_rand_score(pred_labels, true_labels)  # 0.24242424242424243

perf_labels = np.array([0, 0, 0, 1, 1, 1])
ari = adjusted_rand_score(true_labels, perf_labels)  # 1.0

permuted_labels = np.array([1, 1, 1, 0, 0, 0])
ari = adjusted_rand_score(true_labels, permuted_labels)  # 1.0

renamed_labels = np.array([1, 1, 1, 3, 3, 3])
ari = adjusted_rand_score(true_labels, renamed_labels)  # 1.0

true_labels2 = np.array([0, 1, 2, 0, 3, 4, 5, 1])
pred_labels2 = np.array([1, 1, 0, 0, 2, 2, 2, 2])
ari = adjusted_rand_score(true_labels2, pred_labels2)  # -0.12903225806451613</code></pre>

    <h3 class="card-title">Feature clustering</h3>
    <ul>
      <li>Performs feature dimensionality reduction similar to PCA.</li>
    </ul>

<pre><code class="python">data.shape  # (150, 4)

from sklearn.cluster import FeatureAgglomeration
agg = FeatureAgglomeration(n_clusters=2)
new_data = agg.fit_transform(data)  # (150, 2)</code></pre>

  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://www.educative.io/path/become-a-machine-learning-engineer">Become a Machine Learning Engineer</a>
  </div>
</div>

<div class="card mb-4" id="scikit-learn-2">
  <div class="card-body">
    <h2 class="card-title">Linear regression</h2>

    <h3 class="card-title">Import libraries</h3>

<pre><code class="python">import pandas as pd
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn import metrics</code></pre>

    <h3 class="card-title">Import dataset</h3>

<pre><code class="python">df = pd.read_csv('Melbourne_housing_FULL.csv')</code></pre>

    <h3 class="card-title">Remove variables</h3>

<pre><code class="python">del df['Address']
del df['Method']
del df['SellerG']
del df['Date']
del df['Postcode']
del df['YearBuilt']
del df['Type']
del df['Lattitude']
del df['Longtitude']
del df['Regionname']
del df['Suburb']
del df ['CouncilArea']

df.isnull().sum()

df_heat = df.corr()
sns.heatmap(df_heat,annot=True,cmap='coolwarm')

#df.shape

#Remove variables
del df ['Bedroom2']  # Highly correlated with Rooms (0.95)
del df ['Landsize']  # Low correlation (0.033) to the dependent variable of Price.
del df ['Propertycount']  # Low correlation (0.059) to the dependent variable of Price.</code></pre>

    <h3 class="card-title">Remove or modify variables with missing values</h3>
    <ul>
      <li>Use the mean to fill variables with partial correlation to Price.</li>
      <li>Remove rows for variables with a small number of missing values.</li>
      <li>Avoid filling values for variables with significant correlation to Price. Instead, remove those missing values row-by-row.</li>
    </ul>

<pre><code class="python"># Remove variable BuildingArea.
del df ['BuildingArea']

# Fill missing values with the mean for the variable Car.
df['Car'].fillna(df['Car'].mean(),inplace=True)

# Drop remaining missing values on a row-by-row basis.
df.dropna(axis=0, how='any', thresh=None, subset=None, inplace=True)

df.shape</code></pre>

    <h3 class="card-title">Set X and y variables</h3>

<pre><code class="python">X = df[['Rooms', 'Distance', 'Bathroom', 'Car']]
y = df['Price']

# Letâ€™s also shuffle and sub-divide the data into training and test sets using a standard 70/30 split.
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10, shuffle=True)</code></pre>

    <h3 class="card-title">Set algorithm</h3>

<pre><code class="python">model = LinearRegression()
model.fit(X_train, y_train)</code></pre>

    <h3 class="card-title">Find y-intercept and X coefficients</h3>

<pre><code class="python"># Find y-intercept.
print("model.intercept_ result:")
print(model.intercept_)
print("")

# Find x coefficients.
print("model.coef_ result:")
print(model.coef_)
print("")

model_results = pd.DataFrame(model.coef_, X.columns, columns=['Coefficients'])
print(model_results)</code></pre>

    <h3 class="card-title">Predict</h3>

<pre><code class="python">new_house = [
  2, #Rooms
  2.5, #Distance
  1, #Bathroom
  1, #Car
]

new_house_predict = model.predict([new_house])
print(new_house_predict)</code></pre>

    <h3 class="card-title">Evaluate</h3>

<pre><code class="python">prediction = model.predict(X_test)
print(metrics.mean_absolute_error(y_test, prediction))</code></pre>

  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://www.educative.io/path/become-a-machine-learning-engineer">Become a Machine Learning Engineer</a>
  </div>
</div>

<div class="card mb-4" id="scikit-learn-3">
  <div class="card-body">
    <h2 class="card-title">Logistic regression</h2>

    <h3 class="card-title">Import libraries</h3>

<pre><code class="python">import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, classification_report</code></pre>

    <h3 class="card-title">Import dataset</h3>

<pre><code class="python">df = pd.read_csv('18k_Projects.csv', dtype='unicode')</code></pre>

    <h3 class="card-title">Remove variables</h3>

<pre><code class="python">del df ['Id']
del df ['Name']
del df ['Url']
del df ['Location']
del df ['Pledged']
del df ['Creator']
del df ['Category']
del df ['Updates']
del df ['Start']
del df ['End']
del df ['Latitude']
del df ['Longitude']
del df ['Start Timestamp (UTC)']
del df ['End Timestamp (UTC)']
del df ['Creator Bio']
del df ['Creator Website']

print(df.shape)</code></pre>

    <h3 class="card-title">Convert non-numeric values</h3>

<pre><code class="python">df = pd.get_dummies(df, columns=['State', 'Currency', 'Top Category', 'Facebook Connected', 'Has Video'], drop_first = True)
print(df.shape)</code></pre>

    <h3 class="card-title">Remove and fill missing values</h3>

<pre><code class="python">print(df.isnull().sum())

print(df.describe())

# Distribution plot of variable 'Facebook Friends'.
plt.figure(figsize=(12,6))
sns.distplot(df['Facebook Friends'], kde=True, hist=0)

# Distribution plot of variable 'Creator - # Projects Backed'.
plt.figure(figsize=(12,6))
sns.distplot(df['Creator - # Projects Backed'], kde=True, hist=0)

# Fill missing values for 'Creator - # Projects Backed' with the mean value.
df['Creator - # Projects Backed'].fillna(df['Creator - # Projects Backed'].astype(float).mean(), inplace=True)

# Drop remaining missing values for remaining variables.
df.dropna(axis=0, how='any', thresh=None, subset=None, inplace=True)

print(df.shape)</code></pre>

    <h3 class="card-title">Set X and y variables</h3>

<pre><code class="python">X = df.drop('State_successful',axis=1)
y = df['State_successful']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10, shuffle=True)</code></pre>

    <h3 class="card-title">Set algorithm</h3>

<pre><code class="python">model = LogisticRegression(max_iter=10000)
model.fit(X_train, y_train)</code></pre>

    <h3 class="card-title">Predict</h3>

<pre><code class="python">model_predict = model.predict(X_test)
new_project = [
    0, #Comments
    9, #Rewards
    2500, #Goal
    157, #Backers
    31, #Duration in Days
    319, #Facebook Friends
    110, #Facebook Shares
    1, #Creator - # Projects Created
    0, #Creator - # Projects Backed
    0, ## Videos
    12, ## Images
    872, ## Words (Description)
    65, ## Words (Risks and Challenges)
    0, ## FAQs
    0, #Currency_AUD
    1, #Currency_CAD
    0, #Currency_EUR
    0, #Currency_GBP
    0, #Currency_NZD
    0, #Currency_USD
    0, #Top Category_Art
    0, #Top Category_Comics
    0, #Top Category_Crafts
    0, #Top Category_Dance
    0, #Top Category_Design
    0, #Top Category_Fashion
    1, #Top Category_Film & Video
    0, #Top Category_Food
    0, #Top Category_Games
    0, #Top Category_Journalism
    0, #Top Category_Music
    0, #Top Category_Photography
    0, #Top Category_Publishing
    0, #Top Category_Technology
    0, #Top Category_Theater
    #0, #Facebook Connected_No
    #0, #Facebook Connected_Yes
    #0, #Has Video_No
    #1, #Has Video_Yes
]
new_pred = model.predict([new_project])
print(new_pred)</code></pre>

    <h3 class="card-title">Evaluate</h3>

<pre><code class="python"># Confusion matrix.
print(confusion_matrix(y_test, model_predict))

# Classification report.
print(classification_report(y_test, model_predict))</code></pre>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://www.educative.io/path/become-a-machine-learning-engineer">Become a Machine Learning Engineer</a>
  </div>
</div>

<div class="card mb-4" id="scikit-learn-4">
  <div class="card-body">
    <h2 class="card-title">Support vector machine</h2>
    <ul>
      <li>Similar to logistic regression.</li>
      <li>Unlike logistic regression, separate data classes by maximum distance between the partitioned data points.</li>
    </ul>

    <h3 class="card-title">Import libraries</h3>

<pre><code class="python">import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.model_selection import GridSearchCV</code></pre>

    <h3 class="card-title">Import dataset</h3>

<pre><code class="python">df = pd.read_csv('advertising.csv')</code></pre>

    <h3 class="card-title">Remove variables</h3>

<pre><code class="python">del df ['Ad Topic Line']
del df ['Timestamp']</code></pre>

    <h3 class="card-title">Convert non-numeric values</h3>

<pre><code class="python">df = pd.get_dummies(df, columns=['Country','City'])</code></pre>

    <h3 class="card-title">Set X and y variables</h3>

<pre><code class="python">X = df.drop('Clicked on Ad',axis=1)
y = df['Clicked on Ad']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10)</code></pre>

    <h3 class="card-title">Set algorithm</h3>

<pre><code class="python">model = SVC()
model.fit(X_train, y_train)</code></pre>

    <h3 class="card-title">Evaluate</h3>

<pre><code class="python">model_predict = model.predict(X_test)

# Confusion matrix.
print(confusion_matrix(y_test, model_predict))

# Classification report.
print(classification_report(y_test, model_predict))</code></pre>

    <h3 class="card-title">Grid search</h3>

<pre><code class="python">hyperparameters = {'C':[10,25,50],'gamma':[0.001,0.0001,0.00001]}

grid = GridSearchCV(SVC(),hyperparameters)

grid.fit(X_train, y_train)

print(grid.best_params_)</code></pre>

    <h3 class="card-title">Grid search predict</h3>

<pre><code class="python">grid_predictions = grid.predict(X_test)

# Confusion matrix.
print(confusion_matrix(y_test,grid_predictions))

# Classification report.
print(classification_report(y_test,grid_predictions))</code></pre>

  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://www.educative.io/path/become-a-machine-learning-engineer">Become a Machine Learning Engineer</a>
  </div>
</div>

<div class="card mb-4" id="scikit-learn-5">
  <div class="card-body">
    <h2 class="card-title">K-Nearest Neighbors</h2>
    <ul>
      <li>Full training data is used each time a prediction is made..</li>
      <li>Not recommended for analyzing large datasets.</li>
    </ul>

    <h3 class="card-title">Import libraries</h3>

<pre><code class="python">import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report, confusion_matrix</code></pre>

    <h3 class="card-title">Import dataset</h3>

<pre><code class="python">df = pd.read_csv('advertising.csv')</code></pre>

    <h3 class="card-title">Remove variables</h3>

<pre><code class="python">del df ['Ad Topic Line']
del df ['Timestamp']
del df['Male']
del df ['Country']
del df ['City']

print(df.head())</code></pre>

    <h3 class="card-title">Scale data</h3>

<pre><code class="python">scaler = StandardScaler()
scaler.fit(df.drop('Clicked on Ad',axis=1))
scaled_features = scaler.transform(df.drop('Clicked on Ad',axis=1))</code></pre>

    <h3 class="card-title">Set X and y variables</h3>

<pre><code class="python">X = scaled_features
y = df['Clicked on Ad']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10, shuffle=True)</code></pre>

    <h3 class="card-title">Set algorithm</h3>

<pre><code class="python">model = KNeighborsClassifier(n_neighbors=5)
model.fit(X_train, y_train)</code></pre>

    <h3 class="card-title">Evaluate</h3>

<pre><code class="python">model_predict = model.predict(X_test)

print(confusion_matrix(y_test, model_predict))
print(classification_report(y_test, model_predict))</code></pre>

    <h3 class="card-title">Optimize</h3>
    <ul>
      <li>Experiment with the number of neighbors chosen in step 5 and reduce the number of incorrectly predicted outcomes.</li>
    </ul>

    <h3 class="card-title">Predict</h3>

<pre><code class="python">model.predict(scaled_features)[0:10]</code></pre>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://www.educative.io/path/become-a-machine-learning-engineer">Become a Machine Learning Engineer</a>
  </div>
</div>
<!-- Scikit-learn END -->

</div> <!-- /.col-md-12 -->
</div> <!-- /.row -->
</div> <!-- /.container -->

<include src="/footer.html"></include>

</body>

</html>