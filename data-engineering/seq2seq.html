<!DOCTYPE html>

<html lang="en">

<head>

<!-- Metadata -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="">
<meta name="author" content="Seungmoon Rieh">
<meta name="keywords" content="">

<!-- Title and image -->
<title>Seungmoon Rieh</title>
<link href="/img/seungmoonrieh.jpg" rel="icon">

<!-- CSS -->
<link href="/css/bootstrap.min.css" rel="stylesheet">
<link href="/css/monokai-sublime.css" rel="stylesheet">
<link href="/css/site.css" rel="stylesheet">

<!-- JavaScript -->
<script src="/js/jquery.min.js"></script>
<script src="/js/bootstrap.bundle.min.js"></script>
<script src="/js/highlight.pack.js"></script>
<script type="text/javascript" src="/js/include_html.js"></script>
<script type="text/javascript" src="/js/site.js"></script>

</head>

<body>

<include src="/header.html"></include>

<div class="container">
<div class="row">
<div class="col-md-12">
<h1 class="my-4">Data Engineering</h1>

<!-- Seq2Seq BEGIN -->
<div class="card mb-4" id="seq2seq">
  <div class="card-body">
    <h2 class="card-title">Seq2Seq</h2>
    <p class="card-text"></p>
    <ul class="list-unstyled mb-0">
      <li><a href="#seq2seq-1">Seq2Seq</a></li>
    </ul>
  </div>
</div>

<div class="card mb-4" id="seq2seq-1">
  <div class="card-body">
    <h2 class="card-title">Seq2Seq</h2>
    <ul>
      <li>Use training pairs that contain an input sequence and an output sequence.</li>
      <ul>
        <li>Input Task: Extract useful information from the input sequence.</li>
        <li>Output Task: Calculate word probabilities at each output time step, using information from the input sequence and previous words in the output sequence.</li>
        <ul>
          <li>Ground truth sequence - equivalent to the input sequence for a language model.</li>
          <li>Final token sequence - equivalent to the output sequence when training a language model.</li>
        </ul>
      </ul>
      <li>Start-of-sequence (SOS) and end-of-sequence (EOS) tokens. For example, ["SOS", "he", "ate", "apple", EOS"]</li>
      <li>Encoder</li>
      <ul>
        <li>BiLSTM.</li>
        <li>Extract useful information from the input sequence</li>
        <li>Need to pass the final state of the encoder into the decoder.</li>
      </ul>
      <li>Decoder</li>
      <ul>
        <li>LSTM.</li>
        <li>Uses the final state of the encoder as its initial state.</li>
      </ul>
      <li>Attention</li>
      <ul>
        <li>Let decoder decide which encoder outputs are most useful at the current decoding time step.</li>
        <li>Calculates context vector.</li>
      </ul>
    </ul>

<pre><code class="python">import tensorflow as tf
import tensorflow_addons as tfa

# Get c and h vectors for bidirectional LSTM final states
def ref_get_bi_state_parts(state_fw, state_bw):
    bi_state_c = tf.concat([state_fw[0], state_bw[0]], -1)
    bi_state_h = tf.concat([state_fw[1], state_bw[1]], -1)
    return bi_state_c, bi_state_h

def create_basic_decoder(enc_outputs, extended_vocab_size, batch_size, final_state, dec_cell, sampler):
    projection_layer = tf.keras.layers.Dense(extended_vocab_size)
    batch_size = tf.constant(batch_size)
    initial_state = dec_cell.get_initial_state(enc_outputs[0], batch_size=batch_size, dtype=tf.float32)
    decoder = tfa.seq2seq.sampler.BasicDecoder(dec_cell, sampler, initial_state, output_layer=projection_layer)
    return decoder

def run_decoder(decoder,inputs, initial_state, input_seq_lens, is_training, dec_seq_lens):
    dec_outputs, _, _ = decoder(inputs, initial_state=initial_state, sequence_length=input_seq_lens, training=is_training)
    if is_training:
        logits = dec_outputs.rnn_output
        return logits, dec_seq_lens
    return dec_outpus.sample.id

# Seq2seq model
class Seq2SeqModel(object):
    def __init__(self, vocab_size, num_lstm_layers, num_lstm_units):
        self.vocab_size = vocab_size
        # Extended vocabulary includes start, stop token
        self.extended_vocab_size = vocab_size + 2
        self.num_lstm_layers = num_lstm_layers
        self.num_lstm_units = num_lstm_units
        self.tokenizer = tf.keras.preprocessing.text.Tokenizer(
            num_words=vocab_size)

    # Create a sequence training tuple from input/output sequences
    def make_training_tuple(self, input_sequence, output_sequence):
        truncate_front = output_sequence[1:]
        truncate_back = output_sequence[:-1]
        sos_token = [self.vocab_size]
        eos_token = [self.vocab_size + 1]
        input_sequence = sos_token + input_sequence + eos_token
        ground_truth = sos_token + truncate_back
        final_sequence = truncate_front + eos_token
        return input_sequence, ground_truth, final_sequence

    def make_lstm_cell(self, dropout_keep_prob, num_units):
        cell = tf.keras.layers.LSTMCell(num_units, dropout=dropout_keep_prob)
        return cell

    # Create multi-layer LSTM
    def stacked_lstm_cells(self, is_training, num_units):
        dropout_keep_prob = 0.5 if is_training else 1.0
        cell_list = [self.make_lstm_cell(dropout_keep_prob, num_units) for i in range(self.num_lstm_layers)]
        cell = tf.keras.layers.StackedRNNCells(cell_list)
        return cell

    # Get embeddings for input/output sequences
    def get_embeddings(self, sequences, scope_name):
        with tf.compat.v1.variable_scope(scope_name,reuse=tf.compat.v1.AUTO_REUSE):
            cat_column = tf.compat.v1.feature_column \
              .categorical_column_with_identity(
                  'sequences', self.extended_vocab_size)
            embed_size = int(self.extended_vocab_size**0.25)
            embedding_column = tf.compat.v1.feature_column.embedding_column(
                  cat_column, embed_size)
            seq_dict = {'sequences': sequences}
            embeddings= tf.compat.v1.feature_column \
                                 .input_layer(
                                     seq_dict, [embedding_column])
            sequence_lengths = tf.compat.v1.placeholder("int64", shape=(None,), name=scope_name+"/sinput_layer/sequence_length")
            return embeddings, tf.cast(sequence_lengths, tf.int32)

    # Create the encoder for the model
    def encoder(self, encoder_inputs, is_training):
        input_embeddings, input_seq_lens = self.get_embeddings(encoder_inputs, 'encoder_emb')
        cell = self.stacked_lstm_cells(is_training, self.num_lstm_units)

        combined_state = []
        rnn = tf.keras.layers.RNN(
              cell,
              return_sequences=True,
              return_state=True,
              go_backwards=True,
              dtype=tf.float32)
        Bi_rnn = tf.keras.layers.Bidirectional(
              rnn,
              merge_mode='concat'
              )
        input_embeddings = tf.reshape(input_embeddings, [-1,-1,2])
        outputs = Bi_rnn(input_embeddings)
        enc_outputs = outputs[0]
        states_fw =  [ outputs[i]  for i in range(1,self.num_lstm_layers+1)]
        states_bw =  [ outputs[i]  for i in range(self.num_lstm_layers+1,len(outputs))]

        for i in range(self.num_lstm_layers):
            bi_state_c, bi_state_h = ref_get_bi_state_parts(
                states_fw[i], states_bw[i]
            )
            bi_lstm_state = tf.compat.v1.nn.rnn_cell.LSTMStateTuple(bi_state_c, bi_state_h)
            combined_state.append(bi_lstm_state)
        final_state = tuple(combined_state)
        return enc_outputs, input_seq_lens, final_state

    # Helper funtion to combine BiLSTM encoder outputs
    def combine_enc_outputs(self, enc_outputs):
        enc_outputs_fw, enc_outputs_bw = enc_outputs
        return tf.concat([enc_outputs_fw, enc_outputs_bw], -1)

    # Create the stacked LSTM cells for the decoder
    def create_decoder_cell(self, enc_outputs, input_seq_lens, is_training):
        num_decode_units = self.num_lstm_units * 2
        dec_cell = self.stacked_lstm_cells(is_training, num_decode_units)
        combined_enc_outputs = self.combine_enc_outputs(enc_outputs)
        attention_mechanism = tfa.seq2seq.LuongAttention(num_decode_units, combined_enc_outputs, memory_sequence_length=input_seq_lens)
        dec_cell = tfa.seq2seq.AttentionWrapper(dec_cell, attention_mechanism, attention_layer_size=num_decode_units)
        return dec_cell

    # Calculate the model loss
    def calculate_loss(self, logits, dec_seq_lens, decoder_outputs, batch_size):
        binary_sequences = tf.compat.v1.sequence_mask(dec_seq_lens, dtype=tf.float32)
        batch_loss = tf.compat.v1.nn.sparse_softmax_cross_entropy_with_logits(labels=decoder_outputs, logits=logits)
        unpadded_loss = batch_loss * binary_sequences
        per_seq_loss = tf.math.reduce_sum(unpadded_loss) / batch_size
        return per_seq_loss

    # Create the sampler for decoding
    def create_decoder_sampler(self, decoder_inputs, is_training, batch_size):
        if is_training:
            dec_embeddings, dec_seq_lens = self.get_embeddings(decoder_inputs, 'decoder_emb')
            sampler = tfa.seq2seq.sampler.TrainingSampler()
        else:
            embedding_matrix = tf.keras.layers.Embedding(self.vocab_size, int(self.vocab_size/2))
            sampler = tfa.seq2seq.sampler.GreedyEmbeddingSampler(embedding_matrix)
            dec_seq_lens  = None
        return sampler, dec_seq_lens

    def Decoder(self, enc_outputs, input_seq_lens, final_state, batch_size,
            sampler, dec_seq_lens , is_training):
        dec_cell = self.create_decoder_cell(enc_outputs, input_seq_lens, is_training)
        projection_layer = tf.keras.layers.Dense(self.extended_vocab_size)
        batch_s = tf.constant(batch_size)


        initial_state = dec_cell.get_initial_state(enc_outputs[0],batch_size=batch_s , dtype = tf.float32)

        decoder = tfa.seq2seq.BasicDecoder(
            dec_cell, sampler,
            output_layer=projection_layer
            )
        inputs = enc_outputs[0]
        output = run_decoder(decoder,inputs , initial_state , input_seq_lens , is_training , dec_seq_lens)
        return output</code></pre>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://www.educative.io/path/become-a-machine-learning-engineer">Become a Machine Learning Engineer</a>
  </div>
</div>
<!-- Text classification END -->

</div> <!-- /.col-md-12 -->
</div> <!-- /.row -->
</div> <!-- /.container -->

<include src="/footer.html"></include>

</body>

</html>