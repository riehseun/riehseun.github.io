<!DOCTYPE html>

<html lang="en">

<head>

<!-- Metadata -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="">
<meta name="author" content="Seungmoon Rieh">
<meta name="keywords" content="">

<!-- Title and image -->
<title>Seungmoon Rieh</title>
<link href="/img/seungmoonrieh.jpg" rel="icon">

<!-- CSS -->
<link href="/css/bootstrap.min.css" rel="stylesheet">
<link href="/css/monokai-sublime.css" rel="stylesheet">
<link href="/css/site.css" rel="stylesheet">

<!-- JavaScript -->
<script src="/js/jquery.min.js"></script>
<script src="/js/bootstrap.bundle.min.js"></script>
<script src="/js/highlight.pack.js"></script>
<script type="text/javascript" src="/js/include_html.js"></script>
<script type="text/javascript" src="/js/site.js"></script>

</head>

<body>

<include src="/header.html"></include>

<div class="container">
<div class="row">
<div class="col-md-12">
<h1 class="my-4">Data Engineering</h1>

<!-- Spark BEGIN -->
<div class="card mb-4" id="spark">
  <div class="card-body">
    <h2 class="card-title">Spark</h2>
    <p class="card-text"></p>
    <ul class="list-unstyled mb-0">
      <li><a href="#spark-1">Spark</a></li>
      <li><a href="#spark-2">Start with Spark</a></li>
      <li><a href="#spark-3">Spark read</a></li>
    </ul>
  </div>
</div>

<div class="card mb-4" id="spark-1">
  <div class="card-body">
    <h2 class="card-title">Spark</h2>
    <ul>
      <li>Caches data it in memory across the cluster, saving costly round-trips to disk. (As opposed to MapReduce)</li>
      <li>Driver: master process.</li>
      <li>Executor: slave process.</li>
      <li>Spark is compatible with the cluster managers like Hadoop YARN and Kubernetes.</li>
      <li>Cluster mode: user submits a spark application to the cluster manager. which spawns the driver and executor processes on worker nodes.</li>
      <li>Client mode: driver process lives on the client machine, that is used to submit the Spark job.</li>
    </ul>

    <h3 class="card-title">Spark lifecycle</h3>
    <ul>
      <li>User submits a Spark job to the cluster.</li>
      <li>Driver process runs and executes user code.</li>
      <li>Cluster manager launches Executor processes on nodes.</li>
      <li>Driver assigns tasks to executor processes and job execution begins.</li>
      <li>Driver exits and the cluster manager shuts down the executor processes.</li>
    </ul>

    <h3 class="card-title">Resilent distributed datasets (RDD)</h3>
    <ul>
      <li>Read-only (immutable) collection of objects.</li>
      <li>Transformations are applied on RDDs to produce new RDDs.</li>
      <li>Actions are applied to produce results.</li>
      <li>Transformations are lazily evaluated so they aren’t executed until an action is performed.</li>
      <li>RDD keeps track of its lineage, which is a graph of all the parent RDDs.</li>
    </ul>

    <h3 class="card-title">Data frames</h3>
    <ul>
      <li>Looks just like tables in relational databases.</li>
      <li>RDDs can be converted to data frames.</li>
      <li>A partition is a collection of rows from the parent DataFrame.</li>
    </ul>

    <h3 class="card-title">Spark application</h3>
    <ul>
      <li>Each Spark application is associated with one SparkSession.</li>
      <li>SparkSession has another field:SparkContext which represents the connection to the Spark Cluster.</li>
      <li>SparkContext can create RDDs.</li>
      <li>Spark application consists of one or more jobs.</li>
      <li>Each job is made of a directed acyclic graph of stages.</li>
      <li>A stage is split into tasks and is executed in parallel on RDD partitions.</li>
      <li></li>
    </ul>

<pre><code class="bash"># Read in data.
df = spark.read.option("inferSchema", true).option("header", false).text("file:///path/to/file")

# Examine schema.
df.schema</code></pre>

<pre><code class="bash"># Print the context object.
spark.sparkContext

# Read in data.
rdd = spark.sparkContext.textFile("/path/to/textfile")

# Print the number of records read-in.
rdd.count()

# Print the first record.
rdd.first()

# Print all the records.
rdd.foreach(System.out.println(_))

# Print the number of partitions.
rdd.getNumPartitions

# Word count problem.
temp1 = rdd.flatMap(line => line.split(","))
temp1.collect()
temp2 = temp1.map(word => (word, 1))
temp2.collect()
temp3 = temp2.reduceByKey((v1, v2) => v1 + v2)
temp3.collect()
temp4 = temp3.reduce( (t1, t2) => ("total", t1._2 + t2._2) )
temp4.collect()</code></pre>

    <h3 class="card-title">Narrow transformations</h3>
    <ul>
      <li>Input partition contributes to only one output partition.</li>
      <li>map: apply function to each element of RDD.</li>
      <li>flatMap: map then flatten output.</li>
      <li>filter: keep only elements where function is true.</li>
      <li>coalesce: reduce number of partitions.</li>
    </ul>

    <h3 class="card-title">Wide transformations</h3>
    <ul>
      <li>Input partitions contribute to several output partitions.</li>
      <li>groupByKey: (K, V) pairs => (K, list of all V)</li>
    </ul>

    <h3 class="card-title">Actions</h3>
    <ul>
      <li>collect: copy all elements to the driver.</li>
      <li>take(n): copy first n elements.</li>
      <li>reduce(func): aggregate elements with func.</li>
      <li>saveAsTextFile(filename): save to local file or HDFS.</li>
    </ul>

    <h3 class="card-title">Shuffle map task</h3>
    <ul>
      <li>Physical repartitioning of the data.</li>
      <li>Runs in all stages except the final stage.</li>
      <li>Each shuffle task runs a computation on one RDD partition.</li>
      <li>All shuffle tasks write their output to disk.</li>
    </ul>

    <h3 class="card-title">Result task</h3>
    <ul>
      <li>Runs in the last stage and return a result to the user’s program.</li>
      <li>Tasks run in parallel on RDDs partition.</li>
    </ul>

    <h3 class="card-title">Spark SQL</h3>
    <ul>
      <li>Enables querying structured and unstructured data through Spark.</li>
      <li>APIs for Scala, Java, Python to convert result into RDDs.</li>
      <li>Deploy business intelligence tools over Spark.</li>
    </ul>

    <h3 class="card-title">Spark streaming</h3>
    <ul>
      <li>Sources: Kafka, Flume, HDFS, S3, etc.</li>
    </ul>

    <h3 class="card-title">Spark MLLib</h3>
    <ul>
      <li>Machine learning library for Spark</li>
    </ul>

    <h3 class="card-title">Spark GraphX</h3>
    <ul>
      <li>API for graph computation.</li>
      <li>triplets: views that logically join vertex and edge properties.</li>
    </ul>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://www.coursera.org/specializations/big-data">Big data</a> | <a href="https://www.educative.io/courses/introduction-to-big-data-and-hadoop">Introduction to Big Data and Hadoop</a>
  </div>
</div>

<div class="card mb-4" id="spark-2">
  <div class="card-body">
    <h2 class="card-title">Start with Spark</h2>
<pre><code class="bash">bin/spark-shell
# Read a file from local file system.
val data = sc.textFile("tmp/test/file")
val num = Array(1,2,3,4,5,6,7,8,9)
val NewData = sc.parallelize(num)
NewData.count()
# Type Ctrl-D to exit.</code></pre>
  </div>
  <div class="card-footer text-muted">

  </div>
</div>

<div class="card mb-4" id="spark-3">
  <div class="card-body">
    <h2 class="card-title">Start read</h2>
<pre><code class="bash">val file = spark.read.parquet("path_to_parquet_file")
val file = spark.read.load("path_to_source_file")</code></pre>
  </div>
  <div class="card-footer text-muted">

  </div>
</div>
<!-- Spark END -->

</div> <!-- /.col-md-12 -->
</div> <!-- /.row -->
</div> <!-- /.container -->

<include src="/footer.html"></include>

</body>

</html>