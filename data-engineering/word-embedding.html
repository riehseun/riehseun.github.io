<!DOCTYPE html>

<html lang="en">

<head>

<!-- Metadata -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="">
<meta name="author" content="Seungmoon Rieh">
<meta name="keywords" content="">

<!-- Title and image -->
<title>Seungmoon Rieh</title>
<link href="/img/seungmoonrieh.jpg" rel="icon">

<!-- CSS -->
<link href="/css/bootstrap.min.css" rel="stylesheet">
<link href="/css/monokai-sublime.css" rel="stylesheet">
<link href="/css/site.css" rel="stylesheet">

<!-- JavaScript -->
<script src="/js/jquery.min.js"></script>
<script src="/js/bootstrap.bundle.min.js"></script>
<script src="/js/highlight.pack.js"></script>
<script type="text/javascript" src="/js/include_html.js"></script>
<script type="text/javascript" src="/js/site.js"></script>

</head>

<body>

<include src="/header.html"></include>

<div class="container">
<div class="row">
<div class="col-md-12">
<h1 class="my-4">Data Engineering</h1>

<!-- Word embedding BEGIN -->
<div class="card mb-4" id="word-embedding">
  <div class="card-body">
    <h2 class="card-title">Word embedding</h2>
    <p class="card-text"></p>
    <ul class="list-unstyled mb-0">
      <li><a href="#word-embedding-1">Word embedding</a></li>
    </ul>
  </div>
</div>

<div class="card mb-4" id="word-embedding-1">
  <div class="card-body">
    <h2 class="card-title">Word embedding</h2>
    <ul>
      <li><code>Tokenizer</code> converts each vocabulary word to an integer ID. (IDs are given to words by descending frequency)</li>
      <li>Out-of-vocabulary (OOV) - new text contains words not in the corpus vocabulary.</li>
      <li><code>texts_to_sequences</code> automatically filters out all OOV words.</li>
      <li>Skip gram</li>
      <ul>
        <li>“paul likes <strong>singing</strong> in french” -> (singing,paul), (singing,likes), (singing, in), (singing, french)</li>
        <li>Requires much less actual data.</li>
        <li>Represent rarer words or phrases better.</li>
      </ul>
      <li>Continuous bag of words (CBOW)</li>
      <ul>
        <li>“tom eats <strong>spicy</strong> crab salad” -> (tom,eats,crab,salad),spicy)</li>
        <li>Faster to train.</li>
        <li>Provides more accurate embeddings for more common words.</li>
      </ul>
      <li>Variable should be a 2-D matrix that contains embedding vectors for each vocabulary word ID.</li>
      <li>Training an embedding model is equivalent to multiclass classification, where the possible classes include every single vocabulary word.</li>
      <li>NCE Loss convert the multiclass classification problem into a binary classification problem.</li>
    </ul>

<pre><code class="python"></code></pre>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://www.educative.io/path/become-a-machine-learning-engineer">Become a Machine Learning Engineer</a>
  </div>
</div>
<!-- Word embedding END -->

</div> <!-- /.col-md-12 -->
</div> <!-- /.row -->
</div> <!-- /.container -->

<include src="/footer.html"></include>

</body>

</html>