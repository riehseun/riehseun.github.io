<!DOCTYPE html>

<html lang="en">

<head>

<!-- Metadata -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="">
<meta name="author" content="Seungmoon Rieh">
<meta name="keywords" content="">

<!-- Title and image -->
<title>Seungmoon Rieh</title>
<link href="/img/seungmoonrieh.jpg" rel="icon">

<!-- CSS -->
<link href="/css/bootstrap.min.css" rel="stylesheet">
<link href="/css/monokai-sublime.css" rel="stylesheet">
<link href="/css/site.css" rel="stylesheet">

<!-- JavaScript -->
<script src="/js/jquery.min.js"></script>
<script src="/js/bootstrap.bundle.min.js"></script>
<script src="/js/highlight.pack.js"></script>
<script type="text/javascript" src="/js/include_html.js"></script>
<script type="text/javascript" src="/js/site.js"></script>

</head>

<body>

<include src="/header.html"></include>

<div class="container">
<div class="row">
<div class="col-md-12">
<h1 class="my-4">Data Engineering</h1>

<!-- Language model BEGIN -->
<div class="card mb-4" id="language-model">
  <div class="card-body">
    <h2 class="card-title">Language model</h2>
    <p class="card-text"></p>
    <ul class="list-unstyled mb-0">
      <li><a href="#language-model-1">Language model</a></li>
    </ul>
  </div>
</div>

<div class="card mb-4" id="language-model-1">
  <div class="card-body">
    <h2 class="card-title">Language model</h2>
    <ul>
      <li>Assign probabilities to words in sequences of text.</li>
      <li>The probability for each word is conditioned on the words that appear before it in the sequence.</li>
      <li>This is essentially multiclass classification.</li>
      <li>Example</li>
      <ul>
        <li>Original sequence: ["she", "bought", "a", "book", "from" ,"me"]</li>
        <li>Input sequence: ["she", "bought", "a", "book", "from"]</li>
        <li>Target sequence: ["bought", "a", "book", "from" ,"me"]</li>
      </ul>
      <li>Recurrent neural network: language model needs to be able to handle input data of varied lengths.</li>
      <li>LSTM: capture long term dependencies.</li>
      <li>Apply dropout to the input and/or output of each cell unit. (As opposed to hidden units in DL)</li>
    </ul>

<pre><code class="python">import tensorflow as tf

def truncate_sequences(sequence, max_length):
    input_sequence = sequence[:max_length-1]
    target_sequence = sequence[1:max_length]
    return input_sequence, target_sequence

def pad_sequences(sequence, max_length):
    padding_amount = max_length - len(sequence)
    padding = [0] * padding_amount
    input_sequence =  sequence[:-1] + padding
    target_sequence = sequence[1:] + padding
    return input_sequence, target_sequence

# LSTM Language Model
class LanguageModel(object):
    # Model Initialization
    def __init__(self, vocab_size, max_length, num_lstm_units, num_lstm_layers):
        self.vocab_size = vocab_size
        self.max_length = max_length
        self.num_lstm_units = num_lstm_units
        self.num_lstm_layers = num_lstm_layers
        self.tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=vocab_size)

    def get_input_target_sequence(self, sequence):
        seq_len = len(sequence)
        if seq_len >= self.max_length:
            input_sequence, target_sequence = truncate_sequences(
                sequence, self.max_length
            )
        else:
            # Next chapter
            input_sequence, target_sequence = pad_sequences(
                sequence, self.max_length
            )
        return input_sequence, target_sequence

    # Create a cell for the LSTM
    def make_lstm_cell(self, dropout_keep_prob):
        cell = tf.keras.layers.LSTMCell(self.num_lstm_units)
        dropout_cell = tf.compat.v1.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=dropout_keep_prob)
        return dropout_cell

    # Stack multiple layers for the LSTM
    def stacked_lstm_cells(self, is_training):
        if is_training:
            dropout_keep_prob = 0.5
        else:
            dropout_keep_prob = 1.0

        cell_list = []
        for i in range(self.num_lstm_layers):
            cell_list.append(self.make_lstm_cell(dropout_keep_prob))
        cell = tf.keras.layers.StackedRNNCells(cell_list)
        return cell

    # Convert input sequences to embeddings
    def get_input_embeddings(self, input_sequences):
        embedding_dim = int(self.vocab_size**0.25)
        embedding=tf.keras.layers.Embedding(
            self.vocab_size+1, embedding_dim, embeddings_initializer='uniform',
            mask_zero=True, input_length=self.max_length
        )
        input_embeddings = embedding(input_sequences)
        return input_embeddings

    # Run the LSTM on the input sequences
    def run_lstm(self, input_sequences, is_training):
        cell = self.stacked_lstm_cells(is_training)
        input_embeddings = self.get_input_embeddings(input_sequences)
        binary_sequences = tf.math.sign(input_sequences)
        sequence_lengths = tf.math.reduce_sum(binary_sequences, axis=1)
        lstm_outputs = tf.keras.layers.RNN(cell, input_length=sequence_lengths, dtype=tf.float32)(input_embeddings)
        return lstm_outputs, binary_sequences

    def calculate_loss(self, lstm_outputs, binary_sequences, output_sequences):
        logits = tf.keras.layers.Dense(lstm_outputs, self.vocab_size)
        batch_sequence_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=output_sequences, logits=logits)
        unpadded_loss = tf.cast(binary_sequences, tf.float32) * batch_sequence_loss
        overall_loss = tf.math.reduce_sum(unpadded_loss)
        return overall_loss

    # Predict next word ID
    def get_word_predictions(self, word_preds, binary_sequences, batch_size):
        row_indices = tf.range(batch_size)
        final_indexes = tf.math.reduce_sum(binary_sequences, axis=1) - 1
        gather_indices = tf.transpose([row_indices, final_indexes])
        final_id_predictions = tf.gather_nd(word_preds, gather_indices)
        return final_id_predictions</code></pre>
  </div>
  <div class="card-footer text-muted">
    Reference: <a href="https://www.educative.io/path/become-a-machine-learning-engineer">Become a Machine Learning Engineer</a>
  </div>
</div>
<!-- Language model END -->

</div> <!-- /.col-md-12 -->
</div> <!-- /.row -->
</div> <!-- /.container -->

<include src="/footer.html"></include>

</body>

</html>