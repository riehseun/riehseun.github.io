<!DOCTYPE html>

<html lang="en">

<head>

<!-- Metadata -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="">
<meta name="author" content="Seungmoon Rieh">
<meta name="keywords" content="">

<!-- Title and image -->
<title>Seungmoon Rieh</title>
<link href="/img/seungmoonrieh.jpg" rel="icon">

<!-- CSS -->
<link href="/css/bootstrap.min.css" rel="stylesheet">
<link href="/css/monokai-sublime.css" rel="stylesheet">
<link href="/css/site.css" rel="stylesheet">

<!-- JavaScript -->
<script src="/js/jquery.min.js" type="text/javascript"></script>
<script src="/js/bootstrap.bundle.min.js" type="text/javascript"></script>
<script src="/js/highlight.pack.js" type="text/javascript"></script>
<script src="/js/include_html.js" type="text/javascript"></script>
<script src="/js/mathjax/tex-chtml.js" type="text/javascript"></script>
<script src="/js/site.js" type="text/javascript"></script>

</head>

<body>

<include src="/header.html"></include>

<div class="container">
<div class="row">
<div class="col-md-12">
<h1 class="my-4">Machine Learning</h1>

<!-- Harmful Content Detection BEGIN -->
<div class="card mb-4" id="harmful-content-detection">
  <div class="card-body">
    <h2 class="card-title">Harmful Content Detection</h2>
    <ul class="list-unstyled mb-0">
      <li><a href="#harmful-content-detection-">Harmful Content Detection</a></li>
    </ul>
  </div>
</div>

<div class="card mb-4" id="harmful-content-detection-">
  <div class="card-body">
    <h2 class="card-title">Harmful Content Detection</h2>

    <img class="img-fluid" class="card-img-top" src="/machine-learning-system-design/image/mlsd-1/2-1-1.png" alt="Card image cap">

    <pre><code class="markdown">
# Requirement and Problem

Where does potentially harmful content exist? Instagram feed
What is considered as harmful post? Posts with violent or hate text, nude images and videos
Should the system tell author why content was removed? Yes
Can users report post as harmful? Yes
Should the system support multiple languages? No

How many posts does each user create? 1 per day
Is there training data available? Yes, 100M pairs of posts and 1/0 indicating violence/hate/nude or not
How fast should harmful contents be removed? As soon as possible
Should model be deployed to user device? No

Input - post
Output - whether post is harmful or not
Problem - binary classification 

# High-Level Design

# Data and Feature Engineering

## Data model
Post
UserPostInteraction

## Features
Text
Image
Video
Number of reports, likes, comment, share, etc
Comments
Author’s violation history
Date of author’s account creation

## Feature engineering
Text - BERT
Image/Video - CLIP
Numerical - Standardize (Mean 0, Var 1)
Date - convert to numerical (For example, number of days since a fixed date) 

# Model Development

## Baseline
Heuristics - just use number of reports
Logistic regression - binary classification to determine whether a post in harmful or not

## Architecture
Separate model for violence/hate/nude - lots of training data needed, training cost
Create one model but with different classification heads for violence/hate/nude

## Training
Data - use human labelling and user reports
Loss - compute individual loss for violence/hate/nude, then create weighted sum of those losses
Binary classification  - use cross-entropy (log-loss)  

# Inference and Evaluation

## Offline
Recall - percentage of actual harmful content that the model was able to retrieve

## Online
Decrease in number of user reports  
</code></pre>
  </div>
  <div class="card-footer text-muted">
    Reference: 
  </div>
</div>
<!-- Harmful Content Detection END -->

</div> <!-- /.col-md-12 -->
</div> <!-- /.row -->
</div> <!-- /.container -->

<include src="/footer.html"></include>

</body>

</html>